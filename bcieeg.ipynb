{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Tgoh1wYyiRRDUyemYECNRAN30S-d1JGc",
      "authorship_tag": "ABX9TyNem8JJJyESRLKbFSxzytLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snoopies/EEGTest/blob/main/bcieeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **获取数据并解压**"
      ],
      "metadata": {
        "id": "Y_5mZ-dPoeWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
        "!unzip -q BCICIV_2a_gdf.zip\n",
        "!rm BCICIV_2a_gdf.zip"
      ],
      "metadata": {
        "id": "qo7_FvGtnYgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0244f2de-b46b-42e7-dcc4-fa9339aacfe5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-05 08:03:52--  https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
            "Resolving www.bbci.de (www.bbci.de)... 141.23.71.83\n",
            "Connecting to www.bbci.de (www.bbci.de)|141.23.71.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439968864 (420M) [application/zip]\n",
            "Saving to: ‘BCICIV_2a_gdf.zip’\n",
            "\n",
            "BCICIV_2a_gdf.zip   100%[===================>] 419.59M  1.40MB/s    in 1m 45s  \n",
            "\n",
            "2025-11-05 08:05:37 (4.01 MB/s) - ‘BCICIV_2a_gdf.zip’ saved [439968864/439968864]\n",
            "\n",
            "--2025-11-05 08:05:38--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A01T.mat [following]\n",
            "--2025-11-05 08:05:38--  https://lampx.tugraz.at/~bci/database/001-2014/A01T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42806453 (41M)\n",
            "Saving to: ‘A01T.mat’\n",
            "\n",
            "A01T.mat            100%[===================>]  40.82M  11.5MB/s    in 3.6s    \n",
            "\n",
            "2025-11-05 08:05:43 (11.5 MB/s) - ‘A01T.mat’ saved [42806453/42806453]\n",
            "\n",
            "--2025-11-05 08:05:43--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A02T.mat [following]\n",
            "--2025-11-05 08:05:44--  https://lampx.tugraz.at/~bci/database/001-2014/A02T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43068077 (41M)\n",
            "Saving to: ‘A02T.mat’\n",
            "\n",
            "A02T.mat            100%[===================>]  41.07M  10.1MB/s    in 5.2s    \n",
            "\n",
            "2025-11-05 08:05:50 (7.89 MB/s) - ‘A02T.mat’ saved [43068077/43068077]\n",
            "\n",
            "--2025-11-05 08:05:50--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A03T.mat [following]\n",
            "--2025-11-05 08:05:51--  https://lampx.tugraz.at/~bci/database/001-2014/A03T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44057065 (42M)\n",
            "Saving to: ‘A03T.mat’\n",
            "\n",
            "A03T.mat            100%[===================>]  42.02M  11.2MB/s    in 3.7s    \n",
            "\n",
            "2025-11-05 08:05:55 (11.2 MB/s) - ‘A03T.mat’ saved [44057065/44057065]\n",
            "\n",
            "--2025-11-05 08:05:56--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A04T.mat [following]\n",
            "--2025-11-05 08:05:56--  https://lampx.tugraz.at/~bci/database/001-2014/A04T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37150377 (35M)\n",
            "Saving to: ‘A04T.mat’\n",
            "\n",
            "A04T.mat            100%[===================>]  35.43M  9.34MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 08:06:02 (7.85 MB/s) - ‘A04T.mat’ saved [37150377/37150377]\n",
            "\n",
            "--2025-11-05 08:06:02--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A05T.mat [following]\n",
            "--2025-11-05 08:06:02--  https://lampx.tugraz.at/~bci/database/001-2014/A05T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42452392 (40M)\n",
            "Saving to: ‘A05T.mat’\n",
            "\n",
            "A05T.mat            100%[===================>]  40.49M  10.5MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 08:06:08 (8.28 MB/s) - ‘A05T.mat’ saved [42452392/42452392]\n",
            "\n",
            "--2025-11-05 08:06:08--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A06T.mat [following]\n",
            "--2025-11-05 08:06:09--  https://lampx.tugraz.at/~bci/database/001-2014/A06T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44581256 (43M)\n",
            "Saving to: ‘A06T.mat’\n",
            "\n",
            "A06T.mat            100%[===================>]  42.52M  10.8MB/s    in 5.1s    \n",
            "\n",
            "2025-11-05 08:06:16 (8.41 MB/s) - ‘A06T.mat’ saved [44581256/44581256]\n",
            "\n",
            "--2025-11-05 08:06:16--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A07T.mat [following]\n",
            "--2025-11-05 08:06:17--  https://lampx.tugraz.at/~bci/database/001-2014/A07T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42809746 (41M)\n",
            "Saving to: ‘A07T.mat’\n",
            "\n",
            "A07T.mat            100%[===================>]  40.83M  10.5MB/s    in 5.0s    \n",
            "\n",
            "2025-11-05 08:06:22 (8.14 MB/s) - ‘A07T.mat’ saved [42809746/42809746]\n",
            "\n",
            "--2025-11-05 08:06:22--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A08T.mat [following]\n",
            "--2025-11-05 08:06:23--  https://lampx.tugraz.at/~bci/database/001-2014/A08T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45032065 (43M)\n",
            "Saving to: ‘A08T.mat’\n",
            "\n",
            "A08T.mat            100%[===================>]  42.95M  2.41MB/s    in 34s     \n",
            "\n",
            "2025-11-05 08:06:58 (1.26 MB/s) - ‘A08T.mat’ saved [45032065/45032065]\n",
            "\n",
            "--2025-11-05 08:06:58--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A09T.mat [following]\n",
            "--2025-11-05 08:06:59--  https://lampx.tugraz.at/~bci/database/001-2014/A09T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44785478 (43M)\n",
            "Saving to: ‘A09T.mat’\n",
            "\n",
            "A09T.mat            100%[===================>]  42.71M  7.98MB/s    in 9.3s    \n",
            "\n",
            "2025-11-05 08:07:10 (4.60 MB/s) - ‘A09T.mat’ saved [44785478/44785478]\n",
            "\n",
            "--2025-11-05 08:07:10--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A01E.mat [following]\n",
            "--2025-11-05 08:07:10--  https://lampx.tugraz.at/~bci/database/001-2014/A01E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43772146 (42M)\n",
            "Saving to: ‘A01E.mat’\n",
            "\n",
            "A01E.mat            100%[===================>]  41.74M  9.69MB/s    in 5.7s    \n",
            "\n",
            "2025-11-05 08:07:17 (7.35 MB/s) - ‘A01E.mat’ saved [43772146/43772146]\n",
            "\n",
            "--2025-11-05 08:07:17--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A02E.mat [following]\n",
            "--2025-11-05 08:07:18--  https://lampx.tugraz.at/~bci/database/001-2014/A02E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44218409 (42M)\n",
            "Saving to: ‘A02E.mat’\n",
            "\n",
            "A02E.mat            100%[===================>]  42.17M  10.6MB/s    in 5.2s    \n",
            "\n",
            "2025-11-05 08:07:24 (8.14 MB/s) - ‘A02E.mat’ saved [44218409/44218409]\n",
            "\n",
            "--2025-11-05 08:07:24--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A03E.mat [following]\n",
            "--2025-11-05 08:07:24--  https://lampx.tugraz.at/~bci/database/001-2014/A03E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42316292 (40M)\n",
            "Saving to: ‘A03E.mat’\n",
            "\n",
            "A03E.mat            100%[===================>]  40.36M  10.6MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 08:07:30 (8.18 MB/s) - ‘A03E.mat’ saved [42316292/42316292]\n",
            "\n",
            "--2025-11-05 08:07:30--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A04E.mat [following]\n",
            "--2025-11-05 08:07:31--  https://lampx.tugraz.at/~bci/database/001-2014/A04E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41730824 (40M)\n",
            "Saving to: ‘A04E.mat’\n",
            "\n",
            "A04E.mat            100%[===================>]  39.80M  10.4MB/s    in 5.0s    \n",
            "\n",
            "2025-11-05 08:07:37 (8.03 MB/s) - ‘A04E.mat’ saved [41730824/41730824]\n",
            "\n",
            "--2025-11-05 08:07:37--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A05E.mat [following]\n",
            "--2025-11-05 08:07:38--  https://lampx.tugraz.at/~bci/database/001-2014/A05E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44392496 (42M)\n",
            "Saving to: ‘A05E.mat’\n",
            "\n",
            "A05E.mat            100%[===================>]  42.34M  10.7MB/s    in 5.1s    \n",
            "\n",
            "2025-11-05 08:07:44 (8.24 MB/s) - ‘A05E.mat’ saved [44392496/44392496]\n",
            "\n",
            "--2025-11-05 08:07:44--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A06E.mat [following]\n",
            "--2025-11-05 08:07:45--  https://lampx.tugraz.at/~bci/database/001-2014/A06E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43395510 (41M)\n",
            "Saving to: ‘A06E.mat’\n",
            "\n",
            "A06E.mat            100%[===================>]  41.38M  10.4MB/s    in 5.1s    \n",
            "\n",
            "2025-11-05 08:07:51 (8.06 MB/s) - ‘A06E.mat’ saved [43395510/43395510]\n",
            "\n",
            "--2025-11-05 08:07:51--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A07E.mat [following]\n",
            "--2025-11-05 08:07:51--  https://lampx.tugraz.at/~bci/database/001-2014/A07E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42242294 (40M)\n",
            "Saving to: ‘A07E.mat’\n",
            "\n",
            "A07E.mat            100%[===================>]  40.29M  10.6MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 08:07:57 (8.25 MB/s) - ‘A07E.mat’ saved [42242294/42242294]\n",
            "\n",
            "--2025-11-05 08:07:57--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A08E.mat [following]\n",
            "--2025-11-05 08:07:58--  https://lampx.tugraz.at/~bci/database/001-2014/A08E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46282127 (44M)\n",
            "Saving to: ‘A08E.mat’\n",
            "\n",
            "A08E.mat            100%[===================>]  44.14M  10.5MB/s    in 5.4s    \n",
            "\n",
            "2025-11-05 08:08:04 (8.14 MB/s) - ‘A08E.mat’ saved [46282127/46282127]\n",
            "\n",
            "--2025-11-05 08:08:04--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A09E.mat [following]\n",
            "--2025-11-05 08:08:05--  https://lampx.tugraz.at/~bci/database/001-2014/A09E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44780912 (43M)\n",
            "Saving to: ‘A09E.mat’\n",
            "\n",
            "A09E.mat            100%[===================>]  42.71M  10.7MB/s    in 5.1s    \n",
            "\n",
            "2025-11-05 08:08:12 (8.32 MB/s) - ‘A09E.mat’ saved [44780912/44780912]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **安装python依赖**"
      ],
      "metadata": {
        "id": "_xHR_RP8AcyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne"
      ],
      "metadata": {
        "id": "A0GfDYvmAj_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa0ef9e-3e39-4d5c-ad51-19f17d742f72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.10.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.10.5)\n",
            "Downloading mne-1.10.2-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model.py**"
      ],
      "metadata": {
        "id": "cCoqY8yp31eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ez0GDqI10HEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fe91f5-cebe-41e6-bfae-3b8c4d3880d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 22, 1001]             512\n",
            "       BatchNorm2d-2          [-1, 8, 22, 1001]              16\n",
            "            Conv2d-3          [-1, 16, 1, 1001]             352\n",
            "       BatchNorm2d-4          [-1, 16, 1, 1001]              32\n",
            "               ELU-5          [-1, 16, 1, 1001]               0\n",
            "         AvgPool2d-6           [-1, 16, 1, 250]               0\n",
            "           Dropout-7           [-1, 16, 1, 250]               0\n",
            "            Conv2d-8           [-1, 16, 1, 251]             256\n",
            "            Conv2d-9           [-1, 16, 1, 251]             256\n",
            "      BatchNorm2d-10           [-1, 16, 1, 251]              32\n",
            "              ELU-11           [-1, 16, 1, 251]               0\n",
            "        AvgPool2d-12            [-1, 16, 1, 31]               0\n",
            "          Dropout-13            [-1, 16, 1, 31]               0\n",
            "           Linear-14                    [-1, 4]           1,984\n",
            "          Softmax-15                    [-1, 4]               0\n",
            "================================================================\n",
            "Total params: 3,440\n",
            "Trainable params: 3,440\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.08\n",
            "Forward/backward pass size (MB): 3.25\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 3.34\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# 按照论文创建EEGNet\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, n_classes=4, channels=22, samples=1000,\n",
        "                 dropout_rate=0.5, kernel_length1=64, kernel_length2=16,\n",
        "                 f1=8, d=2, f2=16):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.f1 = f1\n",
        "        self.f2 = f2\n",
        "        self.d = d\n",
        "        self.samples = samples\n",
        "        self.n_classes = n_classes\n",
        "        self.channels = channels\n",
        "        self.kernel_length1 = kernel_length1\n",
        "        self.kernel_length2 = kernel_length2\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=self.f1,\n",
        "                kernel_size=(1, self.kernel_length1),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length1//2),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1)\n",
        "        )\n",
        "\n",
        "        block2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1,\n",
        "                out_channels=self.f1*self.d,\n",
        "                kernel_size=(self.channels, 1),\n",
        "                groups=self.f1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1*self.d),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 4),\n",
        "                stride=4\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        block3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f2,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, self.kernel_length2),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length2//2),\n",
        "                groups=self.f1*self.d,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1*self.d,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, 1),\n",
        "                stride=1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 8),\n",
        "                stride=8\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.EEGNetLayer = nn.Sequential(block1, block2, block3)\n",
        "\n",
        "        self.ClassifierBlock = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.f2*round(round(self.samples//4)//8),\n",
        "                out_features=self.n_classes,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) != 4:\n",
        "            x = torch.unsqueeze(x, dim=1)\n",
        "        x = self.EEGNetLayer(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.ClassifierBlock(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 模型结构可视化\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EEGNet().to(device)\n",
        "    print(summary(model, input_size=(1, 22, 1000)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_train.py**"
      ],
      "metadata": {
        "id": "FEh-At_B4JEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import copy\n",
        "import time\n",
        "#from model import EEGNet\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_train_pre'):\n",
        "    os.makedirs('2a_train_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'T' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose=\"error\") for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "events = []\n",
        "event_ids = []\n",
        "for i in range(len(raw)):\n",
        "    event_to_id = dict({'769': 7, '770': 8, '771': 9, '772': 10})\n",
        "    if i == 3:\n",
        "        event_to_id = dict({'769': 5, '770': 6, '771': 7, '772': 8})\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    else:\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    epochs = mne.Epochs(raw[i], events[i], event_ids[i], tmin, tmax, proj=False, baseline=None, preload=True)\n",
        "\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('./'+data_path[i]+'.mat')\n",
        "\n",
        "    # 打印所有键以便调试\n",
        "    print(f\"MAT file keys for {data_path[i]}: {list(labels_file.keys())}\")\n",
        "\n",
        "    # 新的标签提取方法 - 从data结构体的y字段提取\n",
        "    if 'data' in labels_file:\n",
        "        data_struct = labels_file['data']\n",
        "        print(f\"Data structure shape: {data_struct.shape}\")\n",
        "\n",
        "        # 提取所有trial的标签\n",
        "        all_labels = []\n",
        "        for trial_idx in range(data_struct.shape[1]):\n",
        "            trial_data = data_struct[0, trial_idx]\n",
        "            labels = trial_data['y'][0, 0]  # 提取y字段\n",
        "\n",
        "            if labels.size > 0:  # 只处理有标签的trial\n",
        "                trial_labels = labels.flatten().tolist()\n",
        "                all_labels.extend(trial_labels)\n",
        "                print(f\"Trial {trial_idx+1}: 标签数量 {len(trial_labels)}\")\n",
        "            else:\n",
        "                print(f\"Trial {trial_idx+1}: 无标签\")\n",
        "\n",
        "        labels = np.array(all_labels)\n",
        "        print(f\"提取的总标签数量: {len(labels)}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "        # 显示类别对应关系（用于验证）\n",
        "        if len(all_labels) > 0:\n",
        "            classes = data_struct[0, 0]['classes'][0, 0][0]\n",
        "            print(\"类别对应关系:\")\n",
        "            for class_idx, class_name in enumerate(classes):\n",
        "                print(f\"  标签 {class_idx+1}: {class_name[0]}\")\n",
        "\n",
        "    else:\n",
        "        # 备用方法：使用事件信息生成标签\n",
        "        print(\"使用事件信息生成标签\")\n",
        "        labels = events[i][:, 2]\n",
        "        label_mapping = {7: 1, 8: 2, 9: 3, 10: 4}\n",
        "        if i == 3:  # 特殊处理第4个文件\n",
        "            label_mapping = {5: 1, 6: 2, 7: 3, 8: 4}\n",
        "        labels = np.array([label_mapping.get(event_id, 0) for event_id in labels])\n",
        "        labels = labels[labels != 0]  # 移除无效标签\n",
        "\n",
        "    print(f\"最终标签形状: {labels.shape}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "    epochs_data = epochs.get_data(copy=True)[:, :, :-1]\n",
        "\n",
        "    # 确保标签数量与数据样本数量匹配\n",
        "    n_samples = epochs_data.shape[0]\n",
        "    if len(labels) != n_samples:\n",
        "        print(f\"警告: 标签数量 ({len(labels)}) 与数据样本数量 ({n_samples}) 不匹配\")\n",
        "        # 截取或调整标签以匹配数据数量\n",
        "        min_length = min(len(labels), n_samples)\n",
        "        labels = labels[:min_length]\n",
        "        epochs_data = epochs_data[:min_length]\n",
        "        print(f\"调整后: 标签数量 {len(labels)}, 数据样本数量 {epochs_data.shape[0]}\")\n",
        "\n",
        "    n_channels, n_timepoints = epochs_data.shape[1], epochs_data.shape[2]\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    np.savez('2a_train_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "# 5、创建训练集和验证集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(1, 10):\n",
        "        train_data = np.load(f'2a_train_pre/A0{i}T.npz')\n",
        "        x_train.append(train_data['data'])\n",
        "        y_train.append(train_data['label'])\n",
        "\n",
        "    # 合并数据\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_train = torch.FloatTensor(x_train).unsqueeze(1)\n",
        "    y_train = torch.LongTensor(y_train - 1)\n",
        "\n",
        "    # 创建完整数据集\n",
        "    full_dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "    # 计算训练集和验证集大小\n",
        "    dataset_size = len(full_dataset)\n",
        "    val_size = int(dataset_size * 0.2)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # 划分训练集和验证集\n",
        "    train_data, val_data = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # 创建DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# 6、训练模型\n",
        "def train_model_process(model, train_loader, val_loader, num_epochs):\n",
        "    # 设定训练所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # 使用Adam优化器，学习率为0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    # 损失函数为交叉熵函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # 将模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "    # 复制当前模型的参数\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 初始化参数\n",
        "    best_acc = 0.0\n",
        "    train_loss_all = []\n",
        "    val_loss_all = []\n",
        "    train_acc_all = []\n",
        "    val_acc_all = []\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        # 初始化参数\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        train_num = 0\n",
        "        val_num = 0\n",
        "\n",
        "        # 对每一个batch训练和计算\n",
        "        for step, (b_x, b_y) in enumerate(train_loader):\n",
        "            # 将特征放入到训练设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到训练设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为训练模式\n",
        "            model.train()\n",
        "\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 将梯度初始化为0\n",
        "            optimizer.zero_grad()\n",
        "            # 反向传播计算\n",
        "            loss.backward()\n",
        "            # 根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用\n",
        "            optimizer.step()\n",
        "            # 对损失函数进行累加\n",
        "            train_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            train_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于训练的样本数量\n",
        "            train_num += b_x.size(0)\n",
        "\n",
        "        for step, (b_x, b_y) in enumerate(val_loader):\n",
        "            # 将特征放入到验证设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到验证设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 对损失函数进行累加\n",
        "            val_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            val_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于验证的样本数量\n",
        "            val_num += b_x.size(0)\n",
        "\n",
        "        # 计算并保存每一次迭代的loss值和准确率\n",
        "        # 计算并保存训练集的loss值\n",
        "        train_loss_all.append(train_loss / train_num)\n",
        "        # 计算并保存训练集的准确率\n",
        "        train_acc_all.append(train_corrects.double().item() / train_num)\n",
        "\n",
        "        # 计算并保存验证集的loss值\n",
        "        val_loss_all.append(val_loss / val_num)\n",
        "        # 计算并保存验证集的准确率\n",
        "        val_acc_all.append(val_corrects.double().item() / val_num)\n",
        "\n",
        "        print(\"{} train loss:{:.4f} train acc: {:.4f}\".format(epoch, train_loss_all[-1], train_acc_all[-1]))\n",
        "        print(\"{} val loss:{:.4f} val acc: {:.4f}\".format(epoch, val_loss_all[-1], val_acc_all[-1]))\n",
        "\n",
        "        if val_acc_all[-1] > best_acc:\n",
        "            # 保存当前最高准确度\n",
        "            best_acc = val_acc_all[-1]\n",
        "            # 保存当前最高准确度的模型参数\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # 计算训练和验证的耗时\n",
        "        time_use = time.time() - since\n",
        "        print(\"训练和验证耗费的时间{:.0f}m{:.0f}s\".format(time_use//60, time_use % 60))\n",
        "\n",
        "    # 选择最优参数，保存最优参数的模型\n",
        "    torch.save(best_model_wts, \"best_model.pth\")\n",
        "\n",
        "    train_process = pd.DataFrame(data={\"epoch\": range(num_epochs),\n",
        "\"train_loss_all\": train_loss_all,\n",
        "\"val_loss_all\": val_loss_all,\n",
        "\"train_acc_all\": train_acc_all,\n",
        "\"val_acc_all\": val_acc_all})\n",
        "\n",
        "    return train_process\n",
        "\n",
        "\n",
        "# 7、可视化训练集和验证集的损失函数和准确率\n",
        "def matplot_acc_loss(train_process):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 8、模型开始训练\n",
        "if __name__ == '__main__':\n",
        "    model = EEGNet()\n",
        "    train_loader, val_loader = create_simple_dataloaders()\n",
        "    train_process = train_model_process(model, train_loader, val_loader, num_epochs=500)\n",
        "    matplot_acc_loss(train_process)\n",
        "\n"
      ],
      "metadata": {
        "id": "aN-b-UvU3Y7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5186867-86bd-4d97-bcb7-8a3cb6484823"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A01T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A02T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A03T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A04T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 7)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 标签数量 48\n",
            "Trial 3: 标签数量 48\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A05T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A06T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A07T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A08T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A09T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Epoch 0/499\n",
            "----------\n",
            "0 train loss:1.3769 train acc: 0.3211\n",
            "0 val loss:1.3684 val acc: 0.3475\n",
            "训练和验证耗费的时间0m1s\n",
            "Epoch 1/499\n",
            "----------\n",
            "1 train loss:1.3526 train acc: 0.3573\n",
            "1 val loss:1.3383 val acc: 0.3900\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 2/499\n",
            "----------\n",
            "2 train loss:1.3158 train acc: 0.4084\n",
            "2 val loss:1.3092 val acc: 0.4286\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 3/499\n",
            "----------\n",
            "3 train loss:1.2906 train acc: 0.4383\n",
            "3 val loss:1.2808 val acc: 0.4517\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 4/499\n",
            "----------\n",
            "4 train loss:1.2680 train acc: 0.4629\n",
            "4 val loss:1.2618 val acc: 0.4865\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 5/499\n",
            "----------\n",
            "5 train loss:1.2540 train acc: 0.4793\n",
            "5 val loss:1.2396 val acc: 0.4884\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 6/499\n",
            "----------\n",
            "6 train loss:1.2332 train acc: 0.5072\n",
            "6 val loss:1.2281 val acc: 0.5097\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 7/499\n",
            "----------\n",
            "7 train loss:1.2202 train acc: 0.5154\n",
            "7 val loss:1.2135 val acc: 0.5212\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 8/499\n",
            "----------\n",
            "8 train loss:1.2059 train acc: 0.5352\n",
            "8 val loss:1.2012 val acc: 0.5386\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 9/499\n",
            "----------\n",
            "9 train loss:1.1934 train acc: 0.5521\n",
            "9 val loss:1.1920 val acc: 0.5444\n",
            "训练和验证耗费的时间0m6s\n",
            "Epoch 10/499\n",
            "----------\n",
            "10 train loss:1.1804 train acc: 0.5738\n",
            "10 val loss:1.1807 val acc: 0.5637\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 11/499\n",
            "----------\n",
            "11 train loss:1.1678 train acc: 0.5844\n",
            "11 val loss:1.1765 val acc: 0.5792\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 12/499\n",
            "----------\n",
            "12 train loss:1.1556 train acc: 0.5935\n",
            "12 val loss:1.1724 val acc: 0.5753\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 13/499\n",
            "----------\n",
            "13 train loss:1.1620 train acc: 0.5771\n",
            "13 val loss:1.1692 val acc: 0.5830\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 14/499\n",
            "----------\n",
            "14 train loss:1.1451 train acc: 0.5984\n",
            "14 val loss:1.1759 val acc: 0.5637\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 15/499\n",
            "----------\n",
            "15 train loss:1.1394 train acc: 0.6051\n",
            "15 val loss:1.1645 val acc: 0.5907\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 16/499\n",
            "----------\n",
            "16 train loss:1.1341 train acc: 0.6075\n",
            "16 val loss:1.1615 val acc: 0.5965\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 17/499\n",
            "----------\n",
            "17 train loss:1.1331 train acc: 0.6119\n",
            "17 val loss:1.1591 val acc: 0.5888\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 18/499\n",
            "----------\n",
            "18 train loss:1.1176 train acc: 0.6326\n",
            "18 val loss:1.1661 val acc: 0.5579\n",
            "训练和验证耗费的时间0m11s\n",
            "Epoch 19/499\n",
            "----------\n",
            "19 train loss:1.1198 train acc: 0.6254\n",
            "19 val loss:1.1531 val acc: 0.5869\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 20/499\n",
            "----------\n",
            "20 train loss:1.1146 train acc: 0.6283\n",
            "20 val loss:1.1507 val acc: 0.5927\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 21/499\n",
            "----------\n",
            "21 train loss:1.1073 train acc: 0.6422\n",
            "21 val loss:1.1505 val acc: 0.5888\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 22/499\n",
            "----------\n",
            "22 train loss:1.1171 train acc: 0.6215\n",
            "22 val loss:1.1485 val acc: 0.5792\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 23/499\n",
            "----------\n",
            "23 train loss:1.1098 train acc: 0.6331\n",
            "23 val loss:1.1442 val acc: 0.5927\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 24/499\n",
            "----------\n",
            "24 train loss:1.0960 train acc: 0.6543\n",
            "24 val loss:1.1494 val acc: 0.5907\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 25/499\n",
            "----------\n",
            "25 train loss:1.0972 train acc: 0.6442\n",
            "25 val loss:1.1448 val acc: 0.5830\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 26/499\n",
            "----------\n",
            "26 train loss:1.0988 train acc: 0.6475\n",
            "26 val loss:1.1496 val acc: 0.5927\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 27/499\n",
            "----------\n",
            "27 train loss:1.0978 train acc: 0.6480\n",
            "27 val loss:1.1402 val acc: 0.5946\n",
            "训练和验证耗费的时间0m16s\n",
            "Epoch 28/499\n",
            "----------\n",
            "28 train loss:1.0875 train acc: 0.6620\n",
            "28 val loss:1.1418 val acc: 0.5927\n",
            "训练和验证耗费的时间0m16s\n",
            "Epoch 29/499\n",
            "----------\n",
            "29 train loss:1.0929 train acc: 0.6543\n",
            "29 val loss:1.1467 val acc: 0.5888\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 30/499\n",
            "----------\n",
            "30 train loss:1.0903 train acc: 0.6567\n",
            "30 val loss:1.1365 val acc: 0.5907\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 31/499\n",
            "----------\n",
            "31 train loss:1.0924 train acc: 0.6485\n",
            "31 val loss:1.1330 val acc: 0.6042\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 32/499\n",
            "----------\n",
            "32 train loss:1.0948 train acc: 0.6480\n",
            "32 val loss:1.1340 val acc: 0.6023\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 33/499\n",
            "----------\n",
            "33 train loss:1.0760 train acc: 0.6683\n",
            "33 val loss:1.1288 val acc: 0.6042\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 34/499\n",
            "----------\n",
            "34 train loss:1.0821 train acc: 0.6606\n",
            "34 val loss:1.1309 val acc: 0.5965\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 35/499\n",
            "----------\n",
            "35 train loss:1.0791 train acc: 0.6639\n",
            "35 val loss:1.1305 val acc: 0.5927\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 36/499\n",
            "----------\n",
            "36 train loss:1.0800 train acc: 0.6639\n",
            "36 val loss:1.1272 val acc: 0.6081\n",
            "训练和验证耗费的时间0m21s\n",
            "Epoch 37/499\n",
            "----------\n",
            "37 train loss:1.0871 train acc: 0.6495\n",
            "37 val loss:1.1242 val acc: 0.6042\n",
            "训练和验证耗费的时间0m21s\n",
            "Epoch 38/499\n",
            "----------\n",
            "38 train loss:1.0744 train acc: 0.6673\n",
            "38 val loss:1.1377 val acc: 0.5965\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 39/499\n",
            "----------\n",
            "39 train loss:1.0775 train acc: 0.6620\n",
            "39 val loss:1.1298 val acc: 0.6062\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 40/499\n",
            "----------\n",
            "40 train loss:1.0710 train acc: 0.6702\n",
            "40 val loss:1.1449 val acc: 0.5849\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 41/499\n",
            "----------\n",
            "41 train loss:1.0738 train acc: 0.6770\n",
            "41 val loss:1.1333 val acc: 0.5946\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 42/499\n",
            "----------\n",
            "42 train loss:1.0757 train acc: 0.6678\n",
            "42 val loss:1.1265 val acc: 0.6139\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 43/499\n",
            "----------\n",
            "43 train loss:1.0756 train acc: 0.6683\n",
            "43 val loss:1.1248 val acc: 0.5907\n",
            "训练和验证耗费的时间0m25s\n",
            "Epoch 44/499\n",
            "----------\n",
            "44 train loss:1.0731 train acc: 0.6745\n",
            "44 val loss:1.1297 val acc: 0.6004\n",
            "训练和验证耗费的时间0m25s\n",
            "Epoch 45/499\n",
            "----------\n",
            "45 train loss:1.0760 train acc: 0.6601\n",
            "45 val loss:1.1256 val acc: 0.5888\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 46/499\n",
            "----------\n",
            "46 train loss:1.0654 train acc: 0.6779\n",
            "46 val loss:1.1176 val acc: 0.6178\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 47/499\n",
            "----------\n",
            "47 train loss:1.0621 train acc: 0.6794\n",
            "47 val loss:1.1152 val acc: 0.6293\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 48/499\n",
            "----------\n",
            "48 train loss:1.0656 train acc: 0.6770\n",
            "48 val loss:1.1190 val acc: 0.6158\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 49/499\n",
            "----------\n",
            "49 train loss:1.0622 train acc: 0.6798\n",
            "49 val loss:1.1135 val acc: 0.6216\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 50/499\n",
            "----------\n",
            "50 train loss:1.0644 train acc: 0.6745\n",
            "50 val loss:1.1155 val acc: 0.6158\n",
            "训练和验证耗费的时间0m29s\n",
            "Epoch 51/499\n",
            "----------\n",
            "51 train loss:1.0645 train acc: 0.6798\n",
            "51 val loss:1.1213 val acc: 0.6120\n",
            "训练和验证耗费的时间0m29s\n",
            "Epoch 52/499\n",
            "----------\n",
            "52 train loss:1.0662 train acc: 0.6755\n",
            "52 val loss:1.1151 val acc: 0.6139\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 53/499\n",
            "----------\n",
            "53 train loss:1.0665 train acc: 0.6736\n",
            "53 val loss:1.1129 val acc: 0.6158\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 54/499\n",
            "----------\n",
            "54 train loss:1.0619 train acc: 0.6808\n",
            "54 val loss:1.1186 val acc: 0.6236\n",
            "训练和验证耗费的时间0m31s\n",
            "Epoch 55/499\n",
            "----------\n",
            "55 train loss:1.0604 train acc: 0.6823\n",
            "55 val loss:1.1240 val acc: 0.6004\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 56/499\n",
            "----------\n",
            "56 train loss:1.0630 train acc: 0.6842\n",
            "56 val loss:1.1175 val acc: 0.6100\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 57/499\n",
            "----------\n",
            "57 train loss:1.0625 train acc: 0.6784\n",
            "57 val loss:1.1141 val acc: 0.6236\n",
            "训练和验证耗费的时间0m33s\n",
            "Epoch 58/499\n",
            "----------\n",
            "58 train loss:1.0599 train acc: 0.6765\n",
            "58 val loss:1.1168 val acc: 0.6139\n",
            "训练和验证耗费的时间0m33s\n",
            "Epoch 59/499\n",
            "----------\n",
            "59 train loss:1.0627 train acc: 0.6765\n",
            "59 val loss:1.1178 val acc: 0.6158\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 60/499\n",
            "----------\n",
            "60 train loss:1.0634 train acc: 0.6750\n",
            "60 val loss:1.1161 val acc: 0.6197\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 61/499\n",
            "----------\n",
            "61 train loss:1.0554 train acc: 0.6856\n",
            "61 val loss:1.1147 val acc: 0.6100\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 62/499\n",
            "----------\n",
            "62 train loss:1.0595 train acc: 0.6823\n",
            "62 val loss:1.1216 val acc: 0.6139\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 63/499\n",
            "----------\n",
            "63 train loss:1.0637 train acc: 0.6750\n",
            "63 val loss:1.1130 val acc: 0.6139\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 64/499\n",
            "----------\n",
            "64 train loss:1.0460 train acc: 0.6962\n",
            "64 val loss:1.1112 val acc: 0.6255\n",
            "训练和验证耗费的时间0m37s\n",
            "Epoch 65/499\n",
            "----------\n",
            "65 train loss:1.0536 train acc: 0.6914\n",
            "65 val loss:1.1111 val acc: 0.6139\n",
            "训练和验证耗费的时间0m37s\n",
            "Epoch 66/499\n",
            "----------\n",
            "66 train loss:1.0540 train acc: 0.6900\n",
            "66 val loss:1.1103 val acc: 0.6158\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 67/499\n",
            "----------\n",
            "67 train loss:1.0522 train acc: 0.6885\n",
            "67 val loss:1.1082 val acc: 0.6313\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 68/499\n",
            "----------\n",
            "68 train loss:1.0486 train acc: 0.6986\n",
            "68 val loss:1.1085 val acc: 0.6313\n",
            "训练和验证耗费的时间0m39s\n",
            "Epoch 69/499\n",
            "----------\n",
            "69 train loss:1.0519 train acc: 0.6900\n",
            "69 val loss:1.1076 val acc: 0.6236\n",
            "训练和验证耗费的时间0m40s\n",
            "Epoch 70/499\n",
            "----------\n",
            "70 train loss:1.0655 train acc: 0.6770\n",
            "70 val loss:1.1095 val acc: 0.6216\n",
            "训练和验证耗费的时间0m40s\n",
            "Epoch 71/499\n",
            "----------\n",
            "71 train loss:1.0523 train acc: 0.6880\n",
            "71 val loss:1.1129 val acc: 0.6120\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 72/499\n",
            "----------\n",
            "72 train loss:1.0488 train acc: 0.6967\n",
            "72 val loss:1.1057 val acc: 0.6216\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 73/499\n",
            "----------\n",
            "73 train loss:1.0503 train acc: 0.6866\n",
            "73 val loss:1.1083 val acc: 0.6158\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 74/499\n",
            "----------\n",
            "74 train loss:1.0516 train acc: 0.6905\n",
            "74 val loss:1.1084 val acc: 0.6274\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 75/499\n",
            "----------\n",
            "75 train loss:1.0436 train acc: 0.6986\n",
            "75 val loss:1.1042 val acc: 0.6216\n",
            "训练和验证耗费的时间0m43s\n",
            "Epoch 76/499\n",
            "----------\n",
            "76 train loss:1.0407 train acc: 0.6958\n",
            "76 val loss:1.1174 val acc: 0.6158\n",
            "训练和验证耗费的时间0m43s\n",
            "Epoch 77/499\n",
            "----------\n",
            "77 train loss:1.0400 train acc: 0.7030\n",
            "77 val loss:1.1077 val acc: 0.6197\n",
            "训练和验证耗费的时间0m44s\n",
            "Epoch 78/499\n",
            "----------\n",
            "78 train loss:1.0465 train acc: 0.6919\n",
            "78 val loss:1.1037 val acc: 0.6313\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 79/499\n",
            "----------\n",
            "79 train loss:1.0478 train acc: 0.6919\n",
            "79 val loss:1.1048 val acc: 0.6274\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 80/499\n",
            "----------\n",
            "80 train loss:1.0451 train acc: 0.6996\n",
            "80 val loss:1.0992 val acc: 0.6409\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 81/499\n",
            "----------\n",
            "81 train loss:1.0511 train acc: 0.6871\n",
            "81 val loss:1.1038 val acc: 0.6274\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 82/499\n",
            "----------\n",
            "82 train loss:1.0465 train acc: 0.6851\n",
            "82 val loss:1.1043 val acc: 0.6197\n",
            "训练和验证耗费的时间0m47s\n",
            "Epoch 83/499\n",
            "----------\n",
            "83 train loss:1.0421 train acc: 0.6982\n",
            "83 val loss:1.1042 val acc: 0.6274\n",
            "训练和验证耗费的时间0m47s\n",
            "Epoch 84/499\n",
            "----------\n",
            "84 train loss:1.0411 train acc: 0.7035\n",
            "84 val loss:1.1048 val acc: 0.6236\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 85/499\n",
            "----------\n",
            "85 train loss:1.0370 train acc: 0.7078\n",
            "85 val loss:1.0973 val acc: 0.6313\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 86/499\n",
            "----------\n",
            "86 train loss:1.0487 train acc: 0.6895\n",
            "86 val loss:1.0976 val acc: 0.6390\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 87/499\n",
            "----------\n",
            "87 train loss:1.0330 train acc: 0.7093\n",
            "87 val loss:1.0977 val acc: 0.6371\n",
            "训练和验证耗费的时间0m50s\n",
            "Epoch 88/499\n",
            "----------\n",
            "88 train loss:1.0359 train acc: 0.7059\n",
            "88 val loss:1.0939 val acc: 0.6313\n",
            "训练和验证耗费的时间0m50s\n",
            "Epoch 89/499\n",
            "----------\n",
            "89 train loss:1.0369 train acc: 0.7064\n",
            "89 val loss:1.0962 val acc: 0.6371\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 90/499\n",
            "----------\n",
            "90 train loss:1.0367 train acc: 0.7044\n",
            "90 val loss:1.1017 val acc: 0.6255\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 91/499\n",
            "----------\n",
            "91 train loss:1.0294 train acc: 0.7160\n",
            "91 val loss:1.0973 val acc: 0.6351\n",
            "训练和验证耗费的时间0m52s\n",
            "Epoch 92/499\n",
            "----------\n",
            "92 train loss:1.0306 train acc: 0.7107\n",
            "92 val loss:1.0990 val acc: 0.6274\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 93/499\n",
            "----------\n",
            "93 train loss:1.0285 train acc: 0.7126\n",
            "93 val loss:1.0930 val acc: 0.6351\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 94/499\n",
            "----------\n",
            "94 train loss:1.0420 train acc: 0.7030\n",
            "94 val loss:1.0934 val acc: 0.6216\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 95/499\n",
            "----------\n",
            "95 train loss:1.0349 train acc: 0.7102\n",
            "95 val loss:1.0930 val acc: 0.6409\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 96/499\n",
            "----------\n",
            "96 train loss:1.0331 train acc: 0.7088\n",
            "96 val loss:1.0900 val acc: 0.6293\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 97/499\n",
            "----------\n",
            "97 train loss:1.0199 train acc: 0.7203\n",
            "97 val loss:1.0930 val acc: 0.6293\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 98/499\n",
            "----------\n",
            "98 train loss:1.0296 train acc: 0.7126\n",
            "98 val loss:1.0968 val acc: 0.6371\n",
            "训练和验证耗费的时间0m56s\n",
            "Epoch 99/499\n",
            "----------\n",
            "99 train loss:1.0376 train acc: 0.7068\n",
            "99 val loss:1.0828 val acc: 0.6506\n",
            "训练和验证耗费的时间0m56s\n",
            "Epoch 100/499\n",
            "----------\n",
            "100 train loss:1.0360 train acc: 0.7073\n",
            "100 val loss:1.0893 val acc: 0.6351\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 101/499\n",
            "----------\n",
            "101 train loss:1.0377 train acc: 0.7059\n",
            "101 val loss:1.0879 val acc: 0.6351\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 102/499\n",
            "----------\n",
            "102 train loss:1.0349 train acc: 0.7044\n",
            "102 val loss:1.0862 val acc: 0.6390\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 103/499\n",
            "----------\n",
            "103 train loss:1.0304 train acc: 0.7126\n",
            "103 val loss:1.0801 val acc: 0.6506\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 104/499\n",
            "----------\n",
            "104 train loss:1.0389 train acc: 0.7001\n",
            "104 val loss:1.0852 val acc: 0.6564\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 105/499\n",
            "----------\n",
            "105 train loss:1.0229 train acc: 0.7203\n",
            "105 val loss:1.0824 val acc: 0.6448\n",
            "训练和验证耗费的时间0m60s\n",
            "Epoch 106/499\n",
            "----------\n",
            "106 train loss:1.0284 train acc: 0.7097\n",
            "106 val loss:1.0885 val acc: 0.6409\n",
            "训练和验证耗费的时间1m0s\n",
            "Epoch 107/499\n",
            "----------\n",
            "107 train loss:1.0234 train acc: 0.7203\n",
            "107 val loss:1.0850 val acc: 0.6429\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 108/499\n",
            "----------\n",
            "108 train loss:1.0239 train acc: 0.7170\n",
            "108 val loss:1.0850 val acc: 0.6332\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 109/499\n",
            "----------\n",
            "109 train loss:1.0278 train acc: 0.7189\n",
            "109 val loss:1.0894 val acc: 0.6274\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 110/499\n",
            "----------\n",
            "110 train loss:1.0270 train acc: 0.7170\n",
            "110 val loss:1.0797 val acc: 0.6486\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 111/499\n",
            "----------\n",
            "111 train loss:1.0225 train acc: 0.7203\n",
            "111 val loss:1.0890 val acc: 0.6390\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 112/499\n",
            "----------\n",
            "112 train loss:1.0311 train acc: 0.7073\n",
            "112 val loss:1.0769 val acc: 0.6641\n",
            "训练和验证耗费的时间1m4s\n",
            "Epoch 113/499\n",
            "----------\n",
            "113 train loss:1.0276 train acc: 0.7189\n",
            "113 val loss:1.0686 val acc: 0.6622\n",
            "训练和验证耗费的时间1m4s\n",
            "Epoch 114/499\n",
            "----------\n",
            "114 train loss:1.0222 train acc: 0.7175\n",
            "114 val loss:1.0763 val acc: 0.6583\n",
            "训练和验证耗费的时间1m5s\n",
            "Epoch 115/499\n",
            "----------\n",
            "115 train loss:1.0279 train acc: 0.7112\n",
            "115 val loss:1.0758 val acc: 0.6467\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 116/499\n",
            "----------\n",
            "116 train loss:1.0229 train acc: 0.7107\n",
            "116 val loss:1.0778 val acc: 0.6525\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 117/499\n",
            "----------\n",
            "117 train loss:1.0179 train acc: 0.7189\n",
            "117 val loss:1.0723 val acc: 0.6544\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 118/499\n",
            "----------\n",
            "118 train loss:1.0284 train acc: 0.7102\n",
            "118 val loss:1.0735 val acc: 0.6544\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 119/499\n",
            "----------\n",
            "119 train loss:1.0188 train acc: 0.7184\n",
            "119 val loss:1.0770 val acc: 0.6583\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 120/499\n",
            "----------\n",
            "120 train loss:1.0223 train acc: 0.7194\n",
            "120 val loss:1.0770 val acc: 0.6486\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 121/499\n",
            "----------\n",
            "121 train loss:1.0204 train acc: 0.7213\n",
            "121 val loss:1.0742 val acc: 0.6448\n",
            "训练和验证耗费的时间1m9s\n",
            "Epoch 122/499\n",
            "----------\n",
            "122 train loss:1.0101 train acc: 0.7319\n",
            "122 val loss:1.0783 val acc: 0.6544\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 123/499\n",
            "----------\n",
            "123 train loss:1.0102 train acc: 0.7372\n",
            "123 val loss:1.0748 val acc: 0.6564\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 124/499\n",
            "----------\n",
            "124 train loss:1.0243 train acc: 0.7184\n",
            "124 val loss:1.0704 val acc: 0.6680\n",
            "训练和验证耗费的时间1m11s\n",
            "Epoch 125/499\n",
            "----------\n",
            "125 train loss:1.0181 train acc: 0.7199\n",
            "125 val loss:1.0770 val acc: 0.6448\n",
            "训练和验证耗费的时间1m12s\n",
            "Epoch 126/499\n",
            "----------\n",
            "126 train loss:1.0167 train acc: 0.7281\n",
            "126 val loss:1.0733 val acc: 0.6564\n",
            "训练和验证耗费的时间1m13s\n",
            "Epoch 127/499\n",
            "----------\n",
            "127 train loss:1.0174 train acc: 0.7300\n",
            "127 val loss:1.0813 val acc: 0.6467\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 128/499\n",
            "----------\n",
            "128 train loss:1.0165 train acc: 0.7261\n",
            "128 val loss:1.0709 val acc: 0.6660\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 129/499\n",
            "----------\n",
            "129 train loss:1.0120 train acc: 0.7285\n",
            "129 val loss:1.0678 val acc: 0.6641\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 130/499\n",
            "----------\n",
            "130 train loss:1.0063 train acc: 0.7353\n",
            "130 val loss:1.0657 val acc: 0.6660\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 131/499\n",
            "----------\n",
            "131 train loss:1.0162 train acc: 0.7232\n",
            "131 val loss:1.0690 val acc: 0.6583\n",
            "训练和验证耗费的时间1m16s\n",
            "Epoch 132/499\n",
            "----------\n",
            "132 train loss:1.0216 train acc: 0.7189\n",
            "132 val loss:1.0715 val acc: 0.6583\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 133/499\n",
            "----------\n",
            "133 train loss:1.0200 train acc: 0.7165\n",
            "133 val loss:1.0692 val acc: 0.6680\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 134/499\n",
            "----------\n",
            "134 train loss:1.0154 train acc: 0.7290\n",
            "134 val loss:1.0696 val acc: 0.6660\n",
            "训练和验证耗费的时间1m18s\n",
            "Epoch 135/499\n",
            "----------\n",
            "135 train loss:1.0126 train acc: 0.7285\n",
            "135 val loss:1.0758 val acc: 0.6467\n",
            "训练和验证耗费的时间1m18s\n",
            "Epoch 136/499\n",
            "----------\n",
            "136 train loss:1.0130 train acc: 0.7310\n",
            "136 val loss:1.0736 val acc: 0.6641\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 137/499\n",
            "----------\n",
            "137 train loss:1.0123 train acc: 0.7290\n",
            "137 val loss:1.0733 val acc: 0.6583\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 138/499\n",
            "----------\n",
            "138 train loss:1.0168 train acc: 0.7199\n",
            "138 val loss:1.0707 val acc: 0.6564\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 139/499\n",
            "----------\n",
            "139 train loss:1.0155 train acc: 0.7232\n",
            "139 val loss:1.0760 val acc: 0.6506\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 140/499\n",
            "----------\n",
            "140 train loss:1.0100 train acc: 0.7276\n",
            "140 val loss:1.0785 val acc: 0.6448\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 141/499\n",
            "----------\n",
            "141 train loss:1.0141 train acc: 0.7218\n",
            "141 val loss:1.0703 val acc: 0.6660\n",
            "训练和验证耗费的时间1m22s\n",
            "Epoch 142/499\n",
            "----------\n",
            "142 train loss:1.0219 train acc: 0.7093\n",
            "142 val loss:1.0812 val acc: 0.6506\n",
            "训练和验证耗费的时间1m22s\n",
            "Epoch 143/499\n",
            "----------\n",
            "143 train loss:1.0184 train acc: 0.7170\n",
            "143 val loss:1.0759 val acc: 0.6544\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 144/499\n",
            "----------\n",
            "144 train loss:1.0094 train acc: 0.7343\n",
            "144 val loss:1.0716 val acc: 0.6622\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 145/499\n",
            "----------\n",
            "145 train loss:1.0192 train acc: 0.7218\n",
            "145 val loss:1.0783 val acc: 0.6506\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 146/499\n",
            "----------\n",
            "146 train loss:1.0117 train acc: 0.7228\n",
            "146 val loss:1.0653 val acc: 0.6699\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 147/499\n",
            "----------\n",
            "147 train loss:1.0141 train acc: 0.7242\n",
            "147 val loss:1.0680 val acc: 0.6583\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 148/499\n",
            "----------\n",
            "148 train loss:1.0117 train acc: 0.7242\n",
            "148 val loss:1.0709 val acc: 0.6602\n",
            "训练和验证耗费的时间1m26s\n",
            "Epoch 149/499\n",
            "----------\n",
            "149 train loss:1.0166 train acc: 0.7218\n",
            "149 val loss:1.0668 val acc: 0.6622\n",
            "训练和验证耗费的时间1m26s\n",
            "Epoch 150/499\n",
            "----------\n",
            "150 train loss:1.0219 train acc: 0.7179\n",
            "150 val loss:1.0642 val acc: 0.6641\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 151/499\n",
            "----------\n",
            "151 train loss:1.0110 train acc: 0.7281\n",
            "151 val loss:1.0683 val acc: 0.6564\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 152/499\n",
            "----------\n",
            "152 train loss:1.0052 train acc: 0.7363\n",
            "152 val loss:1.0723 val acc: 0.6564\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 153/499\n",
            "----------\n",
            "153 train loss:1.0101 train acc: 0.7310\n",
            "153 val loss:1.0708 val acc: 0.6564\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 154/499\n",
            "----------\n",
            "154 train loss:1.0106 train acc: 0.7276\n",
            "154 val loss:1.0680 val acc: 0.6737\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 155/499\n",
            "----------\n",
            "155 train loss:1.0139 train acc: 0.7247\n",
            "155 val loss:1.0634 val acc: 0.6680\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 156/499\n",
            "----------\n",
            "156 train loss:0.9951 train acc: 0.7493\n",
            "156 val loss:1.0721 val acc: 0.6602\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 157/499\n",
            "----------\n",
            "157 train loss:1.0045 train acc: 0.7416\n",
            "157 val loss:1.0728 val acc: 0.6602\n",
            "训练和验证耗费的时间1m31s\n",
            "Epoch 158/499\n",
            "----------\n",
            "158 train loss:1.0037 train acc: 0.7358\n",
            "158 val loss:1.0707 val acc: 0.6602\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 159/499\n",
            "----------\n",
            "159 train loss:1.0056 train acc: 0.7382\n",
            "159 val loss:1.0709 val acc: 0.6544\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 160/499\n",
            "----------\n",
            "160 train loss:1.0106 train acc: 0.7314\n",
            "160 val loss:1.0673 val acc: 0.6737\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 161/499\n",
            "----------\n",
            "161 train loss:1.0163 train acc: 0.7232\n",
            "161 val loss:1.0610 val acc: 0.6776\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 162/499\n",
            "----------\n",
            "162 train loss:1.0072 train acc: 0.7295\n",
            "162 val loss:1.0650 val acc: 0.6776\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 163/499\n",
            "----------\n",
            "163 train loss:1.0049 train acc: 0.7348\n",
            "163 val loss:1.0659 val acc: 0.6641\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 164/499\n",
            "----------\n",
            "164 train loss:1.0115 train acc: 0.7324\n",
            "164 val loss:1.0605 val acc: 0.6737\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 165/499\n",
            "----------\n",
            "165 train loss:1.0090 train acc: 0.7300\n",
            "165 val loss:1.0645 val acc: 0.6718\n",
            "训练和验证耗费的时间1m36s\n",
            "Epoch 166/499\n",
            "----------\n",
            "166 train loss:1.0055 train acc: 0.7348\n",
            "166 val loss:1.0588 val acc: 0.6718\n",
            "训练和验证耗费的时间1m36s\n",
            "Epoch 167/499\n",
            "----------\n",
            "167 train loss:1.0151 train acc: 0.7261\n",
            "167 val loss:1.0654 val acc: 0.6641\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 168/499\n",
            "----------\n",
            "168 train loss:1.0160 train acc: 0.7281\n",
            "168 val loss:1.0603 val acc: 0.6834\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 169/499\n",
            "----------\n",
            "169 train loss:1.0151 train acc: 0.7228\n",
            "169 val loss:1.0672 val acc: 0.6737\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 170/499\n",
            "----------\n",
            "170 train loss:1.0100 train acc: 0.7324\n",
            "170 val loss:1.0588 val acc: 0.6776\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 171/499\n",
            "----------\n",
            "171 train loss:1.0095 train acc: 0.7290\n",
            "171 val loss:1.0658 val acc: 0.6757\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 172/499\n",
            "----------\n",
            "172 train loss:1.0063 train acc: 0.7329\n",
            "172 val loss:1.0609 val acc: 0.6757\n",
            "训练和验证耗费的时间1m40s\n",
            "Epoch 173/499\n",
            "----------\n",
            "173 train loss:1.0013 train acc: 0.7358\n",
            "173 val loss:1.0620 val acc: 0.6718\n",
            "训练和验证耗费的时间1m40s\n",
            "Epoch 174/499\n",
            "----------\n",
            "174 train loss:1.0096 train acc: 0.7324\n",
            "174 val loss:1.0704 val acc: 0.6602\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 175/499\n",
            "----------\n",
            "175 train loss:1.0016 train acc: 0.7363\n",
            "175 val loss:1.0640 val acc: 0.6757\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 176/499\n",
            "----------\n",
            "176 train loss:1.0125 train acc: 0.7242\n",
            "176 val loss:1.0621 val acc: 0.6737\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 177/499\n",
            "----------\n",
            "177 train loss:0.9952 train acc: 0.7507\n",
            "177 val loss:1.0607 val acc: 0.6757\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 178/499\n",
            "----------\n",
            "178 train loss:1.0013 train acc: 0.7392\n",
            "178 val loss:1.0551 val acc: 0.6834\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 179/499\n",
            "----------\n",
            "179 train loss:1.0241 train acc: 0.7146\n",
            "179 val loss:1.0635 val acc: 0.6795\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 180/499\n",
            "----------\n",
            "180 train loss:0.9984 train acc: 0.7411\n",
            "180 val loss:1.0644 val acc: 0.6718\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 181/499\n",
            "----------\n",
            "181 train loss:1.0053 train acc: 0.7329\n",
            "181 val loss:1.0596 val acc: 0.6834\n",
            "训练和验证耗费的时间1m45s\n",
            "Epoch 182/499\n",
            "----------\n",
            "182 train loss:1.0107 train acc: 0.7314\n",
            "182 val loss:1.0602 val acc: 0.6737\n",
            "训练和验证耗费的时间1m45s\n",
            "Epoch 183/499\n",
            "----------\n",
            "183 train loss:1.0054 train acc: 0.7406\n",
            "183 val loss:1.0594 val acc: 0.6834\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 184/499\n",
            "----------\n",
            "184 train loss:1.0094 train acc: 0.7314\n",
            "184 val loss:1.0651 val acc: 0.6660\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 185/499\n",
            "----------\n",
            "185 train loss:1.0030 train acc: 0.7387\n",
            "185 val loss:1.0682 val acc: 0.6641\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 186/499\n",
            "----------\n",
            "186 train loss:0.9967 train acc: 0.7454\n",
            "186 val loss:1.0568 val acc: 0.6718\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 187/499\n",
            "----------\n",
            "187 train loss:1.0053 train acc: 0.7329\n",
            "187 val loss:1.0657 val acc: 0.6699\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 188/499\n",
            "----------\n",
            "188 train loss:1.0031 train acc: 0.7338\n",
            "188 val loss:1.0594 val acc: 0.6795\n",
            "训练和验证耗费的时间1m49s\n",
            "Epoch 189/499\n",
            "----------\n",
            "189 train loss:1.0028 train acc: 0.7363\n",
            "189 val loss:1.0558 val acc: 0.6757\n",
            "训练和验证耗费的时间1m49s\n",
            "Epoch 190/499\n",
            "----------\n",
            "190 train loss:1.0014 train acc: 0.7406\n",
            "190 val loss:1.0639 val acc: 0.6718\n",
            "训练和验证耗费的时间1m50s\n",
            "Epoch 191/499\n",
            "----------\n",
            "191 train loss:0.9882 train acc: 0.7536\n",
            "191 val loss:1.0630 val acc: 0.6680\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 192/499\n",
            "----------\n",
            "192 train loss:1.0021 train acc: 0.7353\n",
            "192 val loss:1.0667 val acc: 0.6699\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 193/499\n",
            "----------\n",
            "193 train loss:1.0121 train acc: 0.7281\n",
            "193 val loss:1.0669 val acc: 0.6583\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 194/499\n",
            "----------\n",
            "194 train loss:1.0143 train acc: 0.7232\n",
            "194 val loss:1.0672 val acc: 0.6660\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 195/499\n",
            "----------\n",
            "195 train loss:1.0159 train acc: 0.7189\n",
            "195 val loss:1.0535 val acc: 0.6815\n",
            "训练和验证耗费的时间1m53s\n",
            "Epoch 196/499\n",
            "----------\n",
            "196 train loss:0.9990 train acc: 0.7382\n",
            "196 val loss:1.0599 val acc: 0.6834\n",
            "训练和验证耗费的时间1m53s\n",
            "Epoch 197/499\n",
            "----------\n",
            "197 train loss:1.0033 train acc: 0.7353\n",
            "197 val loss:1.0555 val acc: 0.6834\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 198/499\n",
            "----------\n",
            "198 train loss:1.0111 train acc: 0.7281\n",
            "198 val loss:1.0531 val acc: 0.6892\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 199/499\n",
            "----------\n",
            "199 train loss:0.9998 train acc: 0.7387\n",
            "199 val loss:1.0614 val acc: 0.6737\n",
            "训练和验证耗费的时间1m55s\n",
            "Epoch 200/499\n",
            "----------\n",
            "200 train loss:0.9970 train acc: 0.7396\n",
            "200 val loss:1.0648 val acc: 0.6776\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 201/499\n",
            "----------\n",
            "201 train loss:1.0010 train acc: 0.7367\n",
            "201 val loss:1.0615 val acc: 0.6718\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 202/499\n",
            "----------\n",
            "202 train loss:0.9994 train acc: 0.7396\n",
            "202 val loss:1.0572 val acc: 0.6873\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 203/499\n",
            "----------\n",
            "203 train loss:1.0085 train acc: 0.7305\n",
            "203 val loss:1.0648 val acc: 0.6795\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 204/499\n",
            "----------\n",
            "204 train loss:0.9961 train acc: 0.7454\n",
            "204 val loss:1.0582 val acc: 0.6795\n",
            "训练和验证耗费的时间1m58s\n",
            "Epoch 205/499\n",
            "----------\n",
            "205 train loss:0.9966 train acc: 0.7420\n",
            "205 val loss:1.0647 val acc: 0.6564\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 206/499\n",
            "----------\n",
            "206 train loss:1.0027 train acc: 0.7367\n",
            "206 val loss:1.0558 val acc: 0.6834\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 207/499\n",
            "----------\n",
            "207 train loss:1.0005 train acc: 0.7430\n",
            "207 val loss:1.0571 val acc: 0.6853\n",
            "训练和验证耗费的时间1m60s\n",
            "Epoch 208/499\n",
            "----------\n",
            "208 train loss:0.9971 train acc: 0.7445\n",
            "208 val loss:1.0637 val acc: 0.6815\n",
            "训练和验证耗费的时间2m0s\n",
            "Epoch 209/499\n",
            "----------\n",
            "209 train loss:1.0046 train acc: 0.7334\n",
            "209 val loss:1.0572 val acc: 0.6853\n",
            "训练和验证耗费的时间2m1s\n",
            "Epoch 210/499\n",
            "----------\n",
            "210 train loss:1.0051 train acc: 0.7285\n",
            "210 val loss:1.0599 val acc: 0.6718\n",
            "训练和验证耗费的时间2m1s\n",
            "Epoch 211/499\n",
            "----------\n",
            "211 train loss:0.9986 train acc: 0.7367\n",
            "211 val loss:1.0618 val acc: 0.6680\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 212/499\n",
            "----------\n",
            "212 train loss:1.0028 train acc: 0.7343\n",
            "212 val loss:1.0537 val acc: 0.6718\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 213/499\n",
            "----------\n",
            "213 train loss:0.9947 train acc: 0.7416\n",
            "213 val loss:1.0572 val acc: 0.6776\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 214/499\n",
            "----------\n",
            "214 train loss:1.0004 train acc: 0.7396\n",
            "214 val loss:1.0588 val acc: 0.6834\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 215/499\n",
            "----------\n",
            "215 train loss:1.0072 train acc: 0.7314\n",
            "215 val loss:1.0597 val acc: 0.6718\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 216/499\n",
            "----------\n",
            "216 train loss:0.9969 train acc: 0.7493\n",
            "216 val loss:1.0588 val acc: 0.6757\n",
            "训练和验证耗费的时间2m5s\n",
            "Epoch 217/499\n",
            "----------\n",
            "217 train loss:1.0048 train acc: 0.7314\n",
            "217 val loss:1.0590 val acc: 0.6776\n",
            "训练和验证耗费的时间2m5s\n",
            "Epoch 218/499\n",
            "----------\n",
            "218 train loss:0.9867 train acc: 0.7512\n",
            "218 val loss:1.0561 val acc: 0.6873\n",
            "训练和验证耗费的时间2m6s\n",
            "Epoch 219/499\n",
            "----------\n",
            "219 train loss:0.9902 train acc: 0.7512\n",
            "219 val loss:1.0582 val acc: 0.6853\n",
            "训练和验证耗费的时间2m7s\n",
            "Epoch 220/499\n",
            "----------\n",
            "220 train loss:1.0018 train acc: 0.7372\n",
            "220 val loss:1.0520 val acc: 0.6873\n",
            "训练和验证耗费的时间2m7s\n",
            "Epoch 221/499\n",
            "----------\n",
            "221 train loss:1.0066 train acc: 0.7271\n",
            "221 val loss:1.0634 val acc: 0.6641\n",
            "训练和验证耗费的时间2m8s\n",
            "Epoch 222/499\n",
            "----------\n",
            "222 train loss:0.9970 train acc: 0.7420\n",
            "222 val loss:1.0548 val acc: 0.6795\n",
            "训练和验证耗费的时间2m8s\n",
            "Epoch 223/499\n",
            "----------\n",
            "223 train loss:0.9990 train acc: 0.7401\n",
            "223 val loss:1.0535 val acc: 0.6757\n",
            "训练和验证耗费的时间2m9s\n",
            "Epoch 224/499\n",
            "----------\n",
            "224 train loss:0.9963 train acc: 0.7469\n",
            "224 val loss:1.0492 val acc: 0.6873\n",
            "训练和验证耗费的时间2m9s\n",
            "Epoch 225/499\n",
            "----------\n",
            "225 train loss:1.0011 train acc: 0.7348\n",
            "225 val loss:1.0531 val acc: 0.6873\n",
            "训练和验证耗费的时间2m10s\n",
            "Epoch 226/499\n",
            "----------\n",
            "226 train loss:1.0031 train acc: 0.7382\n",
            "226 val loss:1.0547 val acc: 0.6892\n",
            "训练和验证耗费的时间2m11s\n",
            "Epoch 227/499\n",
            "----------\n",
            "227 train loss:1.0023 train acc: 0.7348\n",
            "227 val loss:1.0541 val acc: 0.6776\n",
            "训练和验证耗费的时间2m11s\n",
            "Epoch 228/499\n",
            "----------\n",
            "228 train loss:1.0007 train acc: 0.7363\n",
            "228 val loss:1.0590 val acc: 0.6718\n",
            "训练和验证耗费的时间2m12s\n",
            "Epoch 229/499\n",
            "----------\n",
            "229 train loss:0.9920 train acc: 0.7459\n",
            "229 val loss:1.0496 val acc: 0.6815\n",
            "训练和验证耗费的时间2m12s\n",
            "Epoch 230/499\n",
            "----------\n",
            "230 train loss:0.9896 train acc: 0.7522\n",
            "230 val loss:1.0484 val acc: 0.6911\n",
            "训练和验证耗费的时间2m13s\n",
            "Epoch 231/499\n",
            "----------\n",
            "231 train loss:0.9850 train acc: 0.7560\n",
            "231 val loss:1.0522 val acc: 0.6834\n",
            "训练和验证耗费的时间2m13s\n",
            "Epoch 232/499\n",
            "----------\n",
            "232 train loss:1.0012 train acc: 0.7377\n",
            "232 val loss:1.0463 val acc: 0.6931\n",
            "训练和验证耗费的时间2m14s\n",
            "Epoch 233/499\n",
            "----------\n",
            "233 train loss:0.9894 train acc: 0.7527\n",
            "233 val loss:1.0599 val acc: 0.6795\n",
            "训练和验证耗费的时间2m15s\n",
            "Epoch 234/499\n",
            "----------\n",
            "234 train loss:0.9987 train acc: 0.7425\n",
            "234 val loss:1.0507 val acc: 0.6873\n",
            "训练和验证耗费的时间2m15s\n",
            "Epoch 235/499\n",
            "----------\n",
            "235 train loss:0.9982 train acc: 0.7420\n",
            "235 val loss:1.0465 val acc: 0.6988\n",
            "训练和验证耗费的时间2m16s\n",
            "Epoch 236/499\n",
            "----------\n",
            "236 train loss:1.0073 train acc: 0.7295\n",
            "236 val loss:1.0466 val acc: 0.6969\n",
            "训练和验证耗费的时间2m16s\n",
            "Epoch 237/499\n",
            "----------\n",
            "237 train loss:0.9848 train acc: 0.7594\n",
            "237 val loss:1.0516 val acc: 0.6853\n",
            "训练和验证耗费的时间2m17s\n",
            "Epoch 238/499\n",
            "----------\n",
            "238 train loss:0.9939 train acc: 0.7512\n",
            "238 val loss:1.0525 val acc: 0.6815\n",
            "训练和验证耗费的时间2m17s\n",
            "Epoch 239/499\n",
            "----------\n",
            "239 train loss:1.0011 train acc: 0.7377\n",
            "239 val loss:1.0506 val acc: 0.6853\n",
            "训练和验证耗费的时间2m18s\n",
            "Epoch 240/499\n",
            "----------\n",
            "240 train loss:1.0008 train acc: 0.7435\n",
            "240 val loss:1.0452 val acc: 0.6931\n",
            "训练和验证耗费的时间2m19s\n",
            "Epoch 241/499\n",
            "----------\n",
            "241 train loss:0.9983 train acc: 0.7449\n",
            "241 val loss:1.0501 val acc: 0.6873\n",
            "训练和验证耗费的时间2m19s\n",
            "Epoch 242/499\n",
            "----------\n",
            "242 train loss:0.9951 train acc: 0.7411\n",
            "242 val loss:1.0424 val acc: 0.6853\n",
            "训练和验证耗费的时间2m20s\n",
            "Epoch 243/499\n",
            "----------\n",
            "243 train loss:0.9886 train acc: 0.7546\n",
            "243 val loss:1.0537 val acc: 0.6873\n",
            "训练和验证耗费的时间2m20s\n",
            "Epoch 244/499\n",
            "----------\n",
            "244 train loss:0.9920 train acc: 0.7454\n",
            "244 val loss:1.0415 val acc: 0.6988\n",
            "训练和验证耗费的时间2m21s\n",
            "Epoch 245/499\n",
            "----------\n",
            "245 train loss:0.9985 train acc: 0.7416\n",
            "245 val loss:1.0482 val acc: 0.6950\n",
            "训练和验证耗费的时间2m22s\n",
            "Epoch 246/499\n",
            "----------\n",
            "246 train loss:1.0000 train acc: 0.7445\n",
            "246 val loss:1.0487 val acc: 0.6815\n",
            "训练和验证耗费的时间2m22s\n",
            "Epoch 247/499\n",
            "----------\n",
            "247 train loss:0.9954 train acc: 0.7488\n",
            "247 val loss:1.0528 val acc: 0.6853\n",
            "训练和验证耗费的时间2m23s\n",
            "Epoch 248/499\n",
            "----------\n",
            "248 train loss:0.9919 train acc: 0.7498\n",
            "248 val loss:1.0551 val acc: 0.6776\n",
            "训练和验证耗费的时间2m23s\n",
            "Epoch 249/499\n",
            "----------\n",
            "249 train loss:0.9934 train acc: 0.7478\n",
            "249 val loss:1.0572 val acc: 0.6795\n",
            "训练和验证耗费的时间2m24s\n",
            "Epoch 250/499\n",
            "----------\n",
            "250 train loss:0.9925 train acc: 0.7498\n",
            "250 val loss:1.0508 val acc: 0.6815\n",
            "训练和验证耗费的时间2m24s\n",
            "Epoch 251/499\n",
            "----------\n",
            "251 train loss:0.9929 train acc: 0.7449\n",
            "251 val loss:1.0467 val acc: 0.6969\n",
            "训练和验证耗费的时间2m25s\n",
            "Epoch 252/499\n",
            "----------\n",
            "252 train loss:0.9879 train acc: 0.7536\n",
            "252 val loss:1.0568 val acc: 0.6757\n",
            "训练和验证耗费的时间2m26s\n",
            "Epoch 253/499\n",
            "----------\n",
            "253 train loss:0.9962 train acc: 0.7420\n",
            "253 val loss:1.0454 val acc: 0.6988\n",
            "训练和验证耗费的时间2m26s\n",
            "Epoch 254/499\n",
            "----------\n",
            "254 train loss:0.9932 train acc: 0.7440\n",
            "254 val loss:1.0551 val acc: 0.6815\n",
            "训练和验证耗费的时间2m27s\n",
            "Epoch 255/499\n",
            "----------\n",
            "255 train loss:0.9961 train acc: 0.7469\n",
            "255 val loss:1.0497 val acc: 0.6776\n",
            "训练和验证耗费的时间2m27s\n",
            "Epoch 256/499\n",
            "----------\n",
            "256 train loss:0.9943 train acc: 0.7445\n",
            "256 val loss:1.0488 val acc: 0.6892\n",
            "训练和验证耗费的时间2m28s\n",
            "Epoch 257/499\n",
            "----------\n",
            "257 train loss:0.9954 train acc: 0.7473\n",
            "257 val loss:1.0490 val acc: 0.6873\n",
            "训练和验证耗费的时间2m28s\n",
            "Epoch 258/499\n",
            "----------\n",
            "258 train loss:0.9955 train acc: 0.7454\n",
            "258 val loss:1.0527 val acc: 0.6911\n",
            "训练和验证耗费的时间2m29s\n",
            "Epoch 259/499\n",
            "----------\n",
            "259 train loss:0.9884 train acc: 0.7488\n",
            "259 val loss:1.0461 val acc: 0.6950\n",
            "训练和验证耗费的时间2m30s\n",
            "Epoch 260/499\n",
            "----------\n",
            "260 train loss:0.9988 train acc: 0.7464\n",
            "260 val loss:1.0521 val acc: 0.6815\n",
            "训练和验证耗费的时间2m30s\n",
            "Epoch 261/499\n",
            "----------\n",
            "261 train loss:0.9982 train acc: 0.7334\n",
            "261 val loss:1.0516 val acc: 0.6892\n",
            "训练和验证耗费的时间2m31s\n",
            "Epoch 262/499\n",
            "----------\n",
            "262 train loss:0.9925 train acc: 0.7507\n",
            "262 val loss:1.0464 val acc: 0.6969\n",
            "训练和验证耗费的时间2m31s\n",
            "Epoch 263/499\n",
            "----------\n",
            "263 train loss:0.9910 train acc: 0.7459\n",
            "263 val loss:1.0437 val acc: 0.7027\n",
            "训练和验证耗费的时间2m32s\n",
            "Epoch 264/499\n",
            "----------\n",
            "264 train loss:0.9913 train acc: 0.7478\n",
            "264 val loss:1.0502 val acc: 0.6795\n",
            "训练和验证耗费的时间2m32s\n",
            "Epoch 265/499\n",
            "----------\n",
            "265 train loss:0.9814 train acc: 0.7560\n",
            "265 val loss:1.0402 val acc: 0.6892\n",
            "训练和验证耗费的时间2m33s\n",
            "Epoch 266/499\n",
            "----------\n",
            "266 train loss:0.9920 train acc: 0.7488\n",
            "266 val loss:1.0462 val acc: 0.6873\n",
            "训练和验证耗费的时间2m34s\n",
            "Epoch 267/499\n",
            "----------\n",
            "267 train loss:0.9998 train acc: 0.7343\n",
            "267 val loss:1.0457 val acc: 0.6969\n",
            "训练和验证耗费的时间2m34s\n",
            "Epoch 268/499\n",
            "----------\n",
            "268 train loss:0.9873 train acc: 0.7541\n",
            "268 val loss:1.0515 val acc: 0.6834\n",
            "训练和验证耗费的时间2m35s\n",
            "Epoch 269/499\n",
            "----------\n",
            "269 train loss:0.9945 train acc: 0.7416\n",
            "269 val loss:1.0514 val acc: 0.6873\n",
            "训练和验证耗费的时间2m35s\n",
            "Epoch 270/499\n",
            "----------\n",
            "270 train loss:0.9945 train acc: 0.7420\n",
            "270 val loss:1.0470 val acc: 0.6988\n",
            "训练和验证耗费的时间2m36s\n",
            "Epoch 271/499\n",
            "----------\n",
            "271 train loss:0.9997 train acc: 0.7382\n",
            "271 val loss:1.0472 val acc: 0.7027\n",
            "训练和验证耗费的时间2m36s\n",
            "Epoch 272/499\n",
            "----------\n",
            "272 train loss:0.9877 train acc: 0.7483\n",
            "272 val loss:1.0497 val acc: 0.6931\n",
            "训练和验证耗费的时间2m37s\n",
            "Epoch 273/499\n",
            "----------\n",
            "273 train loss:0.9873 train acc: 0.7565\n",
            "273 val loss:1.0492 val acc: 0.6969\n",
            "训练和验证耗费的时间2m38s\n",
            "Epoch 274/499\n",
            "----------\n",
            "274 train loss:0.9873 train acc: 0.7522\n",
            "274 val loss:1.0459 val acc: 0.6988\n",
            "训练和验证耗费的时间2m38s\n",
            "Epoch 275/499\n",
            "----------\n",
            "275 train loss:0.9991 train acc: 0.7396\n",
            "275 val loss:1.0501 val acc: 0.6911\n",
            "训练和验证耗费的时间2m39s\n",
            "Epoch 276/499\n",
            "----------\n",
            "276 train loss:0.9905 train acc: 0.7483\n",
            "276 val loss:1.0544 val acc: 0.6873\n",
            "训练和验证耗费的时间2m39s\n",
            "Epoch 277/499\n",
            "----------\n",
            "277 train loss:0.9939 train acc: 0.7459\n",
            "277 val loss:1.0474 val acc: 0.6815\n",
            "训练和验证耗费的时间2m40s\n",
            "Epoch 278/499\n",
            "----------\n",
            "278 train loss:1.0032 train acc: 0.7348\n",
            "278 val loss:1.0512 val acc: 0.6931\n",
            "训练和验证耗费的时间2m40s\n",
            "Epoch 279/499\n",
            "----------\n",
            "279 train loss:0.9820 train acc: 0.7570\n",
            "279 val loss:1.0399 val acc: 0.7085\n",
            "训练和验证耗费的时间2m41s\n",
            "Epoch 280/499\n",
            "----------\n",
            "280 train loss:0.9799 train acc: 0.7584\n",
            "280 val loss:1.0471 val acc: 0.6950\n",
            "训练和验证耗费的时间2m42s\n",
            "Epoch 281/499\n",
            "----------\n",
            "281 train loss:0.9838 train acc: 0.7560\n",
            "281 val loss:1.0479 val acc: 0.6873\n",
            "训练和验证耗费的时间2m42s\n",
            "Epoch 282/499\n",
            "----------\n",
            "282 train loss:0.9895 train acc: 0.7531\n",
            "282 val loss:1.0528 val acc: 0.6873\n",
            "训练和验证耗费的时间2m43s\n",
            "Epoch 283/499\n",
            "----------\n",
            "283 train loss:0.9808 train acc: 0.7584\n",
            "283 val loss:1.0435 val acc: 0.6950\n",
            "训练和验证耗费的时间2m43s\n",
            "Epoch 284/499\n",
            "----------\n",
            "284 train loss:0.9860 train acc: 0.7555\n",
            "284 val loss:1.0534 val acc: 0.6795\n",
            "训练和验证耗费的时间2m44s\n",
            "Epoch 285/499\n",
            "----------\n",
            "285 train loss:0.9849 train acc: 0.7584\n",
            "285 val loss:1.0529 val acc: 0.6815\n",
            "训练和验证耗费的时间2m44s\n",
            "Epoch 286/499\n",
            "----------\n",
            "286 train loss:0.9947 train acc: 0.7416\n",
            "286 val loss:1.0547 val acc: 0.6757\n",
            "训练和验证耗费的时间2m45s\n",
            "Epoch 287/499\n",
            "----------\n",
            "287 train loss:0.9845 train acc: 0.7546\n",
            "287 val loss:1.0459 val acc: 0.7008\n",
            "训练和验证耗费的时间2m46s\n",
            "Epoch 288/499\n",
            "----------\n",
            "288 train loss:0.9838 train acc: 0.7604\n",
            "288 val loss:1.0416 val acc: 0.6950\n",
            "训练和验证耗费的时间2m46s\n",
            "Epoch 289/499\n",
            "----------\n",
            "289 train loss:0.9877 train acc: 0.7493\n",
            "289 val loss:1.0451 val acc: 0.6853\n",
            "训练和验证耗费的时间2m47s\n",
            "Epoch 290/499\n",
            "----------\n",
            "290 train loss:0.9904 train acc: 0.7498\n",
            "290 val loss:1.0440 val acc: 0.6950\n",
            "训练和验证耗费的时间2m47s\n",
            "Epoch 291/499\n",
            "----------\n",
            "291 train loss:0.9836 train acc: 0.7589\n",
            "291 val loss:1.0421 val acc: 0.6988\n",
            "训练和验证耗费的时间2m48s\n",
            "Epoch 292/499\n",
            "----------\n",
            "292 train loss:0.9888 train acc: 0.7527\n",
            "292 val loss:1.0554 val acc: 0.6834\n",
            "训练和验证耗费的时间2m48s\n",
            "Epoch 293/499\n",
            "----------\n",
            "293 train loss:0.9919 train acc: 0.7454\n",
            "293 val loss:1.0435 val acc: 0.6969\n",
            "训练和验证耗费的时间2m49s\n",
            "Epoch 294/499\n",
            "----------\n",
            "294 train loss:0.9940 train acc: 0.7387\n",
            "294 val loss:1.0453 val acc: 0.6892\n",
            "训练和验证耗费的时间2m50s\n",
            "Epoch 295/499\n",
            "----------\n",
            "295 train loss:0.9847 train acc: 0.7560\n",
            "295 val loss:1.0455 val acc: 0.6911\n",
            "训练和验证耗费的时间2m50s\n",
            "Epoch 296/499\n",
            "----------\n",
            "296 train loss:0.9980 train acc: 0.7396\n",
            "296 val loss:1.0523 val acc: 0.6931\n",
            "训练和验证耗费的时间2m51s\n",
            "Epoch 297/499\n",
            "----------\n",
            "297 train loss:0.9922 train acc: 0.7483\n",
            "297 val loss:1.0490 val acc: 0.6911\n",
            "训练和验证耗费的时间2m51s\n",
            "Epoch 298/499\n",
            "----------\n",
            "298 train loss:0.9946 train acc: 0.7459\n",
            "298 val loss:1.0472 val acc: 0.6969\n",
            "训练和验证耗费的时间2m52s\n",
            "Epoch 299/499\n",
            "----------\n",
            "299 train loss:0.9849 train acc: 0.7536\n",
            "299 val loss:1.0403 val acc: 0.6969\n",
            "训练和验证耗费的时间2m53s\n",
            "Epoch 300/499\n",
            "----------\n",
            "300 train loss:0.9850 train acc: 0.7570\n",
            "300 val loss:1.0396 val acc: 0.6988\n",
            "训练和验证耗费的时间2m53s\n",
            "Epoch 301/499\n",
            "----------\n",
            "301 train loss:0.9879 train acc: 0.7512\n",
            "301 val loss:1.0384 val acc: 0.7085\n",
            "训练和验证耗费的时间2m54s\n",
            "Epoch 302/499\n",
            "----------\n",
            "302 train loss:0.9958 train acc: 0.7396\n",
            "302 val loss:1.0404 val acc: 0.6950\n",
            "训练和验证耗费的时间2m54s\n",
            "Epoch 303/499\n",
            "----------\n",
            "303 train loss:0.9892 train acc: 0.7488\n",
            "303 val loss:1.0476 val acc: 0.6911\n",
            "训练和验证耗费的时间2m55s\n",
            "Epoch 304/499\n",
            "----------\n",
            "304 train loss:0.9800 train acc: 0.7599\n",
            "304 val loss:1.0432 val acc: 0.6892\n",
            "训练和验证耗费的时间2m55s\n",
            "Epoch 305/499\n",
            "----------\n",
            "305 train loss:0.9825 train acc: 0.7546\n",
            "305 val loss:1.0446 val acc: 0.6815\n",
            "训练和验证耗费的时间2m56s\n",
            "Epoch 306/499\n",
            "----------\n",
            "306 train loss:0.9951 train acc: 0.7449\n",
            "306 val loss:1.0470 val acc: 0.6853\n",
            "训练和验证耗费的时间2m57s\n",
            "Epoch 307/499\n",
            "----------\n",
            "307 train loss:0.9795 train acc: 0.7570\n",
            "307 val loss:1.0529 val acc: 0.6873\n",
            "训练和验证耗费的时间2m57s\n",
            "Epoch 308/499\n",
            "----------\n",
            "308 train loss:0.9890 train acc: 0.7445\n",
            "308 val loss:1.0412 val acc: 0.6988\n",
            "训练和验证耗费的时间2m58s\n",
            "Epoch 309/499\n",
            "----------\n",
            "309 train loss:0.9798 train acc: 0.7589\n",
            "309 val loss:1.0427 val acc: 0.6988\n",
            "训练和验证耗费的时间2m58s\n",
            "Epoch 310/499\n",
            "----------\n",
            "310 train loss:0.9850 train acc: 0.7512\n",
            "310 val loss:1.0431 val acc: 0.6988\n",
            "训练和验证耗费的时间2m59s\n",
            "Epoch 311/499\n",
            "----------\n",
            "311 train loss:0.9858 train acc: 0.7546\n",
            "311 val loss:1.0396 val acc: 0.6931\n",
            "训练和验证耗费的时间2m60s\n",
            "Epoch 312/499\n",
            "----------\n",
            "312 train loss:0.9897 train acc: 0.7507\n",
            "312 val loss:1.0436 val acc: 0.7066\n",
            "训练和验证耗费的时间3m0s\n",
            "Epoch 313/499\n",
            "----------\n",
            "313 train loss:0.9891 train acc: 0.7469\n",
            "313 val loss:1.0539 val acc: 0.6815\n",
            "训练和验证耗费的时间3m1s\n",
            "Epoch 314/499\n",
            "----------\n",
            "314 train loss:0.9892 train acc: 0.7483\n",
            "314 val loss:1.0457 val acc: 0.7008\n",
            "训练和验证耗费的时间3m1s\n",
            "Epoch 315/499\n",
            "----------\n",
            "315 train loss:0.9813 train acc: 0.7604\n",
            "315 val loss:1.0437 val acc: 0.7046\n",
            "训练和验证耗费的时间3m2s\n",
            "Epoch 316/499\n",
            "----------\n",
            "316 train loss:0.9874 train acc: 0.7498\n",
            "316 val loss:1.0531 val acc: 0.6853\n",
            "训练和验证耗费的时间3m2s\n",
            "Epoch 317/499\n",
            "----------\n",
            "317 train loss:0.9788 train acc: 0.7652\n",
            "317 val loss:1.0411 val acc: 0.7027\n",
            "训练和验证耗费的时间3m3s\n",
            "Epoch 318/499\n",
            "----------\n",
            "318 train loss:0.9869 train acc: 0.7493\n",
            "318 val loss:1.0452 val acc: 0.6892\n",
            "训练和验证耗费的时间3m4s\n",
            "Epoch 319/499\n",
            "----------\n",
            "319 train loss:0.9884 train acc: 0.7531\n",
            "319 val loss:1.0391 val acc: 0.7008\n",
            "训练和验证耗费的时间3m4s\n",
            "Epoch 320/499\n",
            "----------\n",
            "320 train loss:0.9885 train acc: 0.7469\n",
            "320 val loss:1.0487 val acc: 0.6911\n",
            "训练和验证耗费的时间3m5s\n",
            "Epoch 321/499\n",
            "----------\n",
            "321 train loss:0.9946 train acc: 0.7454\n",
            "321 val loss:1.0568 val acc: 0.6795\n",
            "训练和验证耗费的时间3m5s\n",
            "Epoch 322/499\n",
            "----------\n",
            "322 train loss:1.0033 train acc: 0.7343\n",
            "322 val loss:1.0442 val acc: 0.6950\n",
            "训练和验证耗费的时间3m6s\n",
            "Epoch 323/499\n",
            "----------\n",
            "323 train loss:0.9736 train acc: 0.7686\n",
            "323 val loss:1.0413 val acc: 0.6969\n",
            "训练和验证耗费的时间3m6s\n",
            "Epoch 324/499\n",
            "----------\n",
            "324 train loss:0.9792 train acc: 0.7623\n",
            "324 val loss:1.0444 val acc: 0.6950\n",
            "训练和验证耗费的时间3m7s\n",
            "Epoch 325/499\n",
            "----------\n",
            "325 train loss:0.9710 train acc: 0.7700\n",
            "325 val loss:1.0451 val acc: 0.6873\n",
            "训练和验证耗费的时间3m8s\n",
            "Epoch 326/499\n",
            "----------\n",
            "326 train loss:0.9931 train acc: 0.7478\n",
            "326 val loss:1.0444 val acc: 0.6931\n",
            "训练和验证耗费的时间3m8s\n",
            "Epoch 327/499\n",
            "----------\n",
            "327 train loss:0.9903 train acc: 0.7502\n",
            "327 val loss:1.0465 val acc: 0.6931\n",
            "训练和验证耗费的时间3m9s\n",
            "Epoch 328/499\n",
            "----------\n",
            "328 train loss:0.9797 train acc: 0.7657\n",
            "328 val loss:1.0420 val acc: 0.6931\n",
            "训练和验证耗费的时间3m9s\n",
            "Epoch 329/499\n",
            "----------\n",
            "329 train loss:0.9931 train acc: 0.7454\n",
            "329 val loss:1.0506 val acc: 0.6950\n",
            "训练和验证耗费的时间3m10s\n",
            "Epoch 330/499\n",
            "----------\n",
            "330 train loss:0.9815 train acc: 0.7570\n",
            "330 val loss:1.0453 val acc: 0.6873\n",
            "训练和验证耗费的时间3m11s\n",
            "Epoch 331/499\n",
            "----------\n",
            "331 train loss:0.9842 train acc: 0.7522\n",
            "331 val loss:1.0474 val acc: 0.6911\n",
            "训练和验证耗费的时间3m11s\n",
            "Epoch 332/499\n",
            "----------\n",
            "332 train loss:0.9812 train acc: 0.7551\n",
            "332 val loss:1.0449 val acc: 0.6776\n",
            "训练和验证耗费的时间3m12s\n",
            "Epoch 333/499\n",
            "----------\n",
            "333 train loss:0.9863 train acc: 0.7522\n",
            "333 val loss:1.0415 val acc: 0.6911\n",
            "训练和验证耗费的时间3m12s\n",
            "Epoch 334/499\n",
            "----------\n",
            "334 train loss:0.9995 train acc: 0.7314\n",
            "334 val loss:1.0438 val acc: 0.6931\n",
            "训练和验证耗费的时间3m13s\n",
            "Epoch 335/499\n",
            "----------\n",
            "335 train loss:0.9717 train acc: 0.7676\n",
            "335 val loss:1.0495 val acc: 0.6873\n",
            "训练和验证耗费的时间3m13s\n",
            "Epoch 336/499\n",
            "----------\n",
            "336 train loss:0.9855 train acc: 0.7570\n",
            "336 val loss:1.0397 val acc: 0.6988\n",
            "训练和验证耗费的时间3m14s\n",
            "Epoch 337/499\n",
            "----------\n",
            "337 train loss:0.9884 train acc: 0.7498\n",
            "337 val loss:1.0449 val acc: 0.6853\n",
            "训练和验证耗费的时间3m15s\n",
            "Epoch 338/499\n",
            "----------\n",
            "338 train loss:0.9850 train acc: 0.7584\n",
            "338 val loss:1.0499 val acc: 0.6834\n",
            "训练和验证耗费的时间3m15s\n",
            "Epoch 339/499\n",
            "----------\n",
            "339 train loss:0.9900 train acc: 0.7459\n",
            "339 val loss:1.0455 val acc: 0.6969\n",
            "训练和验证耗费的时间3m16s\n",
            "Epoch 340/499\n",
            "----------\n",
            "340 train loss:0.9697 train acc: 0.7700\n",
            "340 val loss:1.0402 val acc: 0.6988\n",
            "训练和验证耗费的时间3m16s\n",
            "Epoch 341/499\n",
            "----------\n",
            "341 train loss:0.9877 train acc: 0.7522\n",
            "341 val loss:1.0469 val acc: 0.6892\n",
            "训练和验证耗费的时间3m17s\n",
            "Epoch 342/499\n",
            "----------\n",
            "342 train loss:0.9792 train acc: 0.7575\n",
            "342 val loss:1.0412 val acc: 0.6969\n",
            "训练和验证耗费的时间3m17s\n",
            "Epoch 343/499\n",
            "----------\n",
            "343 train loss:0.9831 train acc: 0.7541\n",
            "343 val loss:1.0453 val acc: 0.6931\n",
            "训练和验证耗费的时间3m18s\n",
            "Epoch 344/499\n",
            "----------\n",
            "344 train loss:0.9866 train acc: 0.7531\n",
            "344 val loss:1.0486 val acc: 0.6911\n",
            "训练和验证耗费的时间3m18s\n",
            "Epoch 345/499\n",
            "----------\n",
            "345 train loss:0.9776 train acc: 0.7642\n",
            "345 val loss:1.0462 val acc: 0.6892\n",
            "训练和验证耗费的时间3m19s\n",
            "Epoch 346/499\n",
            "----------\n",
            "346 train loss:0.9829 train acc: 0.7502\n",
            "346 val loss:1.0380 val acc: 0.6988\n",
            "训练和验证耗费的时间3m20s\n",
            "Epoch 347/499\n",
            "----------\n",
            "347 train loss:0.9832 train acc: 0.7575\n",
            "347 val loss:1.0441 val acc: 0.6950\n",
            "训练和验证耗费的时间3m20s\n",
            "Epoch 348/499\n",
            "----------\n",
            "348 train loss:0.9778 train acc: 0.7628\n",
            "348 val loss:1.0404 val acc: 0.6911\n",
            "训练和验证耗费的时间3m21s\n",
            "Epoch 349/499\n",
            "----------\n",
            "349 train loss:0.9709 train acc: 0.7743\n",
            "349 val loss:1.0384 val acc: 0.6911\n",
            "训练和验证耗费的时间3m21s\n",
            "Epoch 350/499\n",
            "----------\n",
            "350 train loss:0.9833 train acc: 0.7589\n",
            "350 val loss:1.0401 val acc: 0.6892\n",
            "训练和验证耗费的时间3m22s\n",
            "Epoch 351/499\n",
            "----------\n",
            "351 train loss:0.9806 train acc: 0.7604\n",
            "351 val loss:1.0334 val acc: 0.6988\n",
            "训练和验证耗费的时间3m23s\n",
            "Epoch 352/499\n",
            "----------\n",
            "352 train loss:0.9715 train acc: 0.7695\n",
            "352 val loss:1.0407 val acc: 0.6911\n",
            "训练和验证耗费的时间3m23s\n",
            "Epoch 353/499\n",
            "----------\n",
            "353 train loss:0.9859 train acc: 0.7527\n",
            "353 val loss:1.0418 val acc: 0.6969\n",
            "训练和验证耗费的时间3m24s\n",
            "Epoch 354/499\n",
            "----------\n",
            "354 train loss:0.9898 train acc: 0.7440\n",
            "354 val loss:1.0412 val acc: 0.6988\n",
            "训练和验证耗费的时间3m24s\n",
            "Epoch 355/499\n",
            "----------\n",
            "355 train loss:0.9803 train acc: 0.7575\n",
            "355 val loss:1.0432 val acc: 0.6892\n",
            "训练和验证耗费的时间3m25s\n",
            "Epoch 356/499\n",
            "----------\n",
            "356 train loss:0.9897 train acc: 0.7498\n",
            "356 val loss:1.0375 val acc: 0.6988\n",
            "训练和验证耗费的时间3m25s\n",
            "Epoch 357/499\n",
            "----------\n",
            "357 train loss:0.9818 train acc: 0.7580\n",
            "357 val loss:1.0441 val acc: 0.6988\n",
            "训练和验证耗费的时间3m26s\n",
            "Epoch 358/499\n",
            "----------\n",
            "358 train loss:0.9783 train acc: 0.7671\n",
            "358 val loss:1.0399 val acc: 0.7046\n",
            "训练和验证耗费的时间3m27s\n",
            "Epoch 359/499\n",
            "----------\n",
            "359 train loss:0.9820 train acc: 0.7565\n",
            "359 val loss:1.0351 val acc: 0.7027\n",
            "训练和验证耗费的时间3m27s\n",
            "Epoch 360/499\n",
            "----------\n",
            "360 train loss:0.9890 train acc: 0.7507\n",
            "360 val loss:1.0386 val acc: 0.7085\n",
            "训练和验证耗费的时间3m28s\n",
            "Epoch 361/499\n",
            "----------\n",
            "361 train loss:0.9776 train acc: 0.7666\n",
            "361 val loss:1.0357 val acc: 0.7066\n",
            "训练和验证耗费的时间3m28s\n",
            "Epoch 362/499\n",
            "----------\n",
            "362 train loss:0.9707 train acc: 0.7719\n",
            "362 val loss:1.0316 val acc: 0.7220\n",
            "训练和验证耗费的时间3m29s\n",
            "Epoch 363/499\n",
            "----------\n",
            "363 train loss:0.9827 train acc: 0.7551\n",
            "363 val loss:1.0317 val acc: 0.7104\n",
            "训练和验证耗费的时间3m29s\n",
            "Epoch 364/499\n",
            "----------\n",
            "364 train loss:0.9823 train acc: 0.7551\n",
            "364 val loss:1.0356 val acc: 0.7085\n",
            "训练和验证耗费的时间3m30s\n",
            "Epoch 365/499\n",
            "----------\n",
            "365 train loss:0.9760 train acc: 0.7608\n",
            "365 val loss:1.0349 val acc: 0.6931\n",
            "训练和验证耗费的时间3m31s\n",
            "Epoch 366/499\n",
            "----------\n",
            "366 train loss:0.9837 train acc: 0.7570\n",
            "366 val loss:1.0368 val acc: 0.7046\n",
            "训练和验证耗费的时间3m31s\n",
            "Epoch 367/499\n",
            "----------\n",
            "367 train loss:0.9726 train acc: 0.7671\n",
            "367 val loss:1.0389 val acc: 0.7046\n",
            "训练和验证耗费的时间3m32s\n",
            "Epoch 368/499\n",
            "----------\n",
            "368 train loss:0.9778 train acc: 0.7623\n",
            "368 val loss:1.0422 val acc: 0.6834\n",
            "训练和验证耗费的时间3m32s\n",
            "Epoch 369/499\n",
            "----------\n",
            "369 train loss:0.9735 train acc: 0.7671\n",
            "369 val loss:1.0377 val acc: 0.7008\n",
            "训练和验证耗费的时间3m33s\n",
            "Epoch 370/499\n",
            "----------\n",
            "370 train loss:0.9788 train acc: 0.7584\n",
            "370 val loss:1.0410 val acc: 0.6950\n",
            "训练和验证耗费的时间3m34s\n",
            "Epoch 371/499\n",
            "----------\n",
            "371 train loss:0.9840 train acc: 0.7507\n",
            "371 val loss:1.0386 val acc: 0.6988\n",
            "训练和验证耗费的时间3m34s\n",
            "Epoch 372/499\n",
            "----------\n",
            "372 train loss:0.9786 train acc: 0.7628\n",
            "372 val loss:1.0393 val acc: 0.6950\n",
            "训练和验证耗费的时间3m35s\n",
            "Epoch 373/499\n",
            "----------\n",
            "373 train loss:0.9777 train acc: 0.7570\n",
            "373 val loss:1.0337 val acc: 0.7104\n",
            "训练和验证耗费的时间3m35s\n",
            "Epoch 374/499\n",
            "----------\n",
            "374 train loss:0.9820 train acc: 0.7628\n",
            "374 val loss:1.0434 val acc: 0.6931\n",
            "训练和验证耗费的时间3m36s\n",
            "Epoch 375/499\n",
            "----------\n",
            "375 train loss:0.9764 train acc: 0.7662\n",
            "375 val loss:1.0354 val acc: 0.6988\n",
            "训练和验证耗费的时间3m36s\n",
            "Epoch 376/499\n",
            "----------\n",
            "376 train loss:0.9716 train acc: 0.7739\n",
            "376 val loss:1.0375 val acc: 0.7104\n",
            "训练和验证耗费的时间3m37s\n",
            "Epoch 377/499\n",
            "----------\n",
            "377 train loss:0.9777 train acc: 0.7662\n",
            "377 val loss:1.0370 val acc: 0.7008\n",
            "训练和验证耗费的时间3m38s\n",
            "Epoch 378/499\n",
            "----------\n",
            "378 train loss:0.9848 train acc: 0.7517\n",
            "378 val loss:1.0368 val acc: 0.7066\n",
            "训练和验证耗费的时间3m38s\n",
            "Epoch 379/499\n",
            "----------\n",
            "379 train loss:0.9873 train acc: 0.7507\n",
            "379 val loss:1.0378 val acc: 0.7027\n",
            "训练和验证耗费的时间3m39s\n",
            "Epoch 380/499\n",
            "----------\n",
            "380 train loss:0.9729 train acc: 0.7686\n",
            "380 val loss:1.0404 val acc: 0.7066\n",
            "训练和验证耗费的时间3m39s\n",
            "Epoch 381/499\n",
            "----------\n",
            "381 train loss:0.9662 train acc: 0.7763\n",
            "381 val loss:1.0429 val acc: 0.6931\n",
            "训练和验证耗费的时间3m40s\n",
            "Epoch 382/499\n",
            "----------\n",
            "382 train loss:0.9861 train acc: 0.7488\n",
            "382 val loss:1.0482 val acc: 0.6795\n",
            "训练和验证耗费的时间3m40s\n",
            "Epoch 383/499\n",
            "----------\n",
            "383 train loss:0.9855 train acc: 0.7555\n",
            "383 val loss:1.0408 val acc: 0.7027\n",
            "训练和验证耗费的时间3m41s\n",
            "Epoch 384/499\n",
            "----------\n",
            "384 train loss:0.9756 train acc: 0.7637\n",
            "384 val loss:1.0369 val acc: 0.6988\n",
            "训练和验证耗费的时间3m42s\n",
            "Epoch 385/499\n",
            "----------\n",
            "385 train loss:0.9807 train acc: 0.7594\n",
            "385 val loss:1.0431 val acc: 0.6911\n",
            "训练和验证耗费的时间3m42s\n",
            "Epoch 386/499\n",
            "----------\n",
            "386 train loss:0.9778 train acc: 0.7580\n",
            "386 val loss:1.0399 val acc: 0.6931\n",
            "训练和验证耗费的时间3m43s\n",
            "Epoch 387/499\n",
            "----------\n",
            "387 train loss:0.9869 train acc: 0.7512\n",
            "387 val loss:1.0387 val acc: 0.7027\n",
            "训练和验证耗费的时间3m43s\n",
            "Epoch 388/499\n",
            "----------\n",
            "388 train loss:0.9833 train acc: 0.7570\n",
            "388 val loss:1.0322 val acc: 0.7046\n",
            "训练和验证耗费的时间3m44s\n",
            "Epoch 389/499\n",
            "----------\n",
            "389 train loss:0.9787 train acc: 0.7642\n",
            "389 val loss:1.0436 val acc: 0.6853\n",
            "训练和验证耗费的时间3m44s\n",
            "Epoch 390/499\n",
            "----------\n",
            "390 train loss:0.9740 train acc: 0.7671\n",
            "390 val loss:1.0381 val acc: 0.6950\n",
            "训练和验证耗费的时间3m45s\n",
            "Epoch 391/499\n",
            "----------\n",
            "391 train loss:0.9802 train acc: 0.7580\n",
            "391 val loss:1.0344 val acc: 0.7046\n",
            "训练和验证耗费的时间3m46s\n",
            "Epoch 392/499\n",
            "----------\n",
            "392 train loss:0.9769 train acc: 0.7618\n",
            "392 val loss:1.0391 val acc: 0.6988\n",
            "训练和验证耗费的时间3m46s\n",
            "Epoch 393/499\n",
            "----------\n",
            "393 train loss:0.9666 train acc: 0.7797\n",
            "393 val loss:1.0336 val acc: 0.7027\n",
            "训练和验证耗费的时间3m47s\n",
            "Epoch 394/499\n",
            "----------\n",
            "394 train loss:0.9872 train acc: 0.7469\n",
            "394 val loss:1.0373 val acc: 0.7104\n",
            "训练和验证耗费的时间3m47s\n",
            "Epoch 395/499\n",
            "----------\n",
            "395 train loss:0.9738 train acc: 0.7642\n",
            "395 val loss:1.0340 val acc: 0.7104\n",
            "训练和验证耗费的时间3m48s\n",
            "Epoch 396/499\n",
            "----------\n",
            "396 train loss:0.9731 train acc: 0.7686\n",
            "396 val loss:1.0340 val acc: 0.7124\n",
            "训练和验证耗费的时间3m48s\n",
            "Epoch 397/499\n",
            "----------\n",
            "397 train loss:0.9686 train acc: 0.7690\n",
            "397 val loss:1.0370 val acc: 0.7008\n",
            "训练和验证耗费的时间3m49s\n",
            "Epoch 398/499\n",
            "----------\n",
            "398 train loss:0.9692 train acc: 0.7739\n",
            "398 val loss:1.0384 val acc: 0.7027\n",
            "训练和验证耗费的时间3m50s\n",
            "Epoch 399/499\n",
            "----------\n",
            "399 train loss:0.9792 train acc: 0.7642\n",
            "399 val loss:1.0385 val acc: 0.6969\n",
            "训练和验证耗费的时间3m50s\n",
            "Epoch 400/499\n",
            "----------\n",
            "400 train loss:0.9818 train acc: 0.7536\n",
            "400 val loss:1.0448 val acc: 0.6950\n",
            "训练和验证耗费的时间3m51s\n",
            "Epoch 401/499\n",
            "----------\n",
            "401 train loss:0.9771 train acc: 0.7628\n",
            "401 val loss:1.0373 val acc: 0.7085\n",
            "训练和验证耗费的时间3m51s\n",
            "Epoch 402/499\n",
            "----------\n",
            "402 train loss:0.9854 train acc: 0.7454\n",
            "402 val loss:1.0447 val acc: 0.6911\n",
            "训练和验证耗费的时间3m52s\n",
            "Epoch 403/499\n",
            "----------\n",
            "403 train loss:0.9704 train acc: 0.7690\n",
            "403 val loss:1.0401 val acc: 0.6988\n",
            "训练和验证耗费的时间3m53s\n",
            "Epoch 404/499\n",
            "----------\n",
            "404 train loss:0.9686 train acc: 0.7729\n",
            "404 val loss:1.0402 val acc: 0.6911\n",
            "训练和验证耗费的时间3m53s\n",
            "Epoch 405/499\n",
            "----------\n",
            "405 train loss:0.9694 train acc: 0.7739\n",
            "405 val loss:1.0397 val acc: 0.7066\n",
            "训练和验证耗费的时间3m54s\n",
            "Epoch 406/499\n",
            "----------\n",
            "406 train loss:0.9760 train acc: 0.7652\n",
            "406 val loss:1.0400 val acc: 0.7008\n",
            "训练和验证耗费的时间3m54s\n",
            "Epoch 407/499\n",
            "----------\n",
            "407 train loss:0.9788 train acc: 0.7613\n",
            "407 val loss:1.0392 val acc: 0.7008\n",
            "训练和验证耗费的时间3m55s\n",
            "Epoch 408/499\n",
            "----------\n",
            "408 train loss:0.9703 train acc: 0.7715\n",
            "408 val loss:1.0371 val acc: 0.7085\n",
            "训练和验证耗费的时间3m55s\n",
            "Epoch 409/499\n",
            "----------\n",
            "409 train loss:0.9803 train acc: 0.7570\n",
            "409 val loss:1.0353 val acc: 0.7085\n",
            "训练和验证耗费的时间3m56s\n",
            "Epoch 410/499\n",
            "----------\n",
            "410 train loss:0.9732 train acc: 0.7662\n",
            "410 val loss:1.0344 val acc: 0.7085\n",
            "训练和验证耗费的时间3m57s\n",
            "Epoch 411/499\n",
            "----------\n",
            "411 train loss:0.9847 train acc: 0.7536\n",
            "411 val loss:1.0333 val acc: 0.7181\n",
            "训练和验证耗费的时间3m57s\n",
            "Epoch 412/499\n",
            "----------\n",
            "412 train loss:0.9749 train acc: 0.7628\n",
            "412 val loss:1.0401 val acc: 0.6950\n",
            "训练和验证耗费的时间3m58s\n",
            "Epoch 413/499\n",
            "----------\n",
            "413 train loss:0.9748 train acc: 0.7662\n",
            "413 val loss:1.0375 val acc: 0.6969\n",
            "训练和验证耗费的时间3m58s\n",
            "Epoch 414/499\n",
            "----------\n",
            "414 train loss:0.9899 train acc: 0.7512\n",
            "414 val loss:1.0350 val acc: 0.6988\n",
            "训练和验证耗费的时间3m59s\n",
            "Epoch 415/499\n",
            "----------\n",
            "415 train loss:0.9694 train acc: 0.7710\n",
            "415 val loss:1.0387 val acc: 0.7027\n",
            "训练和验证耗费的时间3m60s\n",
            "Epoch 416/499\n",
            "----------\n",
            "416 train loss:0.9713 train acc: 0.7690\n",
            "416 val loss:1.0340 val acc: 0.7066\n",
            "训练和验证耗费的时间4m0s\n",
            "Epoch 417/499\n",
            "----------\n",
            "417 train loss:0.9752 train acc: 0.7647\n",
            "417 val loss:1.0395 val acc: 0.7008\n",
            "训练和验证耗费的时间4m1s\n",
            "Epoch 418/499\n",
            "----------\n",
            "418 train loss:0.9699 train acc: 0.7695\n",
            "418 val loss:1.0457 val acc: 0.6969\n",
            "训练和验证耗费的时间4m1s\n",
            "Epoch 419/499\n",
            "----------\n",
            "419 train loss:0.9765 train acc: 0.7613\n",
            "419 val loss:1.0336 val acc: 0.7085\n",
            "训练和验证耗费的时间4m2s\n",
            "Epoch 420/499\n",
            "----------\n",
            "420 train loss:0.9759 train acc: 0.7657\n",
            "420 val loss:1.0338 val acc: 0.7124\n",
            "训练和验证耗费的时间4m2s\n",
            "Epoch 421/499\n",
            "----------\n",
            "421 train loss:0.9803 train acc: 0.7570\n",
            "421 val loss:1.0350 val acc: 0.7085\n",
            "训练和验证耗费的时间4m3s\n",
            "Epoch 422/499\n",
            "----------\n",
            "422 train loss:0.9675 train acc: 0.7690\n",
            "422 val loss:1.0361 val acc: 0.7085\n",
            "训练和验证耗费的时间4m4s\n",
            "Epoch 423/499\n",
            "----------\n",
            "423 train loss:0.9749 train acc: 0.7671\n",
            "423 val loss:1.0366 val acc: 0.7008\n",
            "训练和验证耗费的时间4m4s\n",
            "Epoch 424/499\n",
            "----------\n",
            "424 train loss:0.9802 train acc: 0.7637\n",
            "424 val loss:1.0310 val acc: 0.7066\n",
            "训练和验证耗费的时间4m5s\n",
            "Epoch 425/499\n",
            "----------\n",
            "425 train loss:0.9739 train acc: 0.7681\n",
            "425 val loss:1.0339 val acc: 0.7066\n",
            "训练和验证耗费的时间4m5s\n",
            "Epoch 426/499\n",
            "----------\n",
            "426 train loss:0.9799 train acc: 0.7570\n",
            "426 val loss:1.0326 val acc: 0.7008\n",
            "训练和验证耗费的时间4m6s\n",
            "Epoch 427/499\n",
            "----------\n",
            "427 train loss:0.9755 train acc: 0.7628\n",
            "427 val loss:1.0296 val acc: 0.7143\n",
            "训练和验证耗费的时间4m6s\n",
            "Epoch 428/499\n",
            "----------\n",
            "428 train loss:0.9848 train acc: 0.7483\n",
            "428 val loss:1.0330 val acc: 0.7027\n",
            "训练和验证耗费的时间4m7s\n",
            "Epoch 429/499\n",
            "----------\n",
            "429 train loss:0.9778 train acc: 0.7652\n",
            "429 val loss:1.0302 val acc: 0.7104\n",
            "训练和验证耗费的时间4m8s\n",
            "Epoch 430/499\n",
            "----------\n",
            "430 train loss:0.9785 train acc: 0.7604\n",
            "430 val loss:1.0347 val acc: 0.7085\n",
            "训练和验证耗费的时间4m8s\n",
            "Epoch 431/499\n",
            "----------\n",
            "431 train loss:0.9716 train acc: 0.7686\n",
            "431 val loss:1.0378 val acc: 0.6931\n",
            "训练和验证耗费的时间4m9s\n",
            "Epoch 432/499\n",
            "----------\n",
            "432 train loss:0.9787 train acc: 0.7613\n",
            "432 val loss:1.0385 val acc: 0.7027\n",
            "训练和验证耗费的时间4m9s\n",
            "Epoch 433/499\n",
            "----------\n",
            "433 train loss:0.9730 train acc: 0.7710\n",
            "433 val loss:1.0403 val acc: 0.6950\n",
            "训练和验证耗费的时间4m10s\n",
            "Epoch 434/499\n",
            "----------\n",
            "434 train loss:0.9639 train acc: 0.7739\n",
            "434 val loss:1.0366 val acc: 0.7008\n",
            "训练和验证耗费的时间4m10s\n",
            "Epoch 435/499\n",
            "----------\n",
            "435 train loss:0.9743 train acc: 0.7613\n",
            "435 val loss:1.0363 val acc: 0.7027\n",
            "训练和验证耗费的时间4m11s\n",
            "Epoch 436/499\n",
            "----------\n",
            "436 train loss:0.9811 train acc: 0.7570\n",
            "436 val loss:1.0357 val acc: 0.7046\n",
            "训练和验证耗费的时间4m12s\n",
            "Epoch 437/499\n",
            "----------\n",
            "437 train loss:0.9781 train acc: 0.7599\n",
            "437 val loss:1.0359 val acc: 0.7066\n",
            "训练和验证耗费的时间4m12s\n",
            "Epoch 438/499\n",
            "----------\n",
            "438 train loss:0.9801 train acc: 0.7575\n",
            "438 val loss:1.0384 val acc: 0.7008\n",
            "训练和验证耗费的时间4m13s\n",
            "Epoch 439/499\n",
            "----------\n",
            "439 train loss:0.9730 train acc: 0.7705\n",
            "439 val loss:1.0406 val acc: 0.6892\n",
            "训练和验证耗费的时间4m13s\n",
            "Epoch 440/499\n",
            "----------\n",
            "440 train loss:0.9752 train acc: 0.7657\n",
            "440 val loss:1.0381 val acc: 0.6931\n",
            "训练和验证耗费的时间4m14s\n",
            "Epoch 441/499\n",
            "----------\n",
            "441 train loss:0.9760 train acc: 0.7642\n",
            "441 val loss:1.0363 val acc: 0.7027\n",
            "训练和验证耗费的时间4m14s\n",
            "Epoch 442/499\n",
            "----------\n",
            "442 train loss:0.9723 train acc: 0.7700\n",
            "442 val loss:1.0298 val acc: 0.7104\n",
            "训练和验证耗费的时间4m15s\n",
            "Epoch 443/499\n",
            "----------\n",
            "443 train loss:0.9781 train acc: 0.7623\n",
            "443 val loss:1.0306 val acc: 0.7027\n",
            "训练和验证耗费的时间4m16s\n",
            "Epoch 444/499\n",
            "----------\n",
            "444 train loss:0.9728 train acc: 0.7633\n",
            "444 val loss:1.0265 val acc: 0.7104\n",
            "训练和验证耗费的时间4m16s\n",
            "Epoch 445/499\n",
            "----------\n",
            "445 train loss:0.9708 train acc: 0.7705\n",
            "445 val loss:1.0329 val acc: 0.6988\n",
            "训练和验证耗费的时间4m17s\n",
            "Epoch 446/499\n",
            "----------\n",
            "446 train loss:0.9733 train acc: 0.7662\n",
            "446 val loss:1.0283 val acc: 0.7104\n",
            "训练和验证耗费的时间4m17s\n",
            "Epoch 447/499\n",
            "----------\n",
            "447 train loss:0.9822 train acc: 0.7551\n",
            "447 val loss:1.0284 val acc: 0.7104\n",
            "训练和验证耗费的时间4m18s\n",
            "Epoch 448/499\n",
            "----------\n",
            "448 train loss:0.9691 train acc: 0.7700\n",
            "448 val loss:1.0353 val acc: 0.7008\n",
            "训练和验证耗费的时间4m18s\n",
            "Epoch 449/499\n",
            "----------\n",
            "449 train loss:0.9652 train acc: 0.7724\n",
            "449 val loss:1.0277 val acc: 0.7162\n",
            "训练和验证耗费的时间4m19s\n",
            "Epoch 450/499\n",
            "----------\n",
            "450 train loss:0.9760 train acc: 0.7637\n",
            "450 val loss:1.0370 val acc: 0.6931\n",
            "训练和验证耗费的时间4m20s\n",
            "Epoch 451/499\n",
            "----------\n",
            "451 train loss:0.9655 train acc: 0.7724\n",
            "451 val loss:1.0356 val acc: 0.7046\n",
            "训练和验证耗费的时间4m20s\n",
            "Epoch 452/499\n",
            "----------\n",
            "452 train loss:0.9730 train acc: 0.7671\n",
            "452 val loss:1.0308 val acc: 0.7046\n",
            "训练和验证耗费的时间4m21s\n",
            "Epoch 453/499\n",
            "----------\n",
            "453 train loss:0.9674 train acc: 0.7772\n",
            "453 val loss:1.0335 val acc: 0.7027\n",
            "训练和验证耗费的时间4m21s\n",
            "Epoch 454/499\n",
            "----------\n",
            "454 train loss:0.9760 train acc: 0.7628\n",
            "454 val loss:1.0310 val acc: 0.7046\n",
            "训练和验证耗费的时间4m22s\n",
            "Epoch 455/499\n",
            "----------\n",
            "455 train loss:0.9815 train acc: 0.7594\n",
            "455 val loss:1.0286 val acc: 0.7046\n",
            "训练和验证耗费的时间4m23s\n",
            "Epoch 456/499\n",
            "----------\n",
            "456 train loss:0.9678 train acc: 0.7743\n",
            "456 val loss:1.0302 val acc: 0.7143\n",
            "训练和验证耗费的时间4m23s\n",
            "Epoch 457/499\n",
            "----------\n",
            "457 train loss:0.9765 train acc: 0.7652\n",
            "457 val loss:1.0380 val acc: 0.6969\n",
            "训练和验证耗费的时间4m24s\n",
            "Epoch 458/499\n",
            "----------\n",
            "458 train loss:0.9696 train acc: 0.7719\n",
            "458 val loss:1.0364 val acc: 0.7124\n",
            "训练和验证耗费的时间4m24s\n",
            "Epoch 459/499\n",
            "----------\n",
            "459 train loss:0.9637 train acc: 0.7782\n",
            "459 val loss:1.0304 val acc: 0.7124\n",
            "训练和验证耗费的时间4m25s\n",
            "Epoch 460/499\n",
            "----------\n",
            "460 train loss:0.9709 train acc: 0.7690\n",
            "460 val loss:1.0351 val acc: 0.7066\n",
            "训练和验证耗费的时间4m25s\n",
            "Epoch 461/499\n",
            "----------\n",
            "461 train loss:0.9764 train acc: 0.7637\n",
            "461 val loss:1.0260 val acc: 0.7201\n",
            "训练和验证耗费的时间4m26s\n",
            "Epoch 462/499\n",
            "----------\n",
            "462 train loss:0.9602 train acc: 0.7816\n",
            "462 val loss:1.0252 val acc: 0.7162\n",
            "训练和验证耗费的时间4m27s\n",
            "Epoch 463/499\n",
            "----------\n",
            "463 train loss:0.9645 train acc: 0.7695\n",
            "463 val loss:1.0249 val acc: 0.7143\n",
            "训练和验证耗费的时间4m27s\n",
            "Epoch 464/499\n",
            "----------\n",
            "464 train loss:0.9625 train acc: 0.7782\n",
            "464 val loss:1.0366 val acc: 0.7008\n",
            "训练和验证耗费的时间4m28s\n",
            "Epoch 465/499\n",
            "----------\n",
            "465 train loss:0.9724 train acc: 0.7652\n",
            "465 val loss:1.0303 val acc: 0.7027\n",
            "训练和验证耗费的时间4m28s\n",
            "Epoch 466/499\n",
            "----------\n",
            "466 train loss:0.9632 train acc: 0.7777\n",
            "466 val loss:1.0292 val acc: 0.7066\n",
            "训练和验证耗费的时间4m29s\n",
            "Epoch 467/499\n",
            "----------\n",
            "467 train loss:0.9675 train acc: 0.7734\n",
            "467 val loss:1.0288 val acc: 0.7046\n",
            "训练和验证耗费的时间4m29s\n",
            "Epoch 468/499\n",
            "----------\n",
            "468 train loss:0.9793 train acc: 0.7584\n",
            "468 val loss:1.0411 val acc: 0.6950\n",
            "训练和验证耗费的时间4m30s\n",
            "Epoch 469/499\n",
            "----------\n",
            "469 train loss:0.9711 train acc: 0.7690\n",
            "469 val loss:1.0330 val acc: 0.7085\n",
            "训练和验证耗费的时间4m31s\n",
            "Epoch 470/499\n",
            "----------\n",
            "470 train loss:0.9640 train acc: 0.7787\n",
            "470 val loss:1.0248 val acc: 0.7239\n",
            "训练和验证耗费的时间4m31s\n",
            "Epoch 471/499\n",
            "----------\n",
            "471 train loss:0.9655 train acc: 0.7715\n",
            "471 val loss:1.0249 val acc: 0.7104\n",
            "训练和验证耗费的时间4m32s\n",
            "Epoch 472/499\n",
            "----------\n",
            "472 train loss:0.9669 train acc: 0.7724\n",
            "472 val loss:1.0314 val acc: 0.7124\n",
            "训练和验证耗费的时间4m32s\n",
            "Epoch 473/499\n",
            "----------\n",
            "473 train loss:0.9704 train acc: 0.7705\n",
            "473 val loss:1.0312 val acc: 0.7008\n",
            "训练和验证耗费的时间4m33s\n",
            "Epoch 474/499\n",
            "----------\n",
            "474 train loss:0.9638 train acc: 0.7763\n",
            "474 val loss:1.0315 val acc: 0.7085\n",
            "训练和验证耗费的时间4m33s\n",
            "Epoch 475/499\n",
            "----------\n",
            "475 train loss:0.9759 train acc: 0.7666\n",
            "475 val loss:1.0406 val acc: 0.6950\n",
            "训练和验证耗费的时间4m34s\n",
            "Epoch 476/499\n",
            "----------\n",
            "476 train loss:0.9643 train acc: 0.7782\n",
            "476 val loss:1.0284 val acc: 0.7104\n",
            "训练和验证耗费的时间4m35s\n",
            "Epoch 477/499\n",
            "----------\n",
            "477 train loss:0.9510 train acc: 0.7907\n",
            "477 val loss:1.0354 val acc: 0.7027\n",
            "训练和验证耗费的时间4m35s\n",
            "Epoch 478/499\n",
            "----------\n",
            "478 train loss:0.9677 train acc: 0.7715\n",
            "478 val loss:1.0272 val acc: 0.7181\n",
            "训练和验证耗费的时间4m36s\n",
            "Epoch 479/499\n",
            "----------\n",
            "479 train loss:0.9687 train acc: 0.7710\n",
            "479 val loss:1.0275 val acc: 0.7143\n",
            "训练和验证耗费的时间4m36s\n",
            "Epoch 480/499\n",
            "----------\n",
            "480 train loss:0.9687 train acc: 0.7739\n",
            "480 val loss:1.0316 val acc: 0.7085\n",
            "训练和验证耗费的时间4m37s\n",
            "Epoch 481/499\n",
            "----------\n",
            "481 train loss:0.9820 train acc: 0.7584\n",
            "481 val loss:1.0366 val acc: 0.7008\n",
            "训练和验证耗费的时间4m38s\n",
            "Epoch 482/499\n",
            "----------\n",
            "482 train loss:0.9689 train acc: 0.7690\n",
            "482 val loss:1.0314 val acc: 0.7046\n",
            "训练和验证耗费的时间4m38s\n",
            "Epoch 483/499\n",
            "----------\n",
            "483 train loss:0.9725 train acc: 0.7666\n",
            "483 val loss:1.0290 val acc: 0.7220\n",
            "训练和验证耗费的时间4m39s\n",
            "Epoch 484/499\n",
            "----------\n",
            "484 train loss:0.9677 train acc: 0.7715\n",
            "484 val loss:1.0237 val acc: 0.7162\n",
            "训练和验证耗费的时间4m39s\n",
            "Epoch 485/499\n",
            "----------\n",
            "485 train loss:0.9645 train acc: 0.7739\n",
            "485 val loss:1.0271 val acc: 0.7124\n",
            "训练和验证耗费的时间4m40s\n",
            "Epoch 486/499\n",
            "----------\n",
            "486 train loss:0.9709 train acc: 0.7637\n",
            "486 val loss:1.0275 val acc: 0.7124\n",
            "训练和验证耗费的时间4m40s\n",
            "Epoch 487/499\n",
            "----------\n",
            "487 train loss:0.9702 train acc: 0.7676\n",
            "487 val loss:1.0293 val acc: 0.7104\n",
            "训练和验证耗费的时间4m41s\n",
            "Epoch 488/499\n",
            "----------\n",
            "488 train loss:0.9671 train acc: 0.7710\n",
            "488 val loss:1.0256 val acc: 0.7201\n",
            "训练和验证耗费的时间4m41s\n",
            "Epoch 489/499\n",
            "----------\n",
            "489 train loss:0.9671 train acc: 0.7724\n",
            "489 val loss:1.0309 val acc: 0.7162\n",
            "训练和验证耗费的时间4m42s\n",
            "Epoch 490/499\n",
            "----------\n",
            "490 train loss:0.9701 train acc: 0.7715\n",
            "490 val loss:1.0195 val acc: 0.7259\n",
            "训练和验证耗费的时间4m43s\n",
            "Epoch 491/499\n",
            "----------\n",
            "491 train loss:0.9647 train acc: 0.7739\n",
            "491 val loss:1.0208 val acc: 0.7201\n",
            "训练和验证耗费的时间4m43s\n",
            "Epoch 492/499\n",
            "----------\n",
            "492 train loss:0.9654 train acc: 0.7724\n",
            "492 val loss:1.0261 val acc: 0.7143\n",
            "训练和验证耗费的时间4m44s\n",
            "Epoch 493/499\n",
            "----------\n",
            "493 train loss:0.9645 train acc: 0.7748\n",
            "493 val loss:1.0225 val acc: 0.7259\n",
            "训练和验证耗费的时间4m44s\n",
            "Epoch 494/499\n",
            "----------\n",
            "494 train loss:0.9690 train acc: 0.7690\n",
            "494 val loss:1.0261 val acc: 0.7162\n",
            "训练和验证耗费的时间4m45s\n",
            "Epoch 495/499\n",
            "----------\n",
            "495 train loss:0.9706 train acc: 0.7695\n",
            "495 val loss:1.0344 val acc: 0.7124\n",
            "训练和验证耗费的时间4m46s\n",
            "Epoch 496/499\n",
            "----------\n",
            "496 train loss:0.9544 train acc: 0.7869\n",
            "496 val loss:1.0367 val acc: 0.7027\n",
            "训练和验证耗费的时间4m46s\n",
            "Epoch 497/499\n",
            "----------\n",
            "497 train loss:0.9555 train acc: 0.7883\n",
            "497 val loss:1.0265 val acc: 0.7220\n",
            "训练和验证耗费的时间4m47s\n",
            "Epoch 498/499\n",
            "----------\n",
            "498 train loss:0.9696 train acc: 0.7686\n",
            "498 val loss:1.0282 val acc: 0.7124\n",
            "训练和验证耗费的时间4m47s\n",
            "Epoch 499/499\n",
            "----------\n",
            "499 train loss:0.9739 train acc: 0.7662\n",
            "499 val loss:1.0236 val acc: 0.7201\n",
            "训练和验证耗费的时间4m48s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_test.py**"
      ],
      "metadata": {
        "id": "U02guqzH4SWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "#from model import EEGNet\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_test_pre'):\n",
        "    os.makedirs('2a_test_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'E' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose='error') for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "events = []\n",
        "for i in range(len(raw)):\n",
        "    event, event_dict = mne.events_from_annotations(raw[i])\n",
        "    events.append(event)\n",
        "\n",
        "    # 打印事件信息用于调试\n",
        "    print(f\"文件 {data_path[i]} 的事件类型: {event_dict}\")\n",
        "\n",
        "    raw[i].load_data()\n",
        "    data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    # 对于测试数据，使用768事件（cue开始）来创建epochs\n",
        "    # 在测试数据中，768事件对应MI任务的开始\n",
        "    event_id_test = {'768': 6}  # 使用768事件\n",
        "\n",
        "    try:\n",
        "        epochs = mne.Epochs(raw[i], events[i], event_id=event_id_test, tmin=tmin, tmax=tmax,\n",
        "                           proj=False, baseline=None, preload=True)\n",
        "    except Exception as e:\n",
        "        print(f\"创建epochs失败: {e}\")\n",
        "        print(f\"事件类型: {np.unique(events[i][:, 2])}\")\n",
        "        continue\n",
        "\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('./'+data_path[i]+'.mat')\n",
        "\n",
        "    # 检查测试数据中是否有classlabel\n",
        "    if 'classlabel' in labels_file:\n",
        "        labels = labels_file['classlabel'].reshape(-1)\n",
        "        print(f\"文件 {data_path[i]} 使用classlabel，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "    else:\n",
        "        # 对于测试数据，我们需要从其他来源获取标签\n",
        "        print(f\"文件 {data_path[i]} 没有classlabel，需要从其他来源获取标签\")\n",
        "\n",
        "        # 方法1: 检查是否有true_labels\n",
        "        if 'true_labels' in labels_file:\n",
        "            labels = labels_file['true_labels'].reshape(-1)\n",
        "            print(f\"使用true_labels，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "        # 方法2: 检查是否有其他标签字段\n",
        "        elif 'label' in labels_file:\n",
        "            labels = labels_file['label'].reshape(-1)\n",
        "            print(f\"使用label，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "        else:\n",
        "            # 方法3: 对于真正的测试数据，我们可能需要使用伪标签或从其他文件获取\n",
        "            print(f\"警告: 无法找到标签信息，使用均衡伪标签\")\n",
        "            # 创建均衡的伪标签 (1,2,3,4)\n",
        "            n_epochs = len(epochs)\n",
        "            labels = np.array([1, 2, 3, 4] * (n_epochs // 4 + 1))[:n_epochs]\n",
        "            print(f\"使用伪标签，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "\n",
        "    epochs_data = epochs.get_data(copy=True)\n",
        "\n",
        "    # 检查是否需要调整时间点\n",
        "    if epochs_data.shape[2] > 1000:\n",
        "        epochs_data = epochs_data[:, :, :1000]  # 取前1000个时间点\n",
        "    elif epochs_data.shape[2] < 1000:\n",
        "        # 如果时间点不足，进行填充\n",
        "        padded_data = np.zeros((epochs_data.shape[0], epochs_data.shape[1], 1000))\n",
        "        padded_data[:, :, :epochs_data.shape[2]] = epochs_data\n",
        "        epochs_data = padded_data\n",
        "\n",
        "    n_samples, n_channels, n_timepoints = epochs_data.shape\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    # 确保数据和标签的数量一致\n",
        "    min_length = min(len(data_scaled), len(labels))\n",
        "    data_scaled = data_scaled[:min_length]\n",
        "    labels = labels[:min_length]\n",
        "\n",
        "    print(f\"文件 {data_path[i]}: 数据形状 {data_scaled.shape}, 标签形状 {labels.shape}\")\n",
        "\n",
        "    np.savez('2a_test_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "\n",
        "\n",
        "# 5、创建测试集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_test, y_test = [], []\n",
        "    for i in range(1, 10):\n",
        "        try:\n",
        "            test_data = np.load(f'2a_test_pre/A0{i}E.npz')\n",
        "            x_test.append(test_data['data'])\n",
        "            y_test.append(test_data['label'])\n",
        "            print(f\"加载 A0{i}E.npz: 数据形状 {test_data['data'].shape}, 标签形状 {test_data['label'].shape}\")\n",
        "            print(f\"标签分布: {np.unique(test_data['label'], return_counts=True)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"加载 A0{i}E.npz 时出错: {e}\")\n",
        "            continue\n",
        "\n",
        "    # 检查是否成功加载了数据\n",
        "    if len(x_test) == 0:\n",
        "        raise ValueError(\"没有成功加载任何测试数据\")\n",
        "\n",
        "    # 合并数据\n",
        "    x_test = np.concatenate(x_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "\n",
        "    print(f\"合并后 - 数据形状: {x_test.shape}, 标签形状: {y_test.shape}\")\n",
        "    print(f\"合并后标签分布: {np.unique(y_test, return_counts=True)}\")\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_test = torch.FloatTensor(x_test).unsqueeze(1)\n",
        "    y_test = torch.LongTensor(y_test - 1)  # 标签从1-4转换为0-3\n",
        "\n",
        "    print(f\"转换后标签范围: {torch.unique(y_test)}\")\n",
        "\n",
        "    # 创建DataLoader\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(x_test, y_test),\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "# 6、模型测试\n",
        "def test_model_process(model, test_loader):\n",
        "    # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"使用设备: {device}\")\n",
        "\n",
        "    # 将模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 打印模型结构\n",
        "    print(\"模型结构:\")\n",
        "    print(model)\n",
        "\n",
        "    # 初始化参数\n",
        "    test_corrects = 0.0\n",
        "    test_num = 0\n",
        "\n",
        "    # 存储预测结果用于分析\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # 只进行前向传播计算，不计算梯度，从而节省内存，加快运行速度\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (test_data_x, test_data_y) in enumerate(test_loader):\n",
        "            # 将特征放入到测试设备中\n",
        "            test_data_x = test_data_x.to(device)\n",
        "            # 将标签放入到测试设备中\n",
        "            test_data_y = test_data_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为测试数据集，输出为对每个样本的预测值\n",
        "            output = model(test_data_x)\n",
        "\n",
        "            # 打印第一个batch的输出用于调试\n",
        "            if batch_idx == 0:\n",
        "                print(f\"模型输出形状: {output.shape}\")\n",
        "                print(f\"前5个样本的输出: {output[:5]}\")\n",
        "                print(f\"前5个样本的softmax概率: {torch.softmax(output[:5], dim=1)}\")\n",
        "\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "\n",
        "            # 存储预测结果和真实标签\n",
        "            all_predictions.extend(pre_lab.cpu().numpy())\n",
        "            all_labels.extend(test_data_y.cpu().numpy())\n",
        "\n",
        "            # 如果预测正确，则准确度test_corrects加1\n",
        "            test_corrects += torch.sum(pre_lab == test_data_y.data)\n",
        "            # 将所有的测试样本进行累加\n",
        "            test_num += test_data_x.size(0)\n",
        "\n",
        "            # 打印第一个batch的预测结果\n",
        "            if batch_idx == 0:\n",
        "                print(f\"第一个batch的预测: {pre_lab[:10]}\")\n",
        "                print(f\"第一个batch的真实标签: {test_data_y[:10]}\")\n",
        "                print(f\"第一个batch的正确数量: {torch.sum(pre_lab == test_data_y.data)}\")\n",
        "\n",
        "    # 计算测试准确率\n",
        "    test_acc = test_corrects.double().item() / test_num\n",
        "\n",
        "    # 分析预测结果\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    print(f\"\\n预测结果分析:\")\n",
        "    print(f\"预测标签分布: {np.unique(all_predictions, return_counts=True)}\")\n",
        "    print(f\"真实标签分布: {np.unique(all_labels, return_counts=True)}\")\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "    print(f\"\\n混淆矩阵:\")\n",
        "    print(confusion_matrix(all_labels, all_predictions))\n",
        "    print(f\"\\n分类报告:\")\n",
        "    print(classification_report(all_labels, all_predictions))\n",
        "\n",
        "    print(f\"\\n测试的准确率为: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "# 7、模型开始测试\n",
        "if __name__ == \"__main__\":\n",
        "    # 检查模型文件是否存在\n",
        "    if not os.path.exists('best_model.pth'):\n",
        "        print(\"错误: 找不到模型文件 'best_model.pth'\")\n",
        "        print(\"请确保模型文件存在于当前目录\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"加载模型...\")\n",
        "    model = EEGNet()\n",
        "\n",
        "    try:\n",
        "        # 尝试使用weights_only=True加载\n",
        "        model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
        "        print(\"模型加载成功 (weights_only=True)\")\n",
        "    except:\n",
        "        try:\n",
        "            # 如果失败，尝试使用weights_only=False\n",
        "            model.load_state_dict(torch.load('best_model.pth', weights_only=False))\n",
        "            print(\"模型加载成功 (weights_only=False)\")\n",
        "        except Exception as e:\n",
        "            print(f\"模型加载失败: {e}\")\n",
        "            exit(1)\n",
        "\n",
        "    print(\"创建数据加载器...\")\n",
        "    test_loader = create_simple_dataloaders()\n",
        "\n",
        "    print(\"开始测试...\")\n",
        "    test_model_process(model, test_loader)"
      ],
      "metadata": {
        "id": "OqfBuPsq3p2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1f0563-7c14-41d8-b483-e82f84668781"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A01E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A02E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A03E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A04E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A05E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A06E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A07E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A08E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A09E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A01E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A01E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A02E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A02E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A03E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A03E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A04E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A04E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A05E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A05E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A06E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A06E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A07E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A07E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A08E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A08E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "文件 A09E 没有classlabel，需要从其他来源获取标签\n",
            "警告: 无法找到标签信息，使用均衡伪标签\n",
            "使用伪标签，标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "文件 A09E: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "加载模型...\n",
            "模型加载成功 (weights_only=True)\n",
            "创建数据加载器...\n",
            "加载 A01E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A02E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A03E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A04E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A05E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A06E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A07E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A08E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "加载 A09E.npz: 数据形状 (288, 22, 1000), 标签形状 (288,)\n",
            "标签分布: (array([1, 2, 3, 4]), array([72, 72, 72, 72]))\n",
            "合并后 - 数据形状: (2592, 22, 1000), 标签形状: (2592,)\n",
            "合并后标签分布: (array([1, 2, 3, 4]), array([648, 648, 648, 648]))\n",
            "转换后标签范围: tensor([0, 1, 2, 3])\n",
            "开始测试...\n",
            "使用设备: cuda\n",
            "模型结构:\n",
            "EEGNet(\n",
            "  (EEGNetLayer): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
            "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(8, 16, kernel_size=(22, 1), stride=(1, 1), groups=8, bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "      (3): AvgPool2d(kernel_size=(1, 4), stride=4, padding=0)\n",
            "      (4): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), groups=16, bias=False)\n",
            "      (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ELU(alpha=1.0)\n",
            "      (4): AvgPool2d(kernel_size=(1, 8), stride=8, padding=0)\n",
            "      (5): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (ClassifierBlock): Sequential(\n",
            "    (0): Linear(in_features=496, out_features=4, bias=False)\n",
            "    (1): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "模型输出形状: torch.Size([32, 4])\n",
            "前5个样本的输出: tensor([[3.2476e-01, 6.7306e-01, 8.8935e-04, 1.2887e-03],\n",
            "        [3.4837e-04, 1.7147e-01, 8.2818e-01, 2.3475e-06],\n",
            "        [8.0881e-01, 7.3736e-04, 6.6256e-07, 1.9045e-01],\n",
            "        [7.5193e-01, 1.1593e-02, 2.3648e-01, 2.8205e-06],\n",
            "        [9.9693e-01, 2.1823e-05, 2.7684e-03, 2.8165e-04]], device='cuda:0')\n",
            "前5个样本的softmax概率: tensor([[0.2588, 0.3667, 0.1872, 0.1873],\n",
            "        [0.1827, 0.2168, 0.4180, 0.1826],\n",
            "        [0.4115, 0.1834, 0.1833, 0.2217],\n",
            "        [0.3928, 0.1874, 0.2346, 0.1852],\n",
            "        [0.4743, 0.1750, 0.1755, 0.1751]], device='cuda:0')\n",
            "第一个batch的预测: tensor([1, 2, 0, 0, 0, 3, 3, 0, 0, 3], device='cuda:0')\n",
            "第一个batch的真实标签: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1], device='cuda:0')\n",
            "第一个batch的正确数量: 4\n",
            "\n",
            "预测结果分析:\n",
            "预测标签分布: (array([0, 1, 2, 3]), array([530, 765, 608, 689]))\n",
            "真实标签分布: (array([0, 1, 2, 3]), array([648, 648, 648, 648]))\n",
            "\n",
            "混淆矩阵:\n",
            "[[136 192 162 158]\n",
            " [138 181 151 178]\n",
            " [129 187 163 169]\n",
            " [127 205 132 184]]\n",
            "\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.21      0.23       648\n",
            "           1       0.24      0.28      0.26       648\n",
            "           2       0.27      0.25      0.26       648\n",
            "           3       0.27      0.28      0.28       648\n",
            "\n",
            "    accuracy                           0.26      2592\n",
            "   macro avg       0.26      0.26      0.26      2592\n",
            "weighted avg       0.26      0.26      0.26      2592\n",
            "\n",
            "\n",
            "测试的准确率为: 0.2562\n"
          ]
        }
      ]
    }
  ]
}