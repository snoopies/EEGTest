{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Tgoh1wYyiRRDUyemYECNRAN30S-d1JGc",
      "authorship_tag": "ABX9TyOWPi5KAuz9B8EOKA0ce9Q1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snoopies/EEGTest/blob/main/bcieeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**model.py**"
      ],
      "metadata": {
        "id": "cCoqY8yp31eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ez0GDqI10HEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d7e408-d3cc-40b9-b6a9-0c7103100b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 22, 1001]             512\n",
            "       BatchNorm2d-2          [-1, 8, 22, 1001]              16\n",
            "            Conv2d-3          [-1, 16, 1, 1001]             352\n",
            "       BatchNorm2d-4          [-1, 16, 1, 1001]              32\n",
            "               ELU-5          [-1, 16, 1, 1001]               0\n",
            "         AvgPool2d-6           [-1, 16, 1, 250]               0\n",
            "           Dropout-7           [-1, 16, 1, 250]               0\n",
            "            Conv2d-8           [-1, 16, 1, 251]             256\n",
            "            Conv2d-9           [-1, 16, 1, 251]             256\n",
            "      BatchNorm2d-10           [-1, 16, 1, 251]              32\n",
            "              ELU-11           [-1, 16, 1, 251]               0\n",
            "        AvgPool2d-12            [-1, 16, 1, 31]               0\n",
            "          Dropout-13            [-1, 16, 1, 31]               0\n",
            "           Linear-14                    [-1, 4]           1,984\n",
            "          Softmax-15                    [-1, 4]               0\n",
            "================================================================\n",
            "Total params: 3,440\n",
            "Trainable params: 3,440\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.08\n",
            "Forward/backward pass size (MB): 3.25\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 3.34\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# 按照论文创建EEGNet\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, n_classes=4, channels=22, samples=1000,\n",
        "                 dropout_rate=0.5, kernel_length1=64, kernel_length2=16,\n",
        "                 f1=8, d=2, f2=16):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.f1 = f1\n",
        "        self.f2 = f2\n",
        "        self.d = d\n",
        "        self.samples = samples\n",
        "        self.n_classes = n_classes\n",
        "        self.channels = channels\n",
        "        self.kernel_length1 = kernel_length1\n",
        "        self.kernel_length2 = kernel_length2\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=self.f1,\n",
        "                kernel_size=(1, self.kernel_length1),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length1//2),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1)\n",
        "        )\n",
        "\n",
        "        block2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1,\n",
        "                out_channels=self.f1*self.d,\n",
        "                kernel_size=(self.channels, 1),\n",
        "                groups=self.f1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1*self.d),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 4),\n",
        "                stride=4\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        block3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f2,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, self.kernel_length2),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length2//2),\n",
        "                groups=self.f1*self.d,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1*self.d,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, 1),\n",
        "                stride=1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 8),\n",
        "                stride=8\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.EEGNetLayer = nn.Sequential(block1, block2, block3)\n",
        "\n",
        "        self.ClassifierBlock = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.f2*round(round(self.samples//4)//8),\n",
        "                out_features=self.n_classes,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) != 4:\n",
        "            x = torch.unsqueeze(x, dim=1)\n",
        "        x = self.EEGNetLayer(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.ClassifierBlock(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 模型结构可视化\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EEGNet().to(device)\n",
        "    print(summary(model, input_size=(1, 22, 1000)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_train.py**"
      ],
      "metadata": {
        "id": "FEh-At_B4JEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import copy\n",
        "import time\n",
        "#from model import EEGNet\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_train_pre'):\n",
        "    os.makedirs('2a_train_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'T' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose=\"error\") for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "events = []\n",
        "event_ids = []\n",
        "for i in range(len(raw)):\n",
        "    event_to_id = dict({'769': 7, '770': 8, '771': 9, '772': 10})\n",
        "    if i == 3:\n",
        "        event_to_id = dict({'769': 5, '770': 6, '771': 7, '772': 8})\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    else:\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    epochs = mne.Epochs(raw[i], events[i], event_ids[i], tmin, tmax, proj=False, baseline=None, preload=True)\n",
        "\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('./'+data_path[i]+'.mat')\n",
        "\n",
        "    # 打印所有键以便调试\n",
        "    print(f\"MAT file keys for {data_path[i]}: {list(labels_file.keys())}\")\n",
        "\n",
        "    # 新的标签提取方法 - 从data结构体的y字段提取\n",
        "    if 'data' in labels_file:\n",
        "        data_struct = labels_file['data']\n",
        "        print(f\"Data structure shape: {data_struct.shape}\")\n",
        "\n",
        "        # 提取所有trial的标签\n",
        "        all_labels = []\n",
        "        for trial_idx in range(data_struct.shape[1]):\n",
        "            trial_data = data_struct[0, trial_idx]\n",
        "            labels = trial_data['y'][0, 0]  # 提取y字段\n",
        "\n",
        "            if labels.size > 0:  # 只处理有标签的trial\n",
        "                trial_labels = labels.flatten().tolist()\n",
        "                all_labels.extend(trial_labels)\n",
        "                print(f\"Trial {trial_idx+1}: 标签数量 {len(trial_labels)}\")\n",
        "            else:\n",
        "                print(f\"Trial {trial_idx+1}: 无标签\")\n",
        "\n",
        "        labels = np.array(all_labels)\n",
        "        print(f\"提取的总标签数量: {len(labels)}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "        # 显示类别对应关系（用于验证）\n",
        "        if len(all_labels) > 0:\n",
        "            classes = data_struct[0, 0]['classes'][0, 0][0]\n",
        "            print(\"类别对应关系:\")\n",
        "            for class_idx, class_name in enumerate(classes):\n",
        "                print(f\"  标签 {class_idx+1}: {class_name[0]}\")\n",
        "\n",
        "    else:\n",
        "        # 备用方法：使用事件信息生成标签\n",
        "        print(\"使用事件信息生成标签\")\n",
        "        labels = events[i][:, 2]\n",
        "        label_mapping = {7: 1, 8: 2, 9: 3, 10: 4}\n",
        "        if i == 3:  # 特殊处理第4个文件\n",
        "            label_mapping = {5: 1, 6: 2, 7: 3, 8: 4}\n",
        "        labels = np.array([label_mapping.get(event_id, 0) for event_id in labels])\n",
        "        labels = labels[labels != 0]  # 移除无效标签\n",
        "\n",
        "    print(f\"最终标签形状: {labels.shape}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "    epochs_data = epochs.get_data(copy=True)[:, :, :-1]\n",
        "\n",
        "    # 确保标签数量与数据样本数量匹配\n",
        "    n_samples = epochs_data.shape[0]\n",
        "    if len(labels) != n_samples:\n",
        "        print(f\"警告: 标签数量 ({len(labels)}) 与数据样本数量 ({n_samples}) 不匹配\")\n",
        "        # 截取或调整标签以匹配数据数量\n",
        "        min_length = min(len(labels), n_samples)\n",
        "        labels = labels[:min_length]\n",
        "        epochs_data = epochs_data[:min_length]\n",
        "        print(f\"调整后: 标签数量 {len(labels)}, 数据样本数量 {epochs_data.shape[0]}\")\n",
        "\n",
        "    n_channels, n_timepoints = epochs_data.shape[1], epochs_data.shape[2]\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    np.savez('2a_train_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "# 5、创建训练集和验证集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(1, 10):\n",
        "        train_data = np.load(f'2a_train_pre/A0{i}T.npz')\n",
        "        x_train.append(train_data['data'])\n",
        "        y_train.append(train_data['label'])\n",
        "\n",
        "    # 合并数据\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_train = torch.FloatTensor(x_train).unsqueeze(1)\n",
        "    y_train = torch.LongTensor(y_train - 1)\n",
        "\n",
        "    # 创建完整数据集\n",
        "    full_dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "    # 计算训练集和验证集大小\n",
        "    dataset_size = len(full_dataset)\n",
        "    val_size = int(dataset_size * 0.2)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # 划分训练集和验证集\n",
        "    train_data, val_data = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # 创建DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# 6、训练模型\n",
        "def train_model_process(model, train_loader, val_loader, num_epochs):\n",
        "    # 设定训练所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # 使用Adam优化器，学习率为0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    # 损失函数为交叉熵函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # 将模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "    # 复制当前模型的参数\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 初始化参数\n",
        "    best_acc = 0.0\n",
        "    train_loss_all = []\n",
        "    val_loss_all = []\n",
        "    train_acc_all = []\n",
        "    val_acc_all = []\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        # 初始化参数\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        train_num = 0\n",
        "        val_num = 0\n",
        "\n",
        "        # 对每一个batch训练和计算\n",
        "        for step, (b_x, b_y) in enumerate(train_loader):\n",
        "            # 将特征放入到训练设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到训练设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为训练模式\n",
        "            model.train()\n",
        "\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 将梯度初始化为0\n",
        "            optimizer.zero_grad()\n",
        "            # 反向传播计算\n",
        "            loss.backward()\n",
        "            # 根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用\n",
        "            optimizer.step()\n",
        "            # 对损失函数进行累加\n",
        "            train_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            train_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于训练的样本数量\n",
        "            train_num += b_x.size(0)\n",
        "\n",
        "        for step, (b_x, b_y) in enumerate(val_loader):\n",
        "            # 将特征放入到验证设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到验证设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 对损失函数进行累加\n",
        "            val_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            val_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于验证的样本数量\n",
        "            val_num += b_x.size(0)\n",
        "\n",
        "        # 计算并保存每一次迭代的loss值和准确率\n",
        "        # 计算并保存训练集的loss值\n",
        "        train_loss_all.append(train_loss / train_num)\n",
        "        # 计算并保存训练集的准确率\n",
        "        train_acc_all.append(train_corrects.double().item() / train_num)\n",
        "\n",
        "        # 计算并保存验证集的loss值\n",
        "        val_loss_all.append(val_loss / val_num)\n",
        "        # 计算并保存验证集的准确率\n",
        "        val_acc_all.append(val_corrects.double().item() / val_num)\n",
        "\n",
        "        print(\"{} train loss:{:.4f} train acc: {:.4f}\".format(epoch, train_loss_all[-1], train_acc_all[-1]))\n",
        "        print(\"{} val loss:{:.4f} val acc: {:.4f}\".format(epoch, val_loss_all[-1], val_acc_all[-1]))\n",
        "\n",
        "        if val_acc_all[-1] > best_acc:\n",
        "            # 保存当前最高准确度\n",
        "            best_acc = val_acc_all[-1]\n",
        "            # 保存当前最高准确度的模型参数\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # 计算训练和验证的耗时\n",
        "        time_use = time.time() - since\n",
        "        print(\"训练和验证耗费的时间{:.0f}m{:.0f}s\".format(time_use//60, time_use % 60))\n",
        "\n",
        "    # 选择最优参数，保存最优参数的模型\n",
        "    torch.save(best_model_wts, \"best_model.pth\")\n",
        "\n",
        "    train_process = pd.DataFrame(data={\"epoch\": range(num_epochs),\n",
        "\"train_loss_all\": train_loss_all,\n",
        "\"val_loss_all\": val_loss_all,\n",
        "\"train_acc_all\": train_acc_all,\n",
        "\"val_acc_all\": val_acc_all})\n",
        "\n",
        "    return train_process\n",
        "\n",
        "\n",
        "# 7、可视化训练集和验证集的损失函数和准确率\n",
        "def matplot_acc_loss(train_process):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 8、模型开始训练\n",
        "if __name__ == '__main__':\n",
        "    model = EEGNet()\n",
        "    train_loader, val_loader = create_simple_dataloaders()\n",
        "    train_process = train_model_process(model, train_loader, val_loader, num_epochs=500)\n",
        "    matplot_acc_loss(train_process)\n",
        "\n"
      ],
      "metadata": {
        "id": "aN-b-UvU3Y7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "639a6a22-529e-4f8c-9814-62f25e7a1dc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A01T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A02T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A03T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A04T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 7)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 标签数量 48\n",
            "Trial 3: 标签数量 48\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A05T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A06T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A07T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A08T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A09T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Epoch 0/499\n",
            "----------\n",
            "0 train loss:1.3804 train acc: 0.2883\n",
            "0 val loss:1.3709 val acc: 0.3320\n",
            "训练和验证耗费的时间0m1s\n",
            "Epoch 1/499\n",
            "----------\n",
            "1 train loss:1.3541 train acc: 0.3679\n",
            "1 val loss:1.3488 val acc: 0.3726\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 2/499\n",
            "----------\n",
            "2 train loss:1.3274 train acc: 0.3973\n",
            "2 val loss:1.3235 val acc: 0.4170\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 3/499\n",
            "----------\n",
            "3 train loss:1.2917 train acc: 0.4484\n",
            "3 val loss:1.2864 val acc: 0.4807\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 4/499\n",
            "----------\n",
            "4 train loss:1.2641 train acc: 0.4797\n",
            "4 val loss:1.2496 val acc: 0.5232\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 5/499\n",
            "----------\n",
            "5 train loss:1.2394 train acc: 0.4908\n",
            "5 val loss:1.2340 val acc: 0.5193\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 6/499\n",
            "----------\n",
            "6 train loss:1.2202 train acc: 0.5217\n",
            "6 val loss:1.2237 val acc: 0.5193\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 7/499\n",
            "----------\n",
            "7 train loss:1.2069 train acc: 0.5391\n",
            "7 val loss:1.2194 val acc: 0.5174\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 8/499\n",
            "----------\n",
            "8 train loss:1.1960 train acc: 0.5448\n",
            "8 val loss:1.2064 val acc: 0.5328\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 9/499\n",
            "----------\n",
            "9 train loss:1.1887 train acc: 0.5535\n",
            "9 val loss:1.2008 val acc: 0.5386\n",
            "训练和验证耗费的时间0m6s\n",
            "Epoch 10/499\n",
            "----------\n",
            "10 train loss:1.1788 train acc: 0.5583\n",
            "10 val loss:1.1922 val acc: 0.5579\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 11/499\n",
            "----------\n",
            "11 train loss:1.1688 train acc: 0.5733\n",
            "11 val loss:1.1876 val acc: 0.5463\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 12/499\n",
            "----------\n",
            "12 train loss:1.1694 train acc: 0.5680\n",
            "12 val loss:1.1814 val acc: 0.5656\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 13/499\n",
            "----------\n",
            "13 train loss:1.1594 train acc: 0.5834\n",
            "13 val loss:1.1798 val acc: 0.5656\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 14/499\n",
            "----------\n",
            "14 train loss:1.1587 train acc: 0.5853\n",
            "14 val loss:1.1756 val acc: 0.5637\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 15/499\n",
            "----------\n",
            "15 train loss:1.1502 train acc: 0.5993\n",
            "15 val loss:1.1709 val acc: 0.5753\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 16/499\n",
            "----------\n",
            "16 train loss:1.1530 train acc: 0.5868\n",
            "16 val loss:1.1712 val acc: 0.5772\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 17/499\n",
            "----------\n",
            "17 train loss:1.1306 train acc: 0.6119\n",
            "17 val loss:1.1713 val acc: 0.5734\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 18/499\n",
            "----------\n",
            "18 train loss:1.1393 train acc: 0.6027\n",
            "18 val loss:1.1645 val acc: 0.5753\n",
            "训练和验证耗费的时间0m11s\n",
            "Epoch 19/499\n",
            "----------\n",
            "19 train loss:1.1362 train acc: 0.6109\n",
            "19 val loss:1.1635 val acc: 0.5792\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 20/499\n",
            "----------\n",
            "20 train loss:1.1339 train acc: 0.6080\n",
            "20 val loss:1.1631 val acc: 0.5830\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 21/499\n",
            "----------\n",
            "21 train loss:1.1246 train acc: 0.6220\n",
            "21 val loss:1.1685 val acc: 0.5676\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 22/499\n",
            "----------\n",
            "22 train loss:1.1209 train acc: 0.6133\n",
            "22 val loss:1.1570 val acc: 0.5811\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 23/499\n",
            "----------\n",
            "23 train loss:1.1215 train acc: 0.6205\n",
            "23 val loss:1.1551 val acc: 0.5849\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 24/499\n",
            "----------\n",
            "24 train loss:1.1262 train acc: 0.6143\n",
            "24 val loss:1.1492 val acc: 0.5907\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 25/499\n",
            "----------\n",
            "25 train loss:1.1156 train acc: 0.6345\n",
            "25 val loss:1.1482 val acc: 0.5849\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 26/499\n",
            "----------\n",
            "26 train loss:1.1085 train acc: 0.6393\n",
            "26 val loss:1.1479 val acc: 0.5946\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 27/499\n",
            "----------\n",
            "27 train loss:1.1119 train acc: 0.6345\n",
            "27 val loss:1.1481 val acc: 0.5946\n",
            "训练和验证耗费的时间0m16s\n",
            "Epoch 28/499\n",
            "----------\n",
            "28 train loss:1.1107 train acc: 0.6297\n",
            "28 val loss:1.1447 val acc: 0.6042\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 29/499\n",
            "----------\n",
            "29 train loss:1.1048 train acc: 0.6389\n",
            "29 val loss:1.1530 val acc: 0.5907\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 30/499\n",
            "----------\n",
            "30 train loss:1.1161 train acc: 0.6225\n",
            "30 val loss:1.1480 val acc: 0.5907\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 31/499\n",
            "----------\n",
            "31 train loss:1.1160 train acc: 0.6244\n",
            "31 val loss:1.1409 val acc: 0.5849\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 32/499\n",
            "----------\n",
            "32 train loss:1.0984 train acc: 0.6437\n",
            "32 val loss:1.1365 val acc: 0.6100\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 33/499\n",
            "----------\n",
            "33 train loss:1.1063 train acc: 0.6384\n",
            "33 val loss:1.1317 val acc: 0.6139\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 34/499\n",
            "----------\n",
            "34 train loss:1.1016 train acc: 0.6471\n",
            "34 val loss:1.1272 val acc: 0.6236\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 35/499\n",
            "----------\n",
            "35 train loss:1.0946 train acc: 0.6475\n",
            "35 val loss:1.1275 val acc: 0.6158\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 36/499\n",
            "----------\n",
            "36 train loss:1.0931 train acc: 0.6500\n",
            "36 val loss:1.1220 val acc: 0.6255\n",
            "训练和验证耗费的时间0m21s\n",
            "Epoch 37/499\n",
            "----------\n",
            "37 train loss:1.0982 train acc: 0.6451\n",
            "37 val loss:1.1274 val acc: 0.6062\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 38/499\n",
            "----------\n",
            "38 train loss:1.0945 train acc: 0.6422\n",
            "38 val loss:1.1125 val acc: 0.6371\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 39/499\n",
            "----------\n",
            "39 train loss:1.0866 train acc: 0.6553\n",
            "39 val loss:1.1163 val acc: 0.6197\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 40/499\n",
            "----------\n",
            "40 train loss:1.1017 train acc: 0.6365\n",
            "40 val loss:1.1133 val acc: 0.6390\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 41/499\n",
            "----------\n",
            "41 train loss:1.0990 train acc: 0.6355\n",
            "41 val loss:1.1167 val acc: 0.6351\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 42/499\n",
            "----------\n",
            "42 train loss:1.0794 train acc: 0.6577\n",
            "42 val loss:1.1079 val acc: 0.6371\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 43/499\n",
            "----------\n",
            "43 train loss:1.0870 train acc: 0.6524\n",
            "43 val loss:1.1093 val acc: 0.6178\n",
            "训练和验证耗费的时间0m25s\n",
            "Epoch 44/499\n",
            "----------\n",
            "44 train loss:1.0731 train acc: 0.6731\n",
            "44 val loss:1.1034 val acc: 0.6313\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 45/499\n",
            "----------\n",
            "45 train loss:1.0779 train acc: 0.6644\n",
            "45 val loss:1.1086 val acc: 0.6409\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 46/499\n",
            "----------\n",
            "46 train loss:1.0798 train acc: 0.6688\n",
            "46 val loss:1.1041 val acc: 0.6274\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 47/499\n",
            "----------\n",
            "47 train loss:1.0727 train acc: 0.6702\n",
            "47 val loss:1.0997 val acc: 0.6467\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 48/499\n",
            "----------\n",
            "48 train loss:1.0792 train acc: 0.6620\n",
            "48 val loss:1.1048 val acc: 0.6448\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 49/499\n",
            "----------\n",
            "49 train loss:1.0748 train acc: 0.6712\n",
            "49 val loss:1.1020 val acc: 0.6255\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 50/499\n",
            "----------\n",
            "50 train loss:1.0791 train acc: 0.6606\n",
            "50 val loss:1.1003 val acc: 0.6390\n",
            "训练和验证耗费的时间0m29s\n",
            "Epoch 51/499\n",
            "----------\n",
            "51 train loss:1.0622 train acc: 0.6900\n",
            "51 val loss:1.0969 val acc: 0.6371\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 52/499\n",
            "----------\n",
            "52 train loss:1.0709 train acc: 0.6712\n",
            "52 val loss:1.0951 val acc: 0.6390\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 53/499\n",
            "----------\n",
            "53 train loss:1.0704 train acc: 0.6794\n",
            "53 val loss:1.0950 val acc: 0.6467\n",
            "训练和验证耗费的时间0m31s\n",
            "Epoch 54/499\n",
            "----------\n",
            "54 train loss:1.0586 train acc: 0.6779\n",
            "54 val loss:1.0911 val acc: 0.6409\n",
            "训练和验证耗费的时间0m31s\n",
            "Epoch 55/499\n",
            "----------\n",
            "55 train loss:1.0673 train acc: 0.6803\n",
            "55 val loss:1.0885 val acc: 0.6448\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 56/499\n",
            "----------\n",
            "56 train loss:1.0654 train acc: 0.6760\n",
            "56 val loss:1.0902 val acc: 0.6429\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 57/499\n",
            "----------\n",
            "57 train loss:1.0649 train acc: 0.6784\n",
            "57 val loss:1.0874 val acc: 0.6506\n",
            "训练和验证耗费的时间0m33s\n",
            "Epoch 58/499\n",
            "----------\n",
            "58 train loss:1.0656 train acc: 0.6750\n",
            "58 val loss:1.0916 val acc: 0.6467\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 59/499\n",
            "----------\n",
            "59 train loss:1.0650 train acc: 0.6688\n",
            "59 val loss:1.0828 val acc: 0.6564\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 60/499\n",
            "----------\n",
            "60 train loss:1.0626 train acc: 0.6842\n",
            "60 val loss:1.0815 val acc: 0.6448\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 61/499\n",
            "----------\n",
            "61 train loss:1.0606 train acc: 0.6770\n",
            "61 val loss:1.0861 val acc: 0.6448\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 62/499\n",
            "----------\n",
            "62 train loss:1.0667 train acc: 0.6798\n",
            "62 val loss:1.0829 val acc: 0.6622\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 63/499\n",
            "----------\n",
            "63 train loss:1.0614 train acc: 0.6765\n",
            "63 val loss:1.0807 val acc: 0.6583\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 64/499\n",
            "----------\n",
            "64 train loss:1.0637 train acc: 0.6755\n",
            "64 val loss:1.0808 val acc: 0.6583\n",
            "训练和验证耗费的时间0m37s\n",
            "Epoch 65/499\n",
            "----------\n",
            "65 train loss:1.0640 train acc: 0.6765\n",
            "65 val loss:1.0767 val acc: 0.6564\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 66/499\n",
            "----------\n",
            "66 train loss:1.0489 train acc: 0.6861\n",
            "66 val loss:1.0749 val acc: 0.6583\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 67/499\n",
            "----------\n",
            "67 train loss:1.0644 train acc: 0.6774\n",
            "67 val loss:1.0718 val acc: 0.6776\n",
            "训练和验证耗费的时间0m39s\n",
            "Epoch 68/499\n",
            "----------\n",
            "68 train loss:1.0474 train acc: 0.6991\n",
            "68 val loss:1.0765 val acc: 0.6564\n",
            "训练和验证耗费的时间0m39s\n",
            "Epoch 69/499\n",
            "----------\n",
            "69 train loss:1.0525 train acc: 0.6813\n",
            "69 val loss:1.0764 val acc: 0.6486\n",
            "训练和验证耗费的时间0m40s\n",
            "Epoch 70/499\n",
            "----------\n",
            "70 train loss:1.0474 train acc: 0.6909\n",
            "70 val loss:1.0770 val acc: 0.6718\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 71/499\n",
            "----------\n",
            "71 train loss:1.0543 train acc: 0.6866\n",
            "71 val loss:1.0746 val acc: 0.6757\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 72/499\n",
            "----------\n",
            "72 train loss:1.0444 train acc: 0.6972\n",
            "72 val loss:1.0721 val acc: 0.6564\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 73/499\n",
            "----------\n",
            "73 train loss:1.0467 train acc: 0.6948\n",
            "73 val loss:1.0763 val acc: 0.6564\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 74/499\n",
            "----------\n",
            "74 train loss:1.0516 train acc: 0.6909\n",
            "74 val loss:1.0670 val acc: 0.6737\n",
            "训练和验证耗费的时间0m43s\n",
            "Epoch 75/499\n",
            "----------\n",
            "75 train loss:1.0408 train acc: 0.7068\n",
            "75 val loss:1.0725 val acc: 0.6680\n",
            "训练和验证耗费的时间0m44s\n",
            "Epoch 76/499\n",
            "----------\n",
            "76 train loss:1.0390 train acc: 0.7073\n",
            "76 val loss:1.0727 val acc: 0.6564\n",
            "训练和验证耗费的时间0m44s\n",
            "Epoch 77/499\n",
            "----------\n",
            "77 train loss:1.0396 train acc: 0.7015\n",
            "77 val loss:1.0713 val acc: 0.6622\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 78/499\n",
            "----------\n",
            "78 train loss:1.0454 train acc: 0.6943\n",
            "78 val loss:1.0730 val acc: 0.6602\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 79/499\n",
            "----------\n",
            "79 train loss:1.0419 train acc: 0.7059\n",
            "79 val loss:1.0655 val acc: 0.6641\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 80/499\n",
            "----------\n",
            "80 train loss:1.0464 train acc: 0.6958\n",
            "80 val loss:1.0727 val acc: 0.6583\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 81/499\n",
            "----------\n",
            "81 train loss:1.0522 train acc: 0.6779\n",
            "81 val loss:1.0667 val acc: 0.6660\n",
            "训练和验证耗费的时间0m47s\n",
            "Epoch 82/499\n",
            "----------\n",
            "82 train loss:1.0426 train acc: 0.6938\n",
            "82 val loss:1.0675 val acc: 0.6757\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 83/499\n",
            "----------\n",
            "83 train loss:1.0559 train acc: 0.6880\n",
            "83 val loss:1.0676 val acc: 0.6699\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 84/499\n",
            "----------\n",
            "84 train loss:1.0387 train acc: 0.7030\n",
            "84 val loss:1.0633 val acc: 0.6622\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 85/499\n",
            "----------\n",
            "85 train loss:1.0419 train acc: 0.6967\n",
            "85 val loss:1.0609 val acc: 0.6680\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 86/499\n",
            "----------\n",
            "86 train loss:1.0328 train acc: 0.7146\n",
            "86 val loss:1.0611 val acc: 0.6699\n",
            "训练和验证耗费的时间0m50s\n",
            "Epoch 87/499\n",
            "----------\n",
            "87 train loss:1.0352 train acc: 0.7059\n",
            "87 val loss:1.0611 val acc: 0.6641\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 88/499\n",
            "----------\n",
            "88 train loss:1.0392 train acc: 0.7035\n",
            "88 val loss:1.0645 val acc: 0.6680\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 89/499\n",
            "----------\n",
            "89 train loss:1.0400 train acc: 0.7006\n",
            "89 val loss:1.0581 val acc: 0.6853\n",
            "训练和验证耗费的时间0m52s\n",
            "Epoch 90/499\n",
            "----------\n",
            "90 train loss:1.0360 train acc: 0.7083\n",
            "90 val loss:1.0597 val acc: 0.6602\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 91/499\n",
            "----------\n",
            "91 train loss:1.0250 train acc: 0.7170\n",
            "91 val loss:1.0523 val acc: 0.6815\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 92/499\n",
            "----------\n",
            "92 train loss:1.0364 train acc: 0.7059\n",
            "92 val loss:1.0530 val acc: 0.6737\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 93/499\n",
            "----------\n",
            "93 train loss:1.0316 train acc: 0.7122\n",
            "93 val loss:1.0599 val acc: 0.6680\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 94/499\n",
            "----------\n",
            "94 train loss:1.0376 train acc: 0.7001\n",
            "94 val loss:1.0533 val acc: 0.6757\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 95/499\n",
            "----------\n",
            "95 train loss:1.0337 train acc: 0.7112\n",
            "95 val loss:1.0566 val acc: 0.6757\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 96/499\n",
            "----------\n",
            "96 train loss:1.0319 train acc: 0.7088\n",
            "96 val loss:1.0570 val acc: 0.6680\n",
            "训练和验证耗费的时间0m56s\n",
            "Epoch 97/499\n",
            "----------\n",
            "97 train loss:1.0318 train acc: 0.7073\n",
            "97 val loss:1.0590 val acc: 0.6699\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 98/499\n",
            "----------\n",
            "98 train loss:1.0289 train acc: 0.7146\n",
            "98 val loss:1.0593 val acc: 0.6680\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 99/499\n",
            "----------\n",
            "99 train loss:1.0254 train acc: 0.7117\n",
            "99 val loss:1.0551 val acc: 0.6834\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 100/499\n",
            "----------\n",
            "100 train loss:1.0421 train acc: 0.6967\n",
            "100 val loss:1.0558 val acc: 0.6873\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 101/499\n",
            "----------\n",
            "101 train loss:1.0308 train acc: 0.7126\n",
            "101 val loss:1.0598 val acc: 0.6699\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 102/499\n",
            "----------\n",
            "102 train loss:1.0416 train acc: 0.7006\n",
            "102 val loss:1.0593 val acc: 0.6718\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 103/499\n",
            "----------\n",
            "103 train loss:1.0338 train acc: 0.7020\n",
            "103 val loss:1.0577 val acc: 0.6757\n",
            "训练和验证耗费的时间1m0s\n",
            "Epoch 104/499\n",
            "----------\n",
            "104 train loss:1.0325 train acc: 0.7073\n",
            "104 val loss:1.0536 val acc: 0.6834\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 105/499\n",
            "----------\n",
            "105 train loss:1.0301 train acc: 0.7078\n",
            "105 val loss:1.0541 val acc: 0.6795\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 106/499\n",
            "----------\n",
            "106 train loss:1.0243 train acc: 0.7184\n",
            "106 val loss:1.0538 val acc: 0.6718\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 107/499\n",
            "----------\n",
            "107 train loss:1.0205 train acc: 0.7203\n",
            "107 val loss:1.0535 val acc: 0.6776\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 108/499\n",
            "----------\n",
            "108 train loss:1.0281 train acc: 0.7088\n",
            "108 val loss:1.0477 val acc: 0.6853\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 109/499\n",
            "----------\n",
            "109 train loss:1.0275 train acc: 0.7131\n",
            "109 val loss:1.0443 val acc: 0.6911\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 110/499\n",
            "----------\n",
            "110 train loss:1.0310 train acc: 0.7088\n",
            "110 val loss:1.0416 val acc: 0.6950\n",
            "训练和验证耗费的时间1m4s\n",
            "Epoch 111/499\n",
            "----------\n",
            "111 train loss:1.0240 train acc: 0.7141\n",
            "111 val loss:1.0505 val acc: 0.6795\n",
            "训练和验证耗费的时间1m5s\n",
            "Epoch 112/499\n",
            "----------\n",
            "112 train loss:1.0185 train acc: 0.7281\n",
            "112 val loss:1.0417 val acc: 0.6892\n",
            "训练和验证耗费的时间1m5s\n",
            "Epoch 113/499\n",
            "----------\n",
            "113 train loss:1.0233 train acc: 0.7160\n",
            "113 val loss:1.0370 val acc: 0.7008\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 114/499\n",
            "----------\n",
            "114 train loss:1.0240 train acc: 0.7175\n",
            "114 val loss:1.0418 val acc: 0.6853\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 115/499\n",
            "----------\n",
            "115 train loss:1.0285 train acc: 0.7122\n",
            "115 val loss:1.0481 val acc: 0.6853\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 116/499\n",
            "----------\n",
            "116 train loss:1.0215 train acc: 0.7237\n",
            "116 val loss:1.0468 val acc: 0.6815\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 117/499\n",
            "----------\n",
            "117 train loss:1.0333 train acc: 0.7073\n",
            "117 val loss:1.0450 val acc: 0.6795\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 118/499\n",
            "----------\n",
            "118 train loss:1.0228 train acc: 0.7146\n",
            "118 val loss:1.0423 val acc: 0.6988\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 119/499\n",
            "----------\n",
            "119 train loss:1.0307 train acc: 0.7102\n",
            "119 val loss:1.0489 val acc: 0.6853\n",
            "训练和验证耗费的时间1m9s\n",
            "Epoch 120/499\n",
            "----------\n",
            "120 train loss:1.0071 train acc: 0.7392\n",
            "120 val loss:1.0423 val acc: 0.6950\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 121/499\n",
            "----------\n",
            "121 train loss:1.0252 train acc: 0.7175\n",
            "121 val loss:1.0413 val acc: 0.6931\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 122/499\n",
            "----------\n",
            "122 train loss:1.0244 train acc: 0.7146\n",
            "122 val loss:1.0409 val acc: 0.6969\n",
            "训练和验证耗费的时间1m11s\n",
            "Epoch 123/499\n",
            "----------\n",
            "123 train loss:1.0273 train acc: 0.7112\n",
            "123 val loss:1.0434 val acc: 0.6911\n",
            "训练和验证耗费的时间1m11s\n",
            "Epoch 124/499\n",
            "----------\n",
            "124 train loss:1.0151 train acc: 0.7218\n",
            "124 val loss:1.0369 val acc: 0.6988\n",
            "训练和验证耗费的时间1m12s\n",
            "Epoch 125/499\n",
            "----------\n",
            "125 train loss:1.0106 train acc: 0.7285\n",
            "125 val loss:1.0376 val acc: 0.6931\n",
            "训练和验证耗费的时间1m13s\n",
            "Epoch 126/499\n",
            "----------\n",
            "126 train loss:1.0244 train acc: 0.7189\n",
            "126 val loss:1.0397 val acc: 0.6892\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 127/499\n",
            "----------\n",
            "127 train loss:1.0195 train acc: 0.7218\n",
            "127 val loss:1.0434 val acc: 0.6834\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 128/499\n",
            "----------\n",
            "128 train loss:1.0182 train acc: 0.7247\n",
            "128 val loss:1.0343 val acc: 0.7008\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 129/499\n",
            "----------\n",
            "129 train loss:1.0264 train acc: 0.7097\n",
            "129 val loss:1.0361 val acc: 0.7066\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 130/499\n",
            "----------\n",
            "130 train loss:1.0205 train acc: 0.7228\n",
            "130 val loss:1.0332 val acc: 0.7143\n",
            "训练和验证耗费的时间1m16s\n",
            "Epoch 131/499\n",
            "----------\n",
            "131 train loss:1.0272 train acc: 0.7102\n",
            "131 val loss:1.0484 val acc: 0.6853\n",
            "训练和验证耗费的时间1m16s\n",
            "Epoch 132/499\n",
            "----------\n",
            "132 train loss:1.0199 train acc: 0.7232\n",
            "132 val loss:1.0395 val acc: 0.6950\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 133/499\n",
            "----------\n",
            "133 train loss:1.0210 train acc: 0.7170\n",
            "133 val loss:1.0394 val acc: 0.6969\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 134/499\n",
            "----------\n",
            "134 train loss:1.0216 train acc: 0.7170\n",
            "134 val loss:1.0394 val acc: 0.7046\n",
            "训练和验证耗费的时间1m18s\n",
            "Epoch 135/499\n",
            "----------\n",
            "135 train loss:1.0241 train acc: 0.7136\n",
            "135 val loss:1.0434 val acc: 0.6873\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 136/499\n",
            "----------\n",
            "136 train loss:1.0082 train acc: 0.7334\n",
            "136 val loss:1.0343 val acc: 0.7008\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 137/499\n",
            "----------\n",
            "137 train loss:1.0157 train acc: 0.7252\n",
            "137 val loss:1.0372 val acc: 0.6931\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 138/499\n",
            "----------\n",
            "138 train loss:1.0225 train acc: 0.7242\n",
            "138 val loss:1.0369 val acc: 0.7008\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 139/499\n",
            "----------\n",
            "139 train loss:1.0036 train acc: 0.7411\n",
            "139 val loss:1.0369 val acc: 0.6873\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 140/499\n",
            "----------\n",
            "140 train loss:1.0127 train acc: 0.7295\n",
            "140 val loss:1.0337 val acc: 0.6969\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 141/499\n",
            "----------\n",
            "141 train loss:1.0065 train acc: 0.7411\n",
            "141 val loss:1.0398 val acc: 0.6931\n",
            "训练和验证耗费的时间1m22s\n",
            "Epoch 142/499\n",
            "----------\n",
            "142 train loss:1.0101 train acc: 0.7290\n",
            "142 val loss:1.0435 val acc: 0.6988\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 143/499\n",
            "----------\n",
            "143 train loss:1.0093 train acc: 0.7319\n",
            "143 val loss:1.0377 val acc: 0.6931\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 144/499\n",
            "----------\n",
            "144 train loss:1.0020 train acc: 0.7372\n",
            "144 val loss:1.0387 val acc: 0.7008\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 145/499\n",
            "----------\n",
            "145 train loss:1.0122 train acc: 0.7281\n",
            "145 val loss:1.0349 val acc: 0.7066\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 146/499\n",
            "----------\n",
            "146 train loss:1.0171 train acc: 0.7261\n",
            "146 val loss:1.0357 val acc: 0.6950\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 147/499\n",
            "----------\n",
            "147 train loss:1.0139 train acc: 0.7290\n",
            "147 val loss:1.0361 val acc: 0.6911\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 148/499\n",
            "----------\n",
            "148 train loss:1.0249 train acc: 0.7155\n",
            "148 val loss:1.0425 val acc: 0.6950\n",
            "训练和验证耗费的时间1m26s\n",
            "Epoch 149/499\n",
            "----------\n",
            "149 train loss:1.0145 train acc: 0.7276\n",
            "149 val loss:1.0369 val acc: 0.6911\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 150/499\n",
            "----------\n",
            "150 train loss:1.0196 train acc: 0.7189\n",
            "150 val loss:1.0320 val acc: 0.7027\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 151/499\n",
            "----------\n",
            "151 train loss:0.9991 train acc: 0.7416\n",
            "151 val loss:1.0365 val acc: 0.6950\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 152/499\n",
            "----------\n",
            "152 train loss:1.0111 train acc: 0.7348\n",
            "152 val loss:1.0285 val acc: 0.7085\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 153/499\n",
            "----------\n",
            "153 train loss:1.0129 train acc: 0.7232\n",
            "153 val loss:1.0372 val acc: 0.6988\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 154/499\n",
            "----------\n",
            "154 train loss:1.0079 train acc: 0.7305\n",
            "154 val loss:1.0275 val acc: 0.6988\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 155/499\n",
            "----------\n",
            "155 train loss:1.0166 train acc: 0.7146\n",
            "155 val loss:1.0304 val acc: 0.7104\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 156/499\n",
            "----------\n",
            "156 train loss:1.0189 train acc: 0.7160\n",
            "156 val loss:1.0352 val acc: 0.6950\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 157/499\n",
            "----------\n",
            "157 train loss:1.0165 train acc: 0.7218\n",
            "157 val loss:1.0310 val acc: 0.6988\n",
            "训练和验证耗费的时间1m31s\n",
            "Epoch 158/499\n",
            "----------\n",
            "158 train loss:1.0217 train acc: 0.7160\n",
            "158 val loss:1.0360 val acc: 0.7046\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 159/499\n",
            "----------\n",
            "159 train loss:1.0124 train acc: 0.7305\n",
            "159 val loss:1.0335 val acc: 0.6988\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 160/499\n",
            "----------\n",
            "160 train loss:1.0017 train acc: 0.7430\n",
            "160 val loss:1.0334 val acc: 0.7008\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 161/499\n",
            "----------\n",
            "161 train loss:1.0217 train acc: 0.7199\n",
            "161 val loss:1.0315 val acc: 0.6950\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 162/499\n",
            "----------\n",
            "162 train loss:1.0105 train acc: 0.7353\n",
            "162 val loss:1.0288 val acc: 0.6988\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 163/499\n",
            "----------\n",
            "163 train loss:1.0154 train acc: 0.7242\n",
            "163 val loss:1.0410 val acc: 0.6950\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 164/499\n",
            "----------\n",
            "164 train loss:1.0116 train acc: 0.7271\n",
            "164 val loss:1.0287 val acc: 0.7046\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 165/499\n",
            "----------\n",
            "165 train loss:1.0094 train acc: 0.7343\n",
            "165 val loss:1.0345 val acc: 0.7046\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 166/499\n",
            "----------\n",
            "166 train loss:1.0048 train acc: 0.7348\n",
            "166 val loss:1.0313 val acc: 0.7027\n",
            "训练和验证耗费的时间1m36s\n",
            "Epoch 167/499\n",
            "----------\n",
            "167 train loss:1.0127 train acc: 0.7285\n",
            "167 val loss:1.0286 val acc: 0.6969\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 168/499\n",
            "----------\n",
            "168 train loss:1.0092 train acc: 0.7310\n",
            "168 val loss:1.0295 val acc: 0.7066\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 169/499\n",
            "----------\n",
            "169 train loss:1.0145 train acc: 0.7228\n",
            "169 val loss:1.0364 val acc: 0.6988\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 170/499\n",
            "----------\n",
            "170 train loss:1.0091 train acc: 0.7247\n",
            "170 val loss:1.0372 val acc: 0.6911\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 171/499\n",
            "----------\n",
            "171 train loss:1.0056 train acc: 0.7334\n",
            "171 val loss:1.0309 val acc: 0.7066\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 172/499\n",
            "----------\n",
            "172 train loss:1.0103 train acc: 0.7310\n",
            "172 val loss:1.0279 val acc: 0.7104\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 173/499\n",
            "----------\n",
            "173 train loss:1.0032 train acc: 0.7367\n",
            "173 val loss:1.0252 val acc: 0.7066\n",
            "训练和验证耗费的时间1m40s\n",
            "Epoch 174/499\n",
            "----------\n",
            "174 train loss:1.0056 train acc: 0.7348\n",
            "174 val loss:1.0224 val acc: 0.7181\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 175/499\n",
            "----------\n",
            "175 train loss:1.0048 train acc: 0.7329\n",
            "175 val loss:1.0331 val acc: 0.6988\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 176/499\n",
            "----------\n",
            "176 train loss:1.0084 train acc: 0.7314\n",
            "176 val loss:1.0314 val acc: 0.7046\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 177/499\n",
            "----------\n",
            "177 train loss:1.0084 train acc: 0.7281\n",
            "177 val loss:1.0293 val acc: 0.7008\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 178/499\n",
            "----------\n",
            "178 train loss:1.0188 train acc: 0.7213\n",
            "178 val loss:1.0340 val acc: 0.6988\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 179/499\n",
            "----------\n",
            "179 train loss:1.0028 train acc: 0.7387\n",
            "179 val loss:1.0308 val acc: 0.7027\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 180/499\n",
            "----------\n",
            "180 train loss:1.0078 train acc: 0.7300\n",
            "180 val loss:1.0334 val acc: 0.6911\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 181/499\n",
            "----------\n",
            "181 train loss:1.0050 train acc: 0.7392\n",
            "181 val loss:1.0232 val acc: 0.7181\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 182/499\n",
            "----------\n",
            "182 train loss:0.9971 train acc: 0.7488\n",
            "182 val loss:1.0248 val acc: 0.7181\n",
            "训练和验证耗费的时间1m45s\n",
            "Epoch 183/499\n",
            "----------\n",
            "183 train loss:1.0159 train acc: 0.7247\n",
            "183 val loss:1.0274 val acc: 0.7104\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 184/499\n",
            "----------\n",
            "184 train loss:1.0023 train acc: 0.7382\n",
            "184 val loss:1.0251 val acc: 0.7085\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 185/499\n",
            "----------\n",
            "185 train loss:1.0059 train acc: 0.7319\n",
            "185 val loss:1.0265 val acc: 0.7027\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 186/499\n",
            "----------\n",
            "186 train loss:1.0110 train acc: 0.7261\n",
            "186 val loss:1.0244 val acc: 0.7104\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 187/499\n",
            "----------\n",
            "187 train loss:1.0074 train acc: 0.7372\n",
            "187 val loss:1.0267 val acc: 0.7124\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 188/499\n",
            "----------\n",
            "188 train loss:1.0009 train acc: 0.7387\n",
            "188 val loss:1.0191 val acc: 0.7201\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 189/499\n",
            "----------\n",
            "189 train loss:1.0011 train acc: 0.7420\n",
            "189 val loss:1.0169 val acc: 0.7278\n",
            "训练和验证耗费的时间1m49s\n",
            "Epoch 190/499\n",
            "----------\n",
            "190 train loss:1.0020 train acc: 0.7387\n",
            "190 val loss:1.0221 val acc: 0.7201\n",
            "训练和验证耗费的时间1m50s\n",
            "Epoch 191/499\n",
            "----------\n",
            "191 train loss:1.0058 train acc: 0.7290\n",
            "191 val loss:1.0231 val acc: 0.7162\n",
            "训练和验证耗费的时间1m50s\n",
            "Epoch 192/499\n",
            "----------\n",
            "192 train loss:1.0022 train acc: 0.7440\n",
            "192 val loss:1.0287 val acc: 0.7066\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 193/499\n",
            "----------\n",
            "193 train loss:1.0060 train acc: 0.7377\n",
            "193 val loss:1.0305 val acc: 0.7085\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 194/499\n",
            "----------\n",
            "194 train loss:0.9969 train acc: 0.7454\n",
            "194 val loss:1.0228 val acc: 0.7162\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 195/499\n",
            "----------\n",
            "195 train loss:0.9991 train acc: 0.7401\n",
            "195 val loss:1.0160 val acc: 0.7239\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 196/499\n",
            "----------\n",
            "196 train loss:0.9971 train acc: 0.7396\n",
            "196 val loss:1.0251 val acc: 0.7124\n",
            "训练和验证耗费的时间1m53s\n",
            "Epoch 197/499\n",
            "----------\n",
            "197 train loss:1.0054 train acc: 0.7329\n",
            "197 val loss:1.0313 val acc: 0.6988\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 198/499\n",
            "----------\n",
            "198 train loss:0.9898 train acc: 0.7594\n",
            "198 val loss:1.0214 val acc: 0.7201\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 199/499\n",
            "----------\n",
            "199 train loss:0.9984 train acc: 0.7416\n",
            "199 val loss:1.0181 val acc: 0.7259\n",
            "训练和验证耗费的时间1m55s\n",
            "Epoch 200/499\n",
            "----------\n",
            "200 train loss:1.0016 train acc: 0.7377\n",
            "200 val loss:1.0226 val acc: 0.7124\n",
            "训练和验证耗费的时间1m55s\n",
            "Epoch 201/499\n",
            "----------\n",
            "201 train loss:1.0052 train acc: 0.7334\n",
            "201 val loss:1.0329 val acc: 0.7046\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 202/499\n",
            "----------\n",
            "202 train loss:1.0045 train acc: 0.7319\n",
            "202 val loss:1.0254 val acc: 0.7104\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 203/499\n",
            "----------\n",
            "203 train loss:0.9946 train acc: 0.7469\n",
            "203 val loss:1.0204 val acc: 0.7143\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 204/499\n",
            "----------\n",
            "204 train loss:0.9989 train acc: 0.7430\n",
            "204 val loss:1.0210 val acc: 0.7162\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 205/499\n",
            "----------\n",
            "205 train loss:1.0060 train acc: 0.7290\n",
            "205 val loss:1.0290 val acc: 0.7027\n",
            "训练和验证耗费的时间1m58s\n",
            "Epoch 206/499\n",
            "----------\n",
            "206 train loss:1.0009 train acc: 0.7420\n",
            "206 val loss:1.0269 val acc: 0.7066\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 207/499\n",
            "----------\n",
            "207 train loss:0.9932 train acc: 0.7498\n",
            "207 val loss:1.0248 val acc: 0.7104\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 208/499\n",
            "----------\n",
            "208 train loss:1.0014 train acc: 0.7367\n",
            "208 val loss:1.0265 val acc: 0.7046\n",
            "训练和验证耗费的时间1m60s\n",
            "Epoch 209/499\n",
            "----------\n",
            "209 train loss:0.9967 train acc: 0.7406\n",
            "209 val loss:1.0227 val acc: 0.7143\n",
            "训练和验证耗费的时间2m0s\n",
            "Epoch 210/499\n",
            "----------\n",
            "210 train loss:0.9949 train acc: 0.7459\n",
            "210 val loss:1.0211 val acc: 0.7201\n",
            "训练和验证耗费的时间2m1s\n",
            "Epoch 211/499\n",
            "----------\n",
            "211 train loss:0.9899 train acc: 0.7498\n",
            "211 val loss:1.0261 val acc: 0.7046\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 212/499\n",
            "----------\n",
            "212 train loss:1.0021 train acc: 0.7382\n",
            "212 val loss:1.0244 val acc: 0.7104\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 213/499\n",
            "----------\n",
            "213 train loss:1.0077 train acc: 0.7372\n",
            "213 val loss:1.0321 val acc: 0.7162\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 214/499\n",
            "----------\n",
            "214 train loss:1.0013 train acc: 0.7416\n",
            "214 val loss:1.0345 val acc: 0.7085\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 215/499\n",
            "----------\n",
            "215 train loss:1.0006 train acc: 0.7363\n",
            "215 val loss:1.0252 val acc: 0.7181\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 216/499\n",
            "----------\n",
            "216 train loss:0.9926 train acc: 0.7507\n",
            "216 val loss:1.0275 val acc: 0.7104\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 217/499\n",
            "----------\n",
            "217 train loss:0.9967 train acc: 0.7435\n",
            "217 val loss:1.0159 val acc: 0.7317\n",
            "训练和验证耗费的时间2m5s\n",
            "Epoch 218/499\n",
            "----------\n",
            "218 train loss:0.9921 train acc: 0.7435\n",
            "218 val loss:1.0168 val acc: 0.7181\n",
            "训练和验证耗费的时间2m6s\n",
            "Epoch 219/499\n",
            "----------\n",
            "219 train loss:0.9941 train acc: 0.7464\n",
            "219 val loss:1.0191 val acc: 0.7162\n",
            "训练和验证耗费的时间2m6s\n",
            "Epoch 220/499\n",
            "----------\n",
            "220 train loss:1.0031 train acc: 0.7377\n",
            "220 val loss:1.0229 val acc: 0.7162\n",
            "训练和验证耗费的时间2m7s\n",
            "Epoch 221/499\n",
            "----------\n",
            "221 train loss:1.0059 train acc: 0.7324\n",
            "221 val loss:1.0287 val acc: 0.7046\n",
            "训练和验证耗费的时间2m7s\n",
            "Epoch 222/499\n",
            "----------\n",
            "222 train loss:1.0010 train acc: 0.7406\n",
            "222 val loss:1.0296 val acc: 0.7066\n",
            "训练和验证耗费的时间2m8s\n",
            "Epoch 223/499\n",
            "----------\n",
            "223 train loss:0.9886 train acc: 0.7527\n",
            "223 val loss:1.0240 val acc: 0.7162\n",
            "训练和验证耗费的时间2m8s\n",
            "Epoch 224/499\n",
            "----------\n",
            "224 train loss:0.9943 train acc: 0.7445\n",
            "224 val loss:1.0285 val acc: 0.7124\n",
            "训练和验证耗费的时间2m9s\n",
            "Epoch 225/499\n",
            "----------\n",
            "225 train loss:0.9830 train acc: 0.7637\n",
            "225 val loss:1.0172 val acc: 0.7162\n",
            "训练和验证耗费的时间2m9s\n",
            "Epoch 226/499\n",
            "----------\n",
            "226 train loss:0.9971 train acc: 0.7396\n",
            "226 val loss:1.0302 val acc: 0.7008\n",
            "训练和验证耗费的时间2m10s\n",
            "Epoch 227/499\n",
            "----------\n",
            "227 train loss:1.0088 train acc: 0.7300\n",
            "227 val loss:1.0176 val acc: 0.7278\n",
            "训练和验证耗费的时间2m11s\n",
            "Epoch 228/499\n",
            "----------\n",
            "228 train loss:0.9899 train acc: 0.7531\n",
            "228 val loss:1.0240 val acc: 0.7008\n",
            "训练和验证耗费的时间2m11s\n",
            "Epoch 229/499\n",
            "----------\n",
            "229 train loss:0.9969 train acc: 0.7396\n",
            "229 val loss:1.0194 val acc: 0.7124\n",
            "训练和验证耗费的时间2m12s\n",
            "Epoch 230/499\n",
            "----------\n",
            "230 train loss:0.9922 train acc: 0.7488\n",
            "230 val loss:1.0239 val acc: 0.7104\n",
            "训练和验证耗费的时间2m12s\n",
            "Epoch 231/499\n",
            "----------\n",
            "231 train loss:0.9859 train acc: 0.7560\n",
            "231 val loss:1.0200 val acc: 0.7104\n",
            "训练和验证耗费的时间2m13s\n",
            "Epoch 232/499\n",
            "----------\n",
            "232 train loss:0.9931 train acc: 0.7454\n",
            "232 val loss:1.0229 val acc: 0.7201\n",
            "训练和验证耗费的时间2m14s\n",
            "Epoch 233/499\n",
            "----------\n",
            "233 train loss:0.9822 train acc: 0.7575\n",
            "233 val loss:1.0238 val acc: 0.7143\n",
            "训练和验证耗费的时间2m14s\n",
            "Epoch 234/499\n",
            "----------\n",
            "234 train loss:1.0043 train acc: 0.7329\n",
            "234 val loss:1.0144 val acc: 0.7297\n",
            "训练和验证耗费的时间2m15s\n",
            "Epoch 235/499\n",
            "----------\n",
            "235 train loss:0.9828 train acc: 0.7613\n",
            "235 val loss:1.0186 val acc: 0.7143\n",
            "训练和验证耗费的时间2m16s\n",
            "Epoch 236/499\n",
            "----------\n",
            "236 train loss:0.9925 train acc: 0.7507\n",
            "236 val loss:1.0220 val acc: 0.7124\n",
            "训练和验证耗费的时间2m16s\n",
            "Epoch 237/499\n",
            "----------\n",
            "237 train loss:0.9898 train acc: 0.7502\n",
            "237 val loss:1.0157 val acc: 0.7220\n",
            "训练和验证耗费的时间2m17s\n",
            "Epoch 238/499\n",
            "----------\n",
            "238 train loss:0.9943 train acc: 0.7478\n",
            "238 val loss:1.0156 val acc: 0.7220\n",
            "训练和验证耗费的时间2m17s\n",
            "Epoch 239/499\n",
            "----------\n",
            "239 train loss:0.9902 train acc: 0.7502\n",
            "239 val loss:1.0170 val acc: 0.7201\n",
            "训练和验证耗费的时间2m18s\n",
            "Epoch 240/499\n",
            "----------\n",
            "240 train loss:0.9963 train acc: 0.7411\n",
            "240 val loss:1.0167 val acc: 0.7162\n",
            "训练和验证耗费的时间2m18s\n",
            "Epoch 241/499\n",
            "----------\n",
            "241 train loss:0.9907 train acc: 0.7493\n",
            "241 val loss:1.0186 val acc: 0.7143\n",
            "训练和验证耗费的时间2m19s\n",
            "Epoch 242/499\n",
            "----------\n",
            "242 train loss:0.9880 train acc: 0.7555\n",
            "242 val loss:1.0187 val acc: 0.7259\n",
            "训练和验证耗费的时间2m20s\n",
            "Epoch 243/499\n",
            "----------\n",
            "243 train loss:0.9834 train acc: 0.7541\n",
            "243 val loss:1.0196 val acc: 0.7162\n",
            "训练和验证耗费的时间2m20s\n",
            "Epoch 244/499\n",
            "----------\n",
            "244 train loss:0.9964 train acc: 0.7425\n",
            "244 val loss:1.0220 val acc: 0.7143\n",
            "训练和验证耗费的时间2m21s\n",
            "Epoch 245/499\n",
            "----------\n",
            "245 train loss:0.9869 train acc: 0.7560\n",
            "245 val loss:1.0120 val acc: 0.7297\n",
            "训练和验证耗费的时间2m21s\n",
            "Epoch 246/499\n",
            "----------\n",
            "246 train loss:0.9947 train acc: 0.7459\n",
            "246 val loss:1.0172 val acc: 0.7124\n",
            "训练和验证耗费的时间2m22s\n",
            "Epoch 247/499\n",
            "----------\n",
            "247 train loss:0.9846 train acc: 0.7546\n",
            "247 val loss:1.0239 val acc: 0.7181\n",
            "训练和验证耗费的时间2m22s\n",
            "Epoch 248/499\n",
            "----------\n",
            "248 train loss:0.9910 train acc: 0.7483\n",
            "248 val loss:1.0101 val acc: 0.7297\n",
            "训练和验证耗费的时间2m23s\n",
            "Epoch 249/499\n",
            "----------\n",
            "249 train loss:0.9834 train acc: 0.7652\n",
            "249 val loss:1.0181 val acc: 0.7181\n",
            "训练和验证耗费的时间2m23s\n",
            "Epoch 250/499\n",
            "----------\n",
            "250 train loss:0.9937 train acc: 0.7425\n",
            "250 val loss:1.0178 val acc: 0.7143\n",
            "训练和验证耗费的时间2m24s\n",
            "Epoch 251/499\n",
            "----------\n",
            "251 train loss:0.9970 train acc: 0.7430\n",
            "251 val loss:1.0155 val acc: 0.7297\n",
            "训练和验证耗费的时间2m25s\n",
            "Epoch 252/499\n",
            "----------\n",
            "252 train loss:0.9869 train acc: 0.7536\n",
            "252 val loss:1.0157 val acc: 0.7278\n",
            "训练和验证耗费的时间2m25s\n",
            "Epoch 253/499\n",
            "----------\n",
            "253 train loss:0.9898 train acc: 0.7541\n",
            "253 val loss:1.0145 val acc: 0.7317\n",
            "训练和验证耗费的时间2m26s\n",
            "Epoch 254/499\n",
            "----------\n",
            "254 train loss:0.9860 train acc: 0.7531\n",
            "254 val loss:1.0188 val acc: 0.7181\n",
            "训练和验证耗费的时间2m26s\n",
            "Epoch 255/499\n",
            "----------\n",
            "255 train loss:0.9933 train acc: 0.7425\n",
            "255 val loss:1.0135 val acc: 0.7259\n",
            "训练和验证耗费的时间2m27s\n",
            "Epoch 256/499\n",
            "----------\n",
            "256 train loss:0.9852 train acc: 0.7522\n",
            "256 val loss:1.0096 val acc: 0.7413\n",
            "训练和验证耗费的时间2m27s\n",
            "Epoch 257/499\n",
            "----------\n",
            "257 train loss:0.9865 train acc: 0.7575\n",
            "257 val loss:1.0176 val acc: 0.7066\n",
            "训练和验证耗费的时间2m28s\n",
            "Epoch 258/499\n",
            "----------\n",
            "258 train loss:0.9944 train acc: 0.7522\n",
            "258 val loss:1.0128 val acc: 0.7259\n",
            "训练和验证耗费的时间2m29s\n",
            "Epoch 259/499\n",
            "----------\n",
            "259 train loss:0.9849 train acc: 0.7580\n",
            "259 val loss:1.0143 val acc: 0.7143\n",
            "训练和验证耗费的时间2m29s\n",
            "Epoch 260/499\n",
            "----------\n",
            "260 train loss:0.9937 train acc: 0.7473\n",
            "260 val loss:1.0176 val acc: 0.7220\n",
            "训练和验证耗费的时间2m30s\n",
            "Epoch 261/499\n",
            "----------\n",
            "261 train loss:0.9820 train acc: 0.7594\n",
            "261 val loss:1.0171 val acc: 0.7201\n",
            "训练和验证耗费的时间2m30s\n",
            "Epoch 262/499\n",
            "----------\n",
            "262 train loss:0.9843 train acc: 0.7551\n",
            "262 val loss:1.0106 val acc: 0.7278\n",
            "训练和验证耗费的时间2m31s\n",
            "Epoch 263/499\n",
            "----------\n",
            "263 train loss:0.9834 train acc: 0.7560\n",
            "263 val loss:1.0110 val acc: 0.7259\n",
            "训练和验证耗费的时间2m31s\n",
            "Epoch 264/499\n",
            "----------\n",
            "264 train loss:1.0038 train acc: 0.7343\n",
            "264 val loss:1.0159 val acc: 0.7201\n",
            "训练和验证耗费的时间2m32s\n",
            "Epoch 265/499\n",
            "----------\n",
            "265 train loss:0.9910 train acc: 0.7478\n",
            "265 val loss:1.0116 val acc: 0.7278\n",
            "训练和验证耗费的时间2m33s\n",
            "Epoch 266/499\n",
            "----------\n",
            "266 train loss:0.9869 train acc: 0.7507\n",
            "266 val loss:1.0166 val acc: 0.7104\n",
            "训练和验证耗费的时间2m33s\n",
            "Epoch 267/499\n",
            "----------\n",
            "267 train loss:0.9885 train acc: 0.7522\n",
            "267 val loss:1.0209 val acc: 0.7085\n",
            "训练和验证耗费的时间2m34s\n",
            "Epoch 268/499\n",
            "----------\n",
            "268 train loss:0.9789 train acc: 0.7560\n",
            "268 val loss:1.0134 val acc: 0.7317\n",
            "训练和验证耗费的时间2m34s\n",
            "Epoch 269/499\n",
            "----------\n",
            "269 train loss:0.9894 train acc: 0.7493\n",
            "269 val loss:1.0115 val acc: 0.7259\n",
            "训练和验证耗费的时间2m35s\n",
            "Epoch 270/499\n",
            "----------\n",
            "270 train loss:0.9875 train acc: 0.7464\n",
            "270 val loss:1.0118 val acc: 0.7239\n",
            "训练和验证耗费的时间2m35s\n",
            "Epoch 271/499\n",
            "----------\n",
            "271 train loss:0.9954 train acc: 0.7459\n",
            "271 val loss:1.0140 val acc: 0.7278\n",
            "训练和验证耗费的时间2m36s\n",
            "Epoch 272/499\n",
            "----------\n",
            "272 train loss:0.9928 train acc: 0.7498\n",
            "272 val loss:1.0129 val acc: 0.7162\n",
            "训练和验证耗费的时间2m37s\n",
            "Epoch 273/499\n",
            "----------\n",
            "273 train loss:0.9880 train acc: 0.7560\n",
            "273 val loss:1.0130 val acc: 0.7201\n",
            "训练和验证耗费的时间2m37s\n",
            "Epoch 274/499\n",
            "----------\n",
            "274 train loss:0.9877 train acc: 0.7527\n",
            "274 val loss:1.0163 val acc: 0.7259\n",
            "训练和验证耗费的时间2m38s\n",
            "Epoch 275/499\n",
            "----------\n",
            "275 train loss:0.9768 train acc: 0.7686\n",
            "275 val loss:1.0155 val acc: 0.7162\n",
            "训练和验证耗费的时间2m38s\n",
            "Epoch 276/499\n",
            "----------\n",
            "276 train loss:0.9875 train acc: 0.7527\n",
            "276 val loss:1.0173 val acc: 0.7181\n",
            "训练和验证耗费的时间2m39s\n",
            "Epoch 277/499\n",
            "----------\n",
            "277 train loss:0.9894 train acc: 0.7527\n",
            "277 val loss:1.0192 val acc: 0.7181\n",
            "训练和验证耗费的时间2m39s\n",
            "Epoch 278/499\n",
            "----------\n",
            "278 train loss:0.9989 train acc: 0.7382\n",
            "278 val loss:1.0057 val acc: 0.7317\n",
            "训练和验证耗费的时间2m40s\n",
            "Epoch 279/499\n",
            "----------\n",
            "279 train loss:0.9795 train acc: 0.7662\n",
            "279 val loss:1.0167 val acc: 0.7220\n",
            "训练和验证耗费的时间2m40s\n",
            "Epoch 280/499\n",
            "----------\n",
            "280 train loss:0.9880 train acc: 0.7502\n",
            "280 val loss:1.0155 val acc: 0.7259\n",
            "训练和验证耗费的时间2m41s\n",
            "Epoch 281/499\n",
            "----------\n",
            "281 train loss:0.9908 train acc: 0.7498\n",
            "281 val loss:1.0092 val acc: 0.7297\n",
            "训练和验证耗费的时间2m42s\n",
            "Epoch 282/499\n",
            "----------\n",
            "282 train loss:0.9812 train acc: 0.7633\n",
            "282 val loss:1.0118 val acc: 0.7201\n",
            "训练和验证耗费的时间2m42s\n",
            "Epoch 283/499\n",
            "----------\n",
            "283 train loss:0.9770 train acc: 0.7671\n",
            "283 val loss:1.0178 val acc: 0.7259\n",
            "训练和验证耗费的时间2m43s\n",
            "Epoch 284/499\n",
            "----------\n",
            "284 train loss:0.9896 train acc: 0.7517\n",
            "284 val loss:1.0153 val acc: 0.7085\n",
            "训练和验证耗费的时间2m43s\n",
            "Epoch 285/499\n",
            "----------\n",
            "285 train loss:0.9817 train acc: 0.7589\n",
            "285 val loss:1.0150 val acc: 0.7239\n",
            "训练和验证耗费的时间2m44s\n",
            "Epoch 286/499\n",
            "----------\n",
            "286 train loss:0.9807 train acc: 0.7628\n",
            "286 val loss:1.0134 val acc: 0.7239\n",
            "训练和验证耗费的时间2m44s\n",
            "Epoch 287/499\n",
            "----------\n",
            "287 train loss:0.9822 train acc: 0.7642\n",
            "287 val loss:1.0181 val acc: 0.7259\n",
            "训练和验证耗费的时间2m45s\n",
            "Epoch 288/499\n",
            "----------\n",
            "288 train loss:0.9873 train acc: 0.7517\n",
            "288 val loss:1.0192 val acc: 0.7181\n",
            "训练和验证耗费的时间2m46s\n",
            "Epoch 289/499\n",
            "----------\n",
            "289 train loss:0.9845 train acc: 0.7555\n",
            "289 val loss:1.0142 val acc: 0.7278\n",
            "训练和验证耗费的时间2m46s\n",
            "Epoch 290/499\n",
            "----------\n",
            "290 train loss:0.9880 train acc: 0.7570\n",
            "290 val loss:1.0153 val acc: 0.7259\n",
            "训练和验证耗费的时间2m47s\n",
            "Epoch 291/499\n",
            "----------\n",
            "291 train loss:0.9912 train acc: 0.7445\n",
            "291 val loss:1.0105 val acc: 0.7336\n",
            "训练和验证耗费的时间2m47s\n",
            "Epoch 292/499\n",
            "----------\n",
            "292 train loss:0.9890 train acc: 0.7464\n",
            "292 val loss:1.0096 val acc: 0.7259\n",
            "训练和验证耗费的时间2m48s\n",
            "Epoch 293/499\n",
            "----------\n",
            "293 train loss:0.9810 train acc: 0.7594\n",
            "293 val loss:1.0092 val acc: 0.7278\n",
            "训练和验证耗费的时间2m48s\n",
            "Epoch 294/499\n",
            "----------\n",
            "294 train loss:0.9836 train acc: 0.7555\n",
            "294 val loss:1.0156 val acc: 0.7162\n",
            "训练和验证耗费的时间2m49s\n",
            "Epoch 295/499\n",
            "----------\n",
            "295 train loss:0.9903 train acc: 0.7469\n",
            "295 val loss:1.0119 val acc: 0.7259\n",
            "训练和验证耗费的时间2m50s\n",
            "Epoch 296/499\n",
            "----------\n",
            "296 train loss:0.9836 train acc: 0.7570\n",
            "296 val loss:1.0094 val acc: 0.7394\n",
            "训练和验证耗费的时间2m50s\n",
            "Epoch 297/499\n",
            "----------\n",
            "297 train loss:0.9934 train acc: 0.7454\n",
            "297 val loss:1.0040 val acc: 0.7355\n",
            "训练和验证耗费的时间2m51s\n",
            "Epoch 298/499\n",
            "----------\n",
            "298 train loss:0.9801 train acc: 0.7618\n",
            "298 val loss:1.0013 val acc: 0.7510\n",
            "训练和验证耗费的时间2m51s\n",
            "Epoch 299/499\n",
            "----------\n",
            "299 train loss:0.9753 train acc: 0.7662\n",
            "299 val loss:1.0062 val acc: 0.7297\n",
            "训练和验证耗费的时间2m52s\n",
            "Epoch 300/499\n",
            "----------\n",
            "300 train loss:0.9920 train acc: 0.7478\n",
            "300 val loss:1.0118 val acc: 0.7355\n",
            "训练和验证耗费的时间2m53s\n",
            "Epoch 301/499\n",
            "----------\n",
            "301 train loss:0.9751 train acc: 0.7705\n",
            "301 val loss:1.0111 val acc: 0.7259\n",
            "训练和验证耗费的时间2m54s\n",
            "Epoch 302/499\n",
            "----------\n",
            "302 train loss:0.9877 train acc: 0.7546\n",
            "302 val loss:1.0065 val acc: 0.7336\n",
            "训练和验证耗费的时间2m54s\n",
            "Epoch 303/499\n",
            "----------\n",
            "303 train loss:0.9805 train acc: 0.7608\n",
            "303 val loss:1.0098 val acc: 0.7355\n",
            "训练和验证耗费的时间2m55s\n",
            "Epoch 304/499\n",
            "----------\n",
            "304 train loss:0.9824 train acc: 0.7565\n",
            "304 val loss:1.0056 val acc: 0.7297\n",
            "训练和验证耗费的时间2m55s\n",
            "Epoch 305/499\n",
            "----------\n",
            "305 train loss:0.9806 train acc: 0.7575\n",
            "305 val loss:1.0147 val acc: 0.7297\n",
            "训练和验证耗费的时间2m56s\n",
            "Epoch 306/499\n",
            "----------\n",
            "306 train loss:0.9854 train acc: 0.7575\n",
            "306 val loss:1.0081 val acc: 0.7336\n",
            "训练和验证耗费的时间2m56s\n",
            "Epoch 307/499\n",
            "----------\n",
            "307 train loss:0.9835 train acc: 0.7594\n",
            "307 val loss:0.9982 val acc: 0.7394\n",
            "训练和验证耗费的时间2m57s\n",
            "Epoch 308/499\n",
            "----------\n",
            "308 train loss:0.9751 train acc: 0.7662\n",
            "308 val loss:1.0057 val acc: 0.7317\n",
            "训练和验证耗费的时间2m58s\n",
            "Epoch 309/499\n",
            "----------\n",
            "309 train loss:0.9789 train acc: 0.7594\n",
            "309 val loss:1.0159 val acc: 0.7201\n",
            "训练和验证耗费的时间2m58s\n",
            "Epoch 310/499\n",
            "----------\n",
            "310 train loss:0.9833 train acc: 0.7570\n",
            "310 val loss:1.0094 val acc: 0.7297\n",
            "训练和验证耗费的时间2m59s\n",
            "Epoch 311/499\n",
            "----------\n",
            "311 train loss:0.9849 train acc: 0.7575\n",
            "311 val loss:1.0037 val acc: 0.7432\n",
            "训练和验证耗费的时间2m59s\n",
            "Epoch 312/499\n",
            "----------\n",
            "312 train loss:0.9891 train acc: 0.7507\n",
            "312 val loss:1.0004 val acc: 0.7452\n",
            "训练和验证耗费的时间2m60s\n",
            "Epoch 313/499\n",
            "----------\n",
            "313 train loss:0.9852 train acc: 0.7502\n",
            "313 val loss:1.0147 val acc: 0.7220\n",
            "训练和验证耗费的时间3m0s\n",
            "Epoch 314/499\n",
            "----------\n",
            "314 train loss:0.9924 train acc: 0.7498\n",
            "314 val loss:1.0077 val acc: 0.7355\n",
            "训练和验证耗费的时间3m1s\n",
            "Epoch 315/499\n",
            "----------\n",
            "315 train loss:0.9735 train acc: 0.7642\n",
            "315 val loss:1.0071 val acc: 0.7201\n",
            "训练和验证耗费的时间3m2s\n",
            "Epoch 316/499\n",
            "----------\n",
            "316 train loss:0.9787 train acc: 0.7628\n",
            "316 val loss:1.0012 val acc: 0.7317\n",
            "训练和验证耗费的时间3m2s\n",
            "Epoch 317/499\n",
            "----------\n",
            "317 train loss:0.9907 train acc: 0.7502\n",
            "317 val loss:1.0032 val acc: 0.7355\n",
            "训练和验证耗费的时间3m3s\n",
            "Epoch 318/499\n",
            "----------\n",
            "318 train loss:0.9835 train acc: 0.7517\n",
            "318 val loss:1.0114 val acc: 0.7355\n",
            "训练和验证耗费的时间3m3s\n",
            "Epoch 319/499\n",
            "----------\n",
            "319 train loss:0.9845 train acc: 0.7584\n",
            "319 val loss:1.0045 val acc: 0.7317\n",
            "训练和验证耗费的时间3m4s\n",
            "Epoch 320/499\n",
            "----------\n",
            "320 train loss:0.9825 train acc: 0.7580\n",
            "320 val loss:1.0071 val acc: 0.7317\n",
            "训练和验证耗费的时间3m4s\n",
            "Epoch 321/499\n",
            "----------\n",
            "321 train loss:0.9802 train acc: 0.7608\n",
            "321 val loss:1.0123 val acc: 0.7278\n",
            "训练和验证耗费的时间3m5s\n",
            "Epoch 322/499\n",
            "----------\n",
            "322 train loss:0.9798 train acc: 0.7594\n",
            "322 val loss:1.0052 val acc: 0.7317\n",
            "训练和验证耗费的时间3m5s\n",
            "Epoch 323/499\n",
            "----------\n",
            "323 train loss:0.9882 train acc: 0.7536\n",
            "323 val loss:1.0147 val acc: 0.7181\n",
            "训练和验证耗费的时间3m6s\n",
            "Epoch 324/499\n",
            "----------\n",
            "324 train loss:0.9747 train acc: 0.7662\n",
            "324 val loss:1.0030 val acc: 0.7432\n",
            "训练和验证耗费的时间3m7s\n",
            "Epoch 325/499\n",
            "----------\n",
            "325 train loss:0.9714 train acc: 0.7748\n",
            "325 val loss:1.0075 val acc: 0.7220\n",
            "训练和验证耗费的时间3m7s\n",
            "Epoch 326/499\n",
            "----------\n",
            "326 train loss:0.9846 train acc: 0.7584\n",
            "326 val loss:1.0073 val acc: 0.7336\n",
            "训练和验证耗费的时间3m8s\n",
            "Epoch 327/499\n",
            "----------\n",
            "327 train loss:0.9766 train acc: 0.7681\n",
            "327 val loss:1.0008 val acc: 0.7394\n",
            "训练和验证耗费的时间3m8s\n",
            "Epoch 328/499\n",
            "----------\n",
            "328 train loss:0.9813 train acc: 0.7618\n",
            "328 val loss:1.0028 val acc: 0.7297\n",
            "训练和验证耗费的时间3m9s\n",
            "Epoch 329/499\n",
            "----------\n",
            "329 train loss:0.9850 train acc: 0.7517\n",
            "329 val loss:1.0016 val acc: 0.7336\n",
            "训练和验证耗费的时间3m9s\n",
            "Epoch 330/499\n",
            "----------\n",
            "330 train loss:0.9857 train acc: 0.7565\n",
            "330 val loss:1.0044 val acc: 0.7394\n",
            "训练和验证耗费的时间3m10s\n",
            "Epoch 331/499\n",
            "----------\n",
            "331 train loss:0.9778 train acc: 0.7594\n",
            "331 val loss:1.0038 val acc: 0.7278\n",
            "训练和验证耗费的时间3m10s\n",
            "Epoch 332/499\n",
            "----------\n",
            "332 train loss:0.9813 train acc: 0.7628\n",
            "332 val loss:1.0055 val acc: 0.7297\n",
            "训练和验证耗费的时间3m11s\n",
            "Epoch 333/499\n",
            "----------\n",
            "333 train loss:0.9841 train acc: 0.7531\n",
            "333 val loss:1.0126 val acc: 0.7162\n",
            "训练和验证耗费的时间3m12s\n",
            "Epoch 334/499\n",
            "----------\n",
            "334 train loss:0.9808 train acc: 0.7608\n",
            "334 val loss:1.0078 val acc: 0.7297\n",
            "训练和验证耗费的时间3m12s\n",
            "Epoch 335/499\n",
            "----------\n",
            "335 train loss:0.9850 train acc: 0.7512\n",
            "335 val loss:1.0077 val acc: 0.7336\n",
            "训练和验证耗费的时间3m13s\n",
            "Epoch 336/499\n",
            "----------\n",
            "336 train loss:0.9788 train acc: 0.7628\n",
            "336 val loss:1.0049 val acc: 0.7181\n",
            "训练和验证耗费的时间3m13s\n",
            "Epoch 337/499\n",
            "----------\n",
            "337 train loss:0.9766 train acc: 0.7690\n",
            "337 val loss:1.0017 val acc: 0.7355\n",
            "训练和验证耗费的时间3m14s\n",
            "Epoch 338/499\n",
            "----------\n",
            "338 train loss:0.9808 train acc: 0.7628\n",
            "338 val loss:1.0018 val acc: 0.7375\n",
            "训练和验证耗费的时间3m15s\n",
            "Epoch 339/499\n",
            "----------\n",
            "339 train loss:0.9868 train acc: 0.7536\n",
            "339 val loss:1.0062 val acc: 0.7297\n",
            "训练和验证耗费的时间3m15s\n",
            "Epoch 340/499\n",
            "----------\n",
            "340 train loss:0.9921 train acc: 0.7507\n",
            "340 val loss:1.0064 val acc: 0.7336\n",
            "训练和验证耗费的时间3m16s\n",
            "Epoch 341/499\n",
            "----------\n",
            "341 train loss:0.9810 train acc: 0.7594\n",
            "341 val loss:1.0050 val acc: 0.7355\n",
            "训练和验证耗费的时间3m16s\n",
            "Epoch 342/499\n",
            "----------\n",
            "342 train loss:0.9802 train acc: 0.7599\n",
            "342 val loss:1.0049 val acc: 0.7413\n",
            "训练和验证耗费的时间3m17s\n",
            "Epoch 343/499\n",
            "----------\n",
            "343 train loss:0.9719 train acc: 0.7666\n",
            "343 val loss:1.0117 val acc: 0.7278\n",
            "训练和验证耗费的时间3m17s\n",
            "Epoch 344/499\n",
            "----------\n",
            "344 train loss:0.9899 train acc: 0.7493\n",
            "344 val loss:1.0132 val acc: 0.7278\n",
            "训练和验证耗费的时间3m18s\n",
            "Epoch 345/499\n",
            "----------\n",
            "345 train loss:0.9721 train acc: 0.7652\n",
            "345 val loss:1.0078 val acc: 0.7297\n",
            "训练和验证耗费的时间3m18s\n",
            "Epoch 346/499\n",
            "----------\n",
            "346 train loss:0.9811 train acc: 0.7628\n",
            "346 val loss:1.0066 val acc: 0.7394\n",
            "训练和验证耗费的时间3m19s\n",
            "Epoch 347/499\n",
            "----------\n",
            "347 train loss:0.9675 train acc: 0.7719\n",
            "347 val loss:1.0023 val acc: 0.7336\n",
            "训练和验证耗费的时间3m20s\n",
            "Epoch 348/499\n",
            "----------\n",
            "348 train loss:0.9727 train acc: 0.7705\n",
            "348 val loss:1.0004 val acc: 0.7413\n",
            "训练和验证耗费的时间3m20s\n",
            "Epoch 349/499\n",
            "----------\n",
            "349 train loss:0.9744 train acc: 0.7686\n",
            "349 val loss:1.0011 val acc: 0.7413\n",
            "训练和验证耗费的时间3m21s\n",
            "Epoch 350/499\n",
            "----------\n",
            "350 train loss:0.9815 train acc: 0.7589\n",
            "350 val loss:1.0071 val acc: 0.7297\n",
            "训练和验证耗费的时间3m21s\n",
            "Epoch 351/499\n",
            "----------\n",
            "351 train loss:0.9779 train acc: 0.7599\n",
            "351 val loss:1.0043 val acc: 0.7375\n",
            "训练和验证耗费的时间3m22s\n",
            "Epoch 352/499\n",
            "----------\n",
            "352 train loss:0.9709 train acc: 0.7719\n",
            "352 val loss:0.9981 val acc: 0.7413\n",
            "训练和验证耗费的时间3m22s\n",
            "Epoch 353/499\n",
            "----------\n",
            "353 train loss:0.9762 train acc: 0.7666\n",
            "353 val loss:0.9988 val acc: 0.7413\n",
            "训练和验证耗费的时间3m23s\n",
            "Epoch 354/499\n",
            "----------\n",
            "354 train loss:0.9710 train acc: 0.7729\n",
            "354 val loss:0.9987 val acc: 0.7375\n",
            "训练和验证耗费的时间3m23s\n",
            "Epoch 355/499\n",
            "----------\n",
            "355 train loss:0.9789 train acc: 0.7594\n",
            "355 val loss:1.0009 val acc: 0.7375\n",
            "训练和验证耗费的时间3m24s\n",
            "Epoch 356/499\n",
            "----------\n",
            "356 train loss:0.9779 train acc: 0.7618\n",
            "356 val loss:1.0022 val acc: 0.7336\n",
            "训练和验证耗费的时间3m25s\n",
            "Epoch 357/499\n",
            "----------\n",
            "357 train loss:0.9789 train acc: 0.7565\n",
            "357 val loss:1.0028 val acc: 0.7394\n",
            "训练和验证耗费的时间3m25s\n",
            "Epoch 358/499\n",
            "----------\n",
            "358 train loss:0.9720 train acc: 0.7666\n",
            "358 val loss:1.0041 val acc: 0.7336\n",
            "训练和验证耗费的时间3m26s\n",
            "Epoch 359/499\n",
            "----------\n",
            "359 train loss:0.9801 train acc: 0.7613\n",
            "359 val loss:1.0081 val acc: 0.7317\n",
            "训练和验证耗费的时间3m26s\n",
            "Epoch 360/499\n",
            "----------\n",
            "360 train loss:0.9747 train acc: 0.7628\n",
            "360 val loss:1.0031 val acc: 0.7317\n",
            "训练和验证耗费的时间3m27s\n",
            "Epoch 361/499\n",
            "----------\n",
            "361 train loss:0.9728 train acc: 0.7671\n",
            "361 val loss:1.0059 val acc: 0.7394\n",
            "训练和验证耗费的时间3m27s\n",
            "Epoch 362/499\n",
            "----------\n",
            "362 train loss:0.9811 train acc: 0.7575\n",
            "362 val loss:1.0001 val acc: 0.7394\n",
            "训练和验证耗费的时间3m28s\n",
            "Epoch 363/499\n",
            "----------\n",
            "363 train loss:0.9796 train acc: 0.7618\n",
            "363 val loss:1.0070 val acc: 0.7239\n",
            "训练和验证耗费的时间3m29s\n",
            "Epoch 364/499\n",
            "----------\n",
            "364 train loss:0.9787 train acc: 0.7594\n",
            "364 val loss:1.0085 val acc: 0.7278\n",
            "训练和验证耗费的时间3m29s\n",
            "Epoch 365/499\n",
            "----------\n",
            "365 train loss:0.9714 train acc: 0.7715\n",
            "365 val loss:1.0033 val acc: 0.7413\n",
            "训练和验证耗费的时间3m30s\n",
            "Epoch 366/499\n",
            "----------\n",
            "366 train loss:0.9694 train acc: 0.7705\n",
            "366 val loss:1.0070 val acc: 0.7297\n",
            "训练和验证耗费的时间3m30s\n",
            "Epoch 367/499\n",
            "----------\n",
            "367 train loss:0.9856 train acc: 0.7522\n",
            "367 val loss:1.0028 val acc: 0.7413\n",
            "训练和验证耗费的时间3m31s\n",
            "Epoch 368/499\n",
            "----------\n",
            "368 train loss:0.9753 train acc: 0.7681\n",
            "368 val loss:1.0130 val acc: 0.7201\n",
            "训练和验证耗费的时间3m31s\n",
            "Epoch 369/499\n",
            "----------\n",
            "369 train loss:0.9754 train acc: 0.7662\n",
            "369 val loss:1.0095 val acc: 0.7201\n",
            "训练和验证耗费的时间3m32s\n",
            "Epoch 370/499\n",
            "----------\n",
            "370 train loss:0.9711 train acc: 0.7705\n",
            "370 val loss:1.0081 val acc: 0.7336\n",
            "训练和验证耗费的时间3m33s\n",
            "Epoch 371/499\n",
            "----------\n",
            "371 train loss:0.9773 train acc: 0.7575\n",
            "371 val loss:1.0045 val acc: 0.7278\n",
            "训练和验证耗费的时间3m33s\n",
            "Epoch 372/499\n",
            "----------\n",
            "372 train loss:0.9703 train acc: 0.7710\n",
            "372 val loss:1.0051 val acc: 0.7317\n",
            "训练和验证耗费的时间3m34s\n",
            "Epoch 373/499\n",
            "----------\n",
            "373 train loss:0.9723 train acc: 0.7710\n",
            "373 val loss:1.0038 val acc: 0.7394\n",
            "训练和验证耗费的时间3m34s\n",
            "Epoch 374/499\n",
            "----------\n",
            "374 train loss:0.9774 train acc: 0.7599\n",
            "374 val loss:1.0121 val acc: 0.7181\n",
            "训练和验证耗费的时间3m35s\n",
            "Epoch 375/499\n",
            "----------\n",
            "375 train loss:0.9815 train acc: 0.7604\n",
            "375 val loss:1.0041 val acc: 0.7394\n",
            "训练和验证耗费的时间3m35s\n",
            "Epoch 376/499\n",
            "----------\n",
            "376 train loss:0.9776 train acc: 0.7662\n",
            "376 val loss:1.0079 val acc: 0.7317\n",
            "训练和验证耗费的时间3m36s\n",
            "Epoch 377/499\n",
            "----------\n",
            "377 train loss:0.9695 train acc: 0.7666\n",
            "377 val loss:1.0038 val acc: 0.7317\n",
            "训练和验证耗费的时间3m36s\n",
            "Epoch 378/499\n",
            "----------\n",
            "378 train loss:0.9680 train acc: 0.7768\n",
            "378 val loss:1.0066 val acc: 0.7239\n",
            "训练和验证耗费的时间3m37s\n",
            "Epoch 379/499\n",
            "----------\n",
            "379 train loss:0.9696 train acc: 0.7710\n",
            "379 val loss:1.0066 val acc: 0.7317\n",
            "训练和验证耗费的时间3m38s\n",
            "Epoch 380/499\n",
            "----------\n",
            "380 train loss:0.9629 train acc: 0.7743\n",
            "380 val loss:1.0082 val acc: 0.7259\n",
            "训练和验证耗费的时间3m38s\n",
            "Epoch 381/499\n",
            "----------\n",
            "381 train loss:0.9671 train acc: 0.7753\n",
            "381 val loss:1.0064 val acc: 0.7220\n",
            "训练和验证耗费的时间3m39s\n",
            "Epoch 382/499\n",
            "----------\n",
            "382 train loss:0.9776 train acc: 0.7604\n",
            "382 val loss:1.0145 val acc: 0.7181\n",
            "训练和验证耗费的时间3m39s\n",
            "Epoch 383/499\n",
            "----------\n",
            "383 train loss:0.9719 train acc: 0.7690\n",
            "383 val loss:1.0121 val acc: 0.7181\n",
            "训练和验证耗费的时间3m40s\n",
            "Epoch 384/499\n",
            "----------\n",
            "384 train loss:0.9737 train acc: 0.7705\n",
            "384 val loss:1.0086 val acc: 0.7259\n",
            "训练和验证耗费的时间3m40s\n",
            "Epoch 385/499\n",
            "----------\n",
            "385 train loss:0.9697 train acc: 0.7710\n",
            "385 val loss:1.0076 val acc: 0.7220\n",
            "训练和验证耗费的时间3m41s\n",
            "Epoch 386/499\n",
            "----------\n",
            "386 train loss:0.9813 train acc: 0.7589\n",
            "386 val loss:1.0081 val acc: 0.7297\n",
            "训练和验证耗费的时间3m42s\n",
            "Epoch 387/499\n",
            "----------\n",
            "387 train loss:0.9719 train acc: 0.7724\n",
            "387 val loss:1.0092 val acc: 0.7278\n",
            "训练和验证耗费的时间3m42s\n",
            "Epoch 388/499\n",
            "----------\n",
            "388 train loss:0.9772 train acc: 0.7594\n",
            "388 val loss:1.0116 val acc: 0.7278\n",
            "训练和验证耗费的时间3m43s\n",
            "Epoch 389/499\n",
            "----------\n",
            "389 train loss:0.9684 train acc: 0.7705\n",
            "389 val loss:1.0093 val acc: 0.7239\n",
            "训练和验证耗费的时间3m43s\n",
            "Epoch 390/499\n",
            "----------\n",
            "390 train loss:0.9727 train acc: 0.7633\n",
            "390 val loss:1.0030 val acc: 0.7413\n",
            "训练和验证耗费的时间3m44s\n",
            "Epoch 391/499\n",
            "----------\n",
            "391 train loss:0.9756 train acc: 0.7652\n",
            "391 val loss:1.0067 val acc: 0.7432\n",
            "训练和验证耗费的时间3m44s\n",
            "Epoch 392/499\n",
            "----------\n",
            "392 train loss:0.9683 train acc: 0.7768\n",
            "392 val loss:1.0057 val acc: 0.7394\n",
            "训练和验证耗费的时间3m45s\n",
            "Epoch 393/499\n",
            "----------\n",
            "393 train loss:0.9772 train acc: 0.7604\n",
            "393 val loss:1.0051 val acc: 0.7297\n",
            "训练和验证耗费的时间3m45s\n",
            "Epoch 394/499\n",
            "----------\n",
            "394 train loss:0.9810 train acc: 0.7604\n",
            "394 val loss:1.0076 val acc: 0.7317\n",
            "训练和验证耗费的时间3m46s\n",
            "Epoch 395/499\n",
            "----------\n",
            "395 train loss:0.9719 train acc: 0.7671\n",
            "395 val loss:1.0212 val acc: 0.7027\n",
            "训练和验证耗费的时间3m47s\n",
            "Epoch 396/499\n",
            "----------\n",
            "396 train loss:0.9857 train acc: 0.7512\n",
            "396 val loss:1.0118 val acc: 0.7297\n",
            "训练和验证耗费的时间3m47s\n",
            "Epoch 397/499\n",
            "----------\n",
            "397 train loss:0.9719 train acc: 0.7690\n",
            "397 val loss:1.0088 val acc: 0.7375\n",
            "训练和验证耗费的时间3m48s\n",
            "Epoch 398/499\n",
            "----------\n",
            "398 train loss:0.9675 train acc: 0.7743\n",
            "398 val loss:1.0067 val acc: 0.7394\n",
            "训练和验证耗费的时间3m48s\n",
            "Epoch 399/499\n",
            "----------\n",
            "399 train loss:0.9766 train acc: 0.7652\n",
            "399 val loss:1.0083 val acc: 0.7336\n",
            "训练和验证耗费的时间3m49s\n",
            "Epoch 400/499\n",
            "----------\n",
            "400 train loss:0.9703 train acc: 0.7724\n",
            "400 val loss:1.0078 val acc: 0.7317\n",
            "训练和验证耗费的时间3m50s\n",
            "Epoch 401/499\n",
            "----------\n",
            "401 train loss:0.9737 train acc: 0.7715\n",
            "401 val loss:1.0077 val acc: 0.7297\n",
            "训练和验证耗费的时间3m50s\n",
            "Epoch 402/499\n",
            "----------\n",
            "402 train loss:0.9731 train acc: 0.7715\n",
            "402 val loss:1.0111 val acc: 0.7317\n",
            "训练和验证耗费的时间3m51s\n",
            "Epoch 403/499\n",
            "----------\n",
            "403 train loss:0.9750 train acc: 0.7719\n",
            "403 val loss:1.0081 val acc: 0.7278\n",
            "训练和验证耗费的时间3m51s\n",
            "Epoch 404/499\n",
            "----------\n",
            "404 train loss:0.9583 train acc: 0.7859\n",
            "404 val loss:1.0127 val acc: 0.7143\n",
            "训练和验证耗费的时间3m52s\n",
            "Epoch 405/499\n",
            "----------\n",
            "405 train loss:0.9720 train acc: 0.7705\n",
            "405 val loss:1.0068 val acc: 0.7317\n",
            "训练和验证耗费的时间3m52s\n",
            "Epoch 406/499\n",
            "----------\n",
            "406 train loss:0.9765 train acc: 0.7628\n",
            "406 val loss:1.0042 val acc: 0.7336\n",
            "训练和验证耗费的时间3m53s\n",
            "Epoch 407/499\n",
            "----------\n",
            "407 train loss:0.9725 train acc: 0.7671\n",
            "407 val loss:1.0057 val acc: 0.7278\n",
            "训练和验证耗费的时间3m53s\n",
            "Epoch 408/499\n",
            "----------\n",
            "408 train loss:0.9731 train acc: 0.7690\n",
            "408 val loss:1.0057 val acc: 0.7297\n",
            "训练和验证耗费的时间3m54s\n",
            "Epoch 409/499\n",
            "----------\n",
            "409 train loss:0.9700 train acc: 0.7695\n",
            "409 val loss:1.0082 val acc: 0.7259\n",
            "训练和验证耗费的时间3m55s\n",
            "Epoch 410/499\n",
            "----------\n",
            "410 train loss:0.9739 train acc: 0.7662\n",
            "410 val loss:1.0035 val acc: 0.7375\n",
            "训练和验证耗费的时间3m55s\n",
            "Epoch 411/499\n",
            "----------\n",
            "411 train loss:0.9696 train acc: 0.7690\n",
            "411 val loss:1.0098 val acc: 0.7297\n",
            "训练和验证耗费的时间3m56s\n",
            "Epoch 412/499\n",
            "----------\n",
            "412 train loss:0.9658 train acc: 0.7777\n",
            "412 val loss:1.0084 val acc: 0.7259\n",
            "训练和验证耗费的时间3m56s\n",
            "Epoch 413/499\n",
            "----------\n",
            "413 train loss:0.9732 train acc: 0.7662\n",
            "413 val loss:1.0130 val acc: 0.7220\n",
            "训练和验证耗费的时间3m57s\n",
            "Epoch 414/499\n",
            "----------\n",
            "414 train loss:0.9720 train acc: 0.7695\n",
            "414 val loss:1.0079 val acc: 0.7297\n",
            "训练和验证耗费的时间3m57s\n",
            "Epoch 415/499\n",
            "----------\n",
            "415 train loss:0.9807 train acc: 0.7565\n",
            "415 val loss:1.0109 val acc: 0.7259\n",
            "训练和验证耗费的时间3m58s\n",
            "Epoch 416/499\n",
            "----------\n",
            "416 train loss:0.9778 train acc: 0.7589\n",
            "416 val loss:1.0040 val acc: 0.7317\n",
            "训练和验证耗费的时间3m58s\n",
            "Epoch 417/499\n",
            "----------\n",
            "417 train loss:0.9773 train acc: 0.7633\n",
            "417 val loss:1.0026 val acc: 0.7394\n",
            "训练和验证耗费的时间3m59s\n",
            "Epoch 418/499\n",
            "----------\n",
            "418 train loss:0.9645 train acc: 0.7729\n",
            "418 val loss:1.0032 val acc: 0.7413\n",
            "训练和验证耗费的时间3m60s\n",
            "Epoch 419/499\n",
            "----------\n",
            "419 train loss:0.9705 train acc: 0.7637\n",
            "419 val loss:1.0005 val acc: 0.7432\n",
            "训练和验证耗费的时间4m0s\n",
            "Epoch 420/499\n",
            "----------\n",
            "420 train loss:0.9652 train acc: 0.7743\n",
            "420 val loss:0.9984 val acc: 0.7529\n",
            "训练和验证耗费的时间4m1s\n",
            "Epoch 421/499\n",
            "----------\n",
            "421 train loss:0.9683 train acc: 0.7743\n",
            "421 val loss:1.0037 val acc: 0.7413\n",
            "训练和验证耗费的时间4m1s\n",
            "Epoch 422/499\n",
            "----------\n",
            "422 train loss:0.9677 train acc: 0.7743\n",
            "422 val loss:1.0040 val acc: 0.7375\n",
            "训练和验证耗费的时间4m2s\n",
            "Epoch 423/499\n",
            "----------\n",
            "423 train loss:0.9645 train acc: 0.7724\n",
            "423 val loss:1.0021 val acc: 0.7336\n",
            "训练和验证耗费的时间4m3s\n",
            "Epoch 424/499\n",
            "----------\n",
            "424 train loss:0.9741 train acc: 0.7633\n",
            "424 val loss:1.0065 val acc: 0.7317\n",
            "训练和验证耗费的时间4m3s\n",
            "Epoch 425/499\n",
            "----------\n",
            "425 train loss:0.9717 train acc: 0.7715\n",
            "425 val loss:1.0018 val acc: 0.7394\n",
            "训练和验证耗费的时间4m4s\n",
            "Epoch 426/499\n",
            "----------\n",
            "426 train loss:0.9571 train acc: 0.7883\n",
            "426 val loss:1.0137 val acc: 0.7201\n",
            "训练和验证耗费的时间4m4s\n",
            "Epoch 427/499\n",
            "----------\n",
            "427 train loss:0.9706 train acc: 0.7681\n",
            "427 val loss:1.0037 val acc: 0.7336\n",
            "训练和验证耗费的时间4m5s\n",
            "Epoch 428/499\n",
            "----------\n",
            "428 train loss:0.9723 train acc: 0.7657\n",
            "428 val loss:1.0014 val acc: 0.7336\n",
            "训练和验证耗费的时间4m5s\n",
            "Epoch 429/499\n",
            "----------\n",
            "429 train loss:0.9722 train acc: 0.7647\n",
            "429 val loss:1.0015 val acc: 0.7471\n",
            "训练和验证耗费的时间4m6s\n",
            "Epoch 430/499\n",
            "----------\n",
            "430 train loss:0.9714 train acc: 0.7686\n",
            "430 val loss:1.0001 val acc: 0.7432\n",
            "训练和验证耗费的时间4m6s\n",
            "Epoch 431/499\n",
            "----------\n",
            "431 train loss:0.9740 train acc: 0.7618\n",
            "431 val loss:0.9959 val acc: 0.7490\n",
            "训练和验证耗费的时间4m7s\n",
            "Epoch 432/499\n",
            "----------\n",
            "432 train loss:0.9773 train acc: 0.7633\n",
            "432 val loss:1.0015 val acc: 0.7355\n",
            "训练和验证耗费的时间4m8s\n",
            "Epoch 433/499\n",
            "----------\n",
            "433 train loss:0.9702 train acc: 0.7647\n",
            "433 val loss:1.0034 val acc: 0.7317\n",
            "训练和验证耗费的时间4m8s\n",
            "Epoch 434/499\n",
            "----------\n",
            "434 train loss:0.9708 train acc: 0.7705\n",
            "434 val loss:1.0009 val acc: 0.7336\n",
            "训练和验证耗费的时间4m9s\n",
            "Epoch 435/499\n",
            "----------\n",
            "435 train loss:0.9824 train acc: 0.7551\n",
            "435 val loss:1.0033 val acc: 0.7259\n",
            "训练和验证耗费的时间4m9s\n",
            "Epoch 436/499\n",
            "----------\n",
            "436 train loss:0.9642 train acc: 0.7753\n",
            "436 val loss:1.0067 val acc: 0.7355\n",
            "训练和验证耗费的时间4m10s\n",
            "Epoch 437/499\n",
            "----------\n",
            "437 train loss:0.9709 train acc: 0.7666\n",
            "437 val loss:0.9997 val acc: 0.7355\n",
            "训练和验证耗费的时间4m10s\n",
            "Epoch 438/499\n",
            "----------\n",
            "438 train loss:0.9673 train acc: 0.7705\n",
            "438 val loss:1.0014 val acc: 0.7355\n",
            "训练和验证耗费的时间4m11s\n",
            "Epoch 439/499\n",
            "----------\n",
            "439 train loss:0.9634 train acc: 0.7792\n",
            "439 val loss:1.0072 val acc: 0.7278\n",
            "训练和验证耗费的时间4m11s\n",
            "Epoch 440/499\n",
            "----------\n",
            "440 train loss:0.9764 train acc: 0.7647\n",
            "440 val loss:1.0191 val acc: 0.7143\n",
            "训练和验证耗费的时间4m12s\n",
            "Epoch 441/499\n",
            "----------\n",
            "441 train loss:0.9721 train acc: 0.7686\n",
            "441 val loss:1.0037 val acc: 0.7413\n",
            "训练和验证耗费的时间4m13s\n",
            "Epoch 442/499\n",
            "----------\n",
            "442 train loss:0.9762 train acc: 0.7662\n",
            "442 val loss:0.9998 val acc: 0.7452\n",
            "训练和验证耗费的时间4m13s\n",
            "Epoch 443/499\n",
            "----------\n",
            "443 train loss:0.9640 train acc: 0.7787\n",
            "443 val loss:1.0042 val acc: 0.7375\n",
            "训练和验证耗费的时间4m14s\n",
            "Epoch 444/499\n",
            "----------\n",
            "444 train loss:0.9734 train acc: 0.7642\n",
            "444 val loss:0.9997 val acc: 0.7375\n",
            "训练和验证耗费的时间4m14s\n",
            "Epoch 445/499\n",
            "----------\n",
            "445 train loss:0.9715 train acc: 0.7719\n",
            "445 val loss:1.0013 val acc: 0.7278\n",
            "训练和验证耗费的时间4m15s\n",
            "Epoch 446/499\n",
            "----------\n",
            "446 train loss:0.9622 train acc: 0.7816\n",
            "446 val loss:1.0017 val acc: 0.7375\n",
            "训练和验证耗费的时间4m16s\n",
            "Epoch 447/499\n",
            "----------\n",
            "447 train loss:0.9680 train acc: 0.7763\n",
            "447 val loss:1.0091 val acc: 0.7317\n",
            "训练和验证耗费的时间4m16s\n",
            "Epoch 448/499\n",
            "----------\n",
            "448 train loss:0.9759 train acc: 0.7618\n",
            "448 val loss:1.0064 val acc: 0.7336\n",
            "训练和验证耗费的时间4m17s\n",
            "Epoch 449/499\n",
            "----------\n",
            "449 train loss:0.9612 train acc: 0.7748\n",
            "449 val loss:1.0033 val acc: 0.7452\n",
            "训练和验证耗费的时间4m17s\n",
            "Epoch 450/499\n",
            "----------\n",
            "450 train loss:0.9631 train acc: 0.7758\n",
            "450 val loss:1.0074 val acc: 0.7278\n",
            "训练和验证耗费的时间4m18s\n",
            "Epoch 451/499\n",
            "----------\n",
            "451 train loss:0.9690 train acc: 0.7729\n",
            "451 val loss:1.0121 val acc: 0.7259\n",
            "训练和验证耗费的时间4m18s\n",
            "Epoch 452/499\n",
            "----------\n",
            "452 train loss:0.9633 train acc: 0.7792\n",
            "452 val loss:1.0023 val acc: 0.7413\n",
            "训练和验证耗费的时间4m19s\n",
            "Epoch 453/499\n",
            "----------\n",
            "453 train loss:0.9706 train acc: 0.7686\n",
            "453 val loss:0.9976 val acc: 0.7355\n",
            "训练和验证耗费的时间4m19s\n",
            "Epoch 454/499\n",
            "----------\n",
            "454 train loss:0.9751 train acc: 0.7623\n",
            "454 val loss:1.0090 val acc: 0.7259\n",
            "训练和验证耗费的时间4m20s\n",
            "Epoch 455/499\n",
            "----------\n",
            "455 train loss:0.9657 train acc: 0.7729\n",
            "455 val loss:0.9965 val acc: 0.7471\n",
            "训练和验证耗费的时间4m21s\n",
            "Epoch 456/499\n",
            "----------\n",
            "456 train loss:0.9740 train acc: 0.7599\n",
            "456 val loss:1.0073 val acc: 0.7278\n",
            "训练和验证耗费的时间4m21s\n",
            "Epoch 457/499\n",
            "----------\n",
            "457 train loss:0.9606 train acc: 0.7859\n",
            "457 val loss:1.0019 val acc: 0.7432\n",
            "训练和验证耗费的时间4m22s\n",
            "Epoch 458/499\n",
            "----------\n",
            "458 train loss:0.9747 train acc: 0.7642\n",
            "458 val loss:0.9980 val acc: 0.7432\n",
            "训练和验证耗费的时间4m22s\n",
            "Epoch 459/499\n",
            "----------\n",
            "459 train loss:0.9691 train acc: 0.7729\n",
            "459 val loss:1.0025 val acc: 0.7375\n",
            "训练和验证耗费的时间4m23s\n",
            "Epoch 460/499\n",
            "----------\n",
            "460 train loss:0.9820 train acc: 0.7565\n",
            "460 val loss:1.0049 val acc: 0.7317\n",
            "训练和验证耗费的时间4m23s\n",
            "Epoch 461/499\n",
            "----------\n",
            "461 train loss:0.9642 train acc: 0.7768\n",
            "461 val loss:1.0000 val acc: 0.7413\n",
            "训练和验证耗费的时间4m24s\n",
            "Epoch 462/499\n",
            "----------\n",
            "462 train loss:0.9758 train acc: 0.7647\n",
            "462 val loss:1.0084 val acc: 0.7259\n",
            "训练和验证耗费的时间4m24s\n",
            "Epoch 463/499\n",
            "----------\n",
            "463 train loss:0.9662 train acc: 0.7768\n",
            "463 val loss:0.9983 val acc: 0.7355\n",
            "训练和验证耗费的时间4m25s\n",
            "Epoch 464/499\n",
            "----------\n",
            "464 train loss:0.9734 train acc: 0.7642\n",
            "464 val loss:0.9991 val acc: 0.7413\n",
            "训练和验证耗费的时间4m26s\n",
            "Epoch 465/499\n",
            "----------\n",
            "465 train loss:0.9648 train acc: 0.7787\n",
            "465 val loss:1.0028 val acc: 0.7336\n",
            "训练和验证耗费的时间4m26s\n",
            "Epoch 466/499\n",
            "----------\n",
            "466 train loss:0.9740 train acc: 0.7647\n",
            "466 val loss:0.9976 val acc: 0.7394\n",
            "训练和验证耗费的时间4m27s\n",
            "Epoch 467/499\n",
            "----------\n",
            "467 train loss:0.9694 train acc: 0.7671\n",
            "467 val loss:1.0051 val acc: 0.7259\n",
            "训练和验证耗费的时间4m27s\n",
            "Epoch 468/499\n",
            "----------\n",
            "468 train loss:0.9679 train acc: 0.7729\n",
            "468 val loss:0.9954 val acc: 0.7452\n",
            "训练和验证耗费的时间4m28s\n",
            "Epoch 469/499\n",
            "----------\n",
            "469 train loss:0.9621 train acc: 0.7743\n",
            "469 val loss:0.9993 val acc: 0.7355\n",
            "训练和验证耗费的时间4m28s\n",
            "Epoch 470/499\n",
            "----------\n",
            "470 train loss:0.9751 train acc: 0.7628\n",
            "470 val loss:0.9958 val acc: 0.7490\n",
            "训练和验证耗费的时间4m29s\n",
            "Epoch 471/499\n",
            "----------\n",
            "471 train loss:0.9747 train acc: 0.7662\n",
            "471 val loss:1.0012 val acc: 0.7394\n",
            "训练和验证耗费的时间4m30s\n",
            "Epoch 472/499\n",
            "----------\n",
            "472 train loss:0.9663 train acc: 0.7719\n",
            "472 val loss:0.9987 val acc: 0.7375\n",
            "训练和验证耗费的时间4m30s\n",
            "Epoch 473/499\n",
            "----------\n",
            "473 train loss:0.9694 train acc: 0.7652\n",
            "473 val loss:0.9976 val acc: 0.7413\n",
            "训练和验证耗费的时间4m31s\n",
            "Epoch 474/499\n",
            "----------\n",
            "474 train loss:0.9722 train acc: 0.7686\n",
            "474 val loss:1.0085 val acc: 0.7297\n",
            "训练和验证耗费的时间4m31s\n",
            "Epoch 475/499\n",
            "----------\n",
            "475 train loss:0.9561 train acc: 0.7850\n",
            "475 val loss:0.9950 val acc: 0.7394\n",
            "训练和验证耗费的时间4m32s\n",
            "Epoch 476/499\n",
            "----------\n",
            "476 train loss:0.9731 train acc: 0.7608\n",
            "476 val loss:1.0265 val acc: 0.7085\n",
            "训练和验证耗费的时间4m32s\n",
            "Epoch 477/499\n",
            "----------\n",
            "477 train loss:0.9797 train acc: 0.7613\n",
            "477 val loss:1.0102 val acc: 0.7181\n",
            "训练和验证耗费的时间4m33s\n",
            "Epoch 478/499\n",
            "----------\n",
            "478 train loss:0.9653 train acc: 0.7758\n",
            "478 val loss:1.0030 val acc: 0.7355\n",
            "训练和验证耗费的时间4m34s\n",
            "Epoch 479/499\n",
            "----------\n",
            "479 train loss:0.9668 train acc: 0.7797\n",
            "479 val loss:1.0074 val acc: 0.7239\n",
            "训练和验证耗费的时间4m34s\n",
            "Epoch 480/499\n",
            "----------\n",
            "480 train loss:0.9623 train acc: 0.7734\n",
            "480 val loss:1.0062 val acc: 0.7278\n",
            "训练和验证耗费的时间4m35s\n",
            "Epoch 481/499\n",
            "----------\n",
            "481 train loss:0.9705 train acc: 0.7681\n",
            "481 val loss:1.0029 val acc: 0.7259\n",
            "训练和验证耗费的时间4m35s\n",
            "Epoch 482/499\n",
            "----------\n",
            "482 train loss:0.9685 train acc: 0.7719\n",
            "482 val loss:0.9993 val acc: 0.7490\n",
            "训练和验证耗费的时间4m36s\n",
            "Epoch 483/499\n",
            "----------\n",
            "483 train loss:0.9678 train acc: 0.7705\n",
            "483 val loss:1.0048 val acc: 0.7413\n",
            "训练和验证耗费的时间4m36s\n",
            "Epoch 484/499\n",
            "----------\n",
            "484 train loss:0.9674 train acc: 0.7777\n",
            "484 val loss:1.0000 val acc: 0.7452\n",
            "训练和验证耗费的时间4m37s\n",
            "Epoch 485/499\n",
            "----------\n",
            "485 train loss:0.9719 train acc: 0.7647\n",
            "485 val loss:1.0048 val acc: 0.7259\n",
            "训练和验证耗费的时间4m37s\n",
            "Epoch 486/499\n",
            "----------\n",
            "486 train loss:0.9628 train acc: 0.7763\n",
            "486 val loss:1.0045 val acc: 0.7278\n",
            "训练和验证耗费的时间4m38s\n",
            "Epoch 487/499\n",
            "----------\n",
            "487 train loss:0.9649 train acc: 0.7748\n",
            "487 val loss:1.0019 val acc: 0.7432\n",
            "训练和验证耗费的时间4m39s\n",
            "Epoch 488/499\n",
            "----------\n",
            "488 train loss:0.9632 train acc: 0.7821\n",
            "488 val loss:1.0062 val acc: 0.7259\n",
            "训练和验证耗费的时间4m39s\n",
            "Epoch 489/499\n",
            "----------\n",
            "489 train loss:0.9744 train acc: 0.7662\n",
            "489 val loss:1.0018 val acc: 0.7278\n",
            "训练和验证耗费的时间4m40s\n",
            "Epoch 490/499\n",
            "----------\n",
            "490 train loss:0.9625 train acc: 0.7825\n",
            "490 val loss:0.9984 val acc: 0.7375\n",
            "训练和验证耗费的时间4m40s\n",
            "Epoch 491/499\n",
            "----------\n",
            "491 train loss:0.9625 train acc: 0.7753\n",
            "491 val loss:1.0098 val acc: 0.7220\n",
            "训练和验证耗费的时间4m41s\n",
            "Epoch 492/499\n",
            "----------\n",
            "492 train loss:0.9612 train acc: 0.7763\n",
            "492 val loss:1.0064 val acc: 0.7355\n",
            "训练和验证耗费的时间4m41s\n",
            "Epoch 493/499\n",
            "----------\n",
            "493 train loss:0.9620 train acc: 0.7806\n",
            "493 val loss:1.0044 val acc: 0.7317\n",
            "训练和验证耗费的时间4m42s\n",
            "Epoch 494/499\n",
            "----------\n",
            "494 train loss:0.9687 train acc: 0.7724\n",
            "494 val loss:1.0017 val acc: 0.7413\n",
            "训练和验证耗费的时间4m43s\n",
            "Epoch 495/499\n",
            "----------\n",
            "495 train loss:0.9755 train acc: 0.7580\n",
            "495 val loss:1.0041 val acc: 0.7317\n",
            "训练和验证耗费的时间4m43s\n",
            "Epoch 496/499\n",
            "----------\n",
            "496 train loss:0.9638 train acc: 0.7768\n",
            "496 val loss:1.0048 val acc: 0.7220\n",
            "训练和验证耗费的时间4m44s\n",
            "Epoch 497/499\n",
            "----------\n",
            "497 train loss:0.9707 train acc: 0.7719\n",
            "497 val loss:0.9956 val acc: 0.7490\n",
            "训练和验证耗费的时间4m44s\n",
            "Epoch 498/499\n",
            "----------\n",
            "498 train loss:0.9658 train acc: 0.7715\n",
            "498 val loss:1.0020 val acc: 0.7355\n",
            "训练和验证耗费的时间4m45s\n",
            "Epoch 499/499\n",
            "----------\n",
            "499 train loss:0.9686 train acc: 0.7686\n",
            "499 val loss:1.0077 val acc: 0.7220\n",
            "训练和验证耗费的时间4m45s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_test.py**"
      ],
      "metadata": {
        "id": "U02guqzH4SWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from model import EEGNet\n",
        "import os\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_test_pre'):\n",
        "    os.makedirs('2a_test_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'E' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./2a/'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose='error') for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "event_to_id = dict({'783': 7})\n",
        "events = []\n",
        "event_ids = []\n",
        "for i in range(len(raw)):\n",
        "    event, _ = mne.events_from_annotations(raw[i])\n",
        "    events.append(event)\n",
        "    ids = np.unique(events[i][:, 2])\n",
        "    event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "    event_ids.append(event_id)\n",
        "\n",
        "    raw[i].load_data()\n",
        "    data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    epochs = mne.Epochs(raw[i], events[i], event_ids[i], tmin, tmax, proj=False, baseline=None, preload=True)\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('2a_labels/'+data_path[i]+'.mat')\n",
        "    labels = labels_file['classlabel'].reshape(288)\n",
        "    epochs_data = epochs.get_data(copy=True)[:, :, :-1]\n",
        "\n",
        "    n_samples, n_channels, n_timepoints = epochs_data.shape\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    np.savez('2a_test_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "\n",
        "\n",
        "# 5、创建测试集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_test, y_test = [], []\n",
        "    for i in range(1, 10):\n",
        "        test_data = np.load(f'2a_test_pre/A0{i}E.npz')\n",
        "        x_test.append(test_data['data'])\n",
        "        y_test.append(test_data['label'])\n",
        "\n",
        "    # 合并数据\n",
        "    x_test = np.concatenate(x_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_test = torch.FloatTensor(x_test).unsqueeze(1)\n",
        "    y_test = torch.LongTensor(y_test - 1)\n",
        "\n",
        "    # 创建DataLoader\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(x_test, y_test),\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "# 6、模型测试\n",
        "def test_model_process(model, test_loader):\n",
        "    # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "    # 讲模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "    # 初始化参数\n",
        "    test_corrects = 0.0\n",
        "    test_num = 0\n",
        "    # 只进行前向传播计算，不计算梯度，从而节省内存，加快运行速度\n",
        "    with torch.no_grad():\n",
        "        for test_data_x, test_data_y in test_loader:\n",
        "            # 将特征放入到测试设备中\n",
        "            test_data_x = test_data_x.to(device)\n",
        "            # 将标签放入到测试设备中\n",
        "            test_data_y = test_data_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为测试数据集，输出为对每个样本的预测值\n",
        "            output = model(test_data_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 如果预测正确，则准确度test_corrects加1\n",
        "            test_corrects += torch.sum(pre_lab == test_data_y.data)\n",
        "            # 将所有的测试样本进行累加\n",
        "            test_num += test_data_x.size(0)\n",
        "\n",
        "    # 计算测试准确率\n",
        "    test_acc = test_corrects.double().item() / test_num\n",
        "    print(\"测试的准确率为：\", test_acc)\n",
        "\n",
        "\n",
        "# 7、模型开始测试\n",
        "if __name__ == \"__main__\":\n",
        "    model = EEGNet()\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loader = create_simple_dataloaders()\n",
        "    test_model_process(model, test_loader)\n"
      ],
      "metadata": {
        "id": "OqfBuPsq3p2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "836962e2-5c96-4b71-9577-f5123d7ac6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-665368722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEEGNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***获取数据并解压***"
      ],
      "metadata": {
        "id": "Y_5mZ-dPoeWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
        "!unzip -q BCICIV_2a_gdf.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo7_FvGtnYgE",
        "outputId": "7eceda4c-ed6c-434a-af95-82689cd11eb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-05 07:24:19--  https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
            "Resolving www.bbci.de (www.bbci.de)... 141.23.71.83\n",
            "Connecting to www.bbci.de (www.bbci.de)|141.23.71.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439968864 (420M) [application/zip]\n",
            "Saving to: ‘BCICIV_2a_gdf.zip’\n",
            "\n",
            "BCICIV_2a_gdf.zip   100%[===================>] 419.59M  22.4MB/s    in 20s     \n",
            "\n",
            "2025-11-05 07:24:40 (21.5 MB/s) - ‘BCICIV_2a_gdf.zip’ saved [439968864/439968864]\n",
            "\n",
            "--2025-11-05 07:24:40--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A01T.mat [following]\n",
            "--2025-11-05 07:24:41--  https://lampx.tugraz.at/~bci/database/001-2014/A01T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42806453 (41M)\n",
            "Saving to: ‘A01T.mat’\n",
            "\n",
            "A01T.mat            100%[===================>]  40.82M  11.8MB/s    in 3.5s    \n",
            "\n",
            "2025-11-05 07:24:45 (11.8 MB/s) - ‘A01T.mat’ saved [42806453/42806453]\n",
            "\n",
            "--2025-11-05 07:24:45--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A02T.mat [following]\n",
            "--2025-11-05 07:24:46--  https://lampx.tugraz.at/~bci/database/001-2014/A02T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43068077 (41M)\n",
            "Saving to: ‘A02T.mat’\n",
            "\n",
            "A02T.mat            100%[===================>]  41.07M  11.3MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:24:51 (8.79 MB/s) - ‘A02T.mat’ saved [43068077/43068077]\n",
            "\n",
            "--2025-11-05 07:24:51--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A03T.mat [following]\n",
            "--2025-11-05 07:24:52--  https://lampx.tugraz.at/~bci/database/001-2014/A03T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44057065 (42M)\n",
            "Saving to: ‘A03T.mat’\n",
            "\n",
            "A03T.mat            100%[===================>]  42.02M  11.6MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:24:57 (8.97 MB/s) - ‘A03T.mat’ saved [44057065/44057065]\n",
            "\n",
            "--2025-11-05 07:24:57--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A04T.mat [following]\n",
            "--2025-11-05 07:24:58--  https://lampx.tugraz.at/~bci/database/001-2014/A04T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37150377 (35M)\n",
            "Saving to: ‘A04T.mat’\n",
            "\n",
            "A04T.mat            100%[===================>]  35.43M  10.1MB/s    in 4.2s    \n",
            "\n",
            "2025-11-05 07:25:03 (8.51 MB/s) - ‘A04T.mat’ saved [37150377/37150377]\n",
            "\n",
            "--2025-11-05 07:25:03--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A05T.mat [following]\n",
            "--2025-11-05 07:25:04--  https://lampx.tugraz.at/~bci/database/001-2014/A05T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42452392 (40M)\n",
            "Saving to: ‘A05T.mat’\n",
            "\n",
            "A05T.mat            100%[===================>]  40.49M  11.4MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:25:09 (8.98 MB/s) - ‘A05T.mat’ saved [42452392/42452392]\n",
            "\n",
            "--2025-11-05 07:25:09--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A06T.mat [following]\n",
            "--2025-11-05 07:25:10--  https://lampx.tugraz.at/~bci/database/001-2014/A06T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44581256 (43M)\n",
            "Saving to: ‘A06T.mat’\n",
            "\n",
            "A06T.mat            100%[===================>]  42.52M  11.2MB/s    in 4.8s    \n",
            "\n",
            "2025-11-05 07:25:15 (8.77 MB/s) - ‘A06T.mat’ saved [44581256/44581256]\n",
            "\n",
            "--2025-11-05 07:25:15--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A07T.mat [following]\n",
            "--2025-11-05 07:25:16--  https://lampx.tugraz.at/~bci/database/001-2014/A07T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42809746 (41M)\n",
            "Saving to: ‘A07T.mat’\n",
            "\n",
            "A07T.mat            100%[===================>]  40.83M  11.3MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:25:22 (8.74 MB/s) - ‘A07T.mat’ saved [42809746/42809746]\n",
            "\n",
            "--2025-11-05 07:25:22--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A08T.mat [following]\n",
            "--2025-11-05 07:25:23--  https://lampx.tugraz.at/~bci/database/001-2014/A08T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45032065 (43M)\n",
            "Saving to: ‘A08T.mat’\n",
            "\n",
            "A08T.mat            100%[===================>]  42.95M  11.2MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 07:25:28 (8.80 MB/s) - ‘A08T.mat’ saved [45032065/45032065]\n",
            "\n",
            "--2025-11-05 07:25:28--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A09T.mat [following]\n",
            "--2025-11-05 07:25:29--  https://lampx.tugraz.at/~bci/database/001-2014/A09T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44785478 (43M)\n",
            "Saving to: ‘A09T.mat’\n",
            "\n",
            "A09T.mat            100%[===================>]  42.71M  11.3MB/s    in 4.8s    \n",
            "\n",
            "2025-11-05 07:25:34 (8.84 MB/s) - ‘A09T.mat’ saved [44785478/44785478]\n",
            "\n",
            "--2025-11-05 07:25:34--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A01E.mat [following]\n",
            "--2025-11-05 07:25:35--  https://lampx.tugraz.at/~bci/database/001-2014/A01E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43772146 (42M)\n",
            "Saving to: ‘A01E.mat’\n",
            "\n",
            "A01E.mat            100%[===================>]  41.74M  13.4MB/s    in 3.1s    \n",
            "\n",
            "2025-11-05 07:25:39 (13.4 MB/s) - ‘A01E.mat’ saved [43772146/43772146]\n",
            "\n",
            "--2025-11-05 07:25:39--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A02E.mat [following]\n",
            "--2025-11-05 07:25:40--  https://lampx.tugraz.at/~bci/database/001-2014/A02E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44218409 (42M)\n",
            "Saving to: ‘A02E.mat’\n",
            "\n",
            "A02E.mat            100%[===================>]  42.17M  11.6MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:25:45 (8.99 MB/s) - ‘A02E.mat’ saved [44218409/44218409]\n",
            "\n",
            "--2025-11-05 07:25:45--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A03E.mat [following]\n",
            "--2025-11-05 07:25:46--  https://lampx.tugraz.at/~bci/database/001-2014/A03E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42316292 (40M)\n",
            "Saving to: ‘A03E.mat’\n",
            "\n",
            "A03E.mat            100%[===================>]  40.36M  11.3MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:25:52 (8.90 MB/s) - ‘A03E.mat’ saved [42316292/42316292]\n",
            "\n",
            "--2025-11-05 07:25:52--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A04E.mat [following]\n",
            "--2025-11-05 07:25:52--  https://lampx.tugraz.at/~bci/database/001-2014/A04E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41730824 (40M)\n",
            "Saving to: ‘A04E.mat’\n",
            "\n",
            "A04E.mat            100%[===================>]  39.80M  11.2MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:25:58 (8.84 MB/s) - ‘A04E.mat’ saved [41730824/41730824]\n",
            "\n",
            "--2025-11-05 07:25:58--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A05E.mat [following]\n",
            "--2025-11-05 07:25:58--  https://lampx.tugraz.at/~bci/database/001-2014/A05E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44392496 (42M)\n",
            "Saving to: ‘A05E.mat’\n",
            "\n",
            "A05E.mat            100%[===================>]  42.34M  11.1MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 07:26:04 (8.70 MB/s) - ‘A05E.mat’ saved [44392496/44392496]\n",
            "\n",
            "--2025-11-05 07:26:04--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A06E.mat [following]\n",
            "--2025-11-05 07:26:05--  https://lampx.tugraz.at/~bci/database/001-2014/A06E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43395510 (41M)\n",
            "Saving to: ‘A06E.mat’\n",
            "\n",
            "A06E.mat            100%[===================>]  41.38M  11.5MB/s    in 4.6s    \n",
            "\n",
            "2025-11-05 07:26:10 (9.09 MB/s) - ‘A06E.mat’ saved [43395510/43395510]\n",
            "\n",
            "--2025-11-05 07:26:10--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A07E.mat [following]\n",
            "--2025-11-05 07:26:11--  https://lampx.tugraz.at/~bci/database/001-2014/A07E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42242294 (40M)\n",
            "Saving to: ‘A07E.mat’\n",
            "\n",
            "A07E.mat            100%[===================>]  40.29M  11.3MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:26:16 (8.93 MB/s) - ‘A07E.mat’ saved [42242294/42242294]\n",
            "\n",
            "--2025-11-05 07:26:16--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A08E.mat [following]\n",
            "--2025-11-05 07:26:17--  https://lampx.tugraz.at/~bci/database/001-2014/A08E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46282127 (44M)\n",
            "Saving to: ‘A08E.mat’\n",
            "\n",
            "A08E.mat            100%[===================>]  44.14M  11.2MB/s    in 5.0s    \n",
            "\n",
            "2025-11-05 07:26:23 (8.84 MB/s) - ‘A08E.mat’ saved [46282127/46282127]\n",
            "\n",
            "--2025-11-05 07:26:23--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A09E.mat [following]\n",
            "--2025-11-05 07:26:24--  https://lampx.tugraz.at/~bci/database/001-2014/A09E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44780912 (43M)\n",
            "Saving to: ‘A09E.mat’\n",
            "\n",
            "A09E.mat            100%[===================>]  42.71M  11.6MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:26:29 (9.06 MB/s) - ‘A09E.mat’ saved [44780912/44780912]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}