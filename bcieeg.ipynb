{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Tgoh1wYyiRRDUyemYECNRAN30S-d1JGc",
      "authorship_tag": "ABX9TyOWPi5KAuz9B8EOKA0ce9Q1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snoopies/EEGTest/blob/main/bcieeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**model.py**"
      ],
      "metadata": {
        "id": "cCoqY8yp31eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ez0GDqI10HEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e050466b-5263-4424-cda7-f11bf400b5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 22, 1001]             512\n",
            "       BatchNorm2d-2          [-1, 8, 22, 1001]              16\n",
            "            Conv2d-3          [-1, 16, 1, 1001]             352\n",
            "       BatchNorm2d-4          [-1, 16, 1, 1001]              32\n",
            "               ELU-5          [-1, 16, 1, 1001]               0\n",
            "         AvgPool2d-6           [-1, 16, 1, 250]               0\n",
            "           Dropout-7           [-1, 16, 1, 250]               0\n",
            "            Conv2d-8           [-1, 16, 1, 251]             256\n",
            "            Conv2d-9           [-1, 16, 1, 251]             256\n",
            "      BatchNorm2d-10           [-1, 16, 1, 251]              32\n",
            "              ELU-11           [-1, 16, 1, 251]               0\n",
            "        AvgPool2d-12            [-1, 16, 1, 31]               0\n",
            "          Dropout-13            [-1, 16, 1, 31]               0\n",
            "           Linear-14                    [-1, 4]           1,984\n",
            "          Softmax-15                    [-1, 4]               0\n",
            "================================================================\n",
            "Total params: 3,440\n",
            "Trainable params: 3,440\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.08\n",
            "Forward/backward pass size (MB): 3.25\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 3.34\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# 按照论文创建EEGNet\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, n_classes=4, channels=22, samples=1000,\n",
        "                 dropout_rate=0.5, kernel_length1=64, kernel_length2=16,\n",
        "                 f1=8, d=2, f2=16):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.f1 = f1\n",
        "        self.f2 = f2\n",
        "        self.d = d\n",
        "        self.samples = samples\n",
        "        self.n_classes = n_classes\n",
        "        self.channels = channels\n",
        "        self.kernel_length1 = kernel_length1\n",
        "        self.kernel_length2 = kernel_length2\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=self.f1,\n",
        "                kernel_size=(1, self.kernel_length1),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length1//2),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1)\n",
        "        )\n",
        "\n",
        "        block2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1,\n",
        "                out_channels=self.f1*self.d,\n",
        "                kernel_size=(self.channels, 1),\n",
        "                groups=self.f1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1*self.d),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 4),\n",
        "                stride=4\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        block3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f2,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, self.kernel_length2),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length2//2),\n",
        "                groups=self.f1*self.d,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1*self.d,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, 1),\n",
        "                stride=1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 8),\n",
        "                stride=8\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.EEGNetLayer = nn.Sequential(block1, block2, block3)\n",
        "\n",
        "        self.ClassifierBlock = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.f2*round(round(self.samples//4)//8),\n",
        "                out_features=self.n_classes,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) != 4:\n",
        "            x = torch.unsqueeze(x, dim=1)\n",
        "        x = self.EEGNetLayer(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.ClassifierBlock(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 模型结构可视化\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EEGNet().to(device)\n",
        "    print(summary(model, input_size=(1, 22, 1000)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_train.py**"
      ],
      "metadata": {
        "id": "FEh-At_B4JEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import copy\n",
        "import time\n",
        "#from model import EEGNet\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_train_pre'):\n",
        "    os.makedirs('2a_train_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'T' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose=\"error\") for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "events = []\n",
        "event_ids = []\n",
        "for i in range(len(raw)):\n",
        "    event_to_id = dict({'769': 7, '770': 8, '771': 9, '772': 10})\n",
        "    if i == 3:\n",
        "        event_to_id = dict({'769': 5, '770': 6, '771': 7, '772': 8})\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    else:\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    epochs = mne.Epochs(raw[i], events[i], event_ids[i], tmin, tmax, proj=False, baseline=None, preload=True)\n",
        "\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('./'+data_path[i]+'.mat')\n",
        "\n",
        "    # 打印所有键以便调试\n",
        "    print(f\"MAT file keys for {data_path[i]}: {list(labels_file.keys())}\")\n",
        "\n",
        "    # 新的标签提取方法 - 从data结构体的y字段提取\n",
        "    if 'data' in labels_file:\n",
        "        data_struct = labels_file['data']\n",
        "        print(f\"Data structure shape: {data_struct.shape}\")\n",
        "\n",
        "        # 提取所有trial的标签\n",
        "        all_labels = []\n",
        "        for trial_idx in range(data_struct.shape[1]):\n",
        "            trial_data = data_struct[0, trial_idx]\n",
        "            labels = trial_data['y'][0, 0]  # 提取y字段\n",
        "\n",
        "            if labels.size > 0:  # 只处理有标签的trial\n",
        "                trial_labels = labels.flatten().tolist()\n",
        "                all_labels.extend(trial_labels)\n",
        "                print(f\"Trial {trial_idx+1}: 标签数量 {len(trial_labels)}\")\n",
        "            else:\n",
        "                print(f\"Trial {trial_idx+1}: 无标签\")\n",
        "\n",
        "        labels = np.array(all_labels)\n",
        "        print(f\"提取的总标签数量: {len(labels)}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "        # 显示类别对应关系（用于验证）\n",
        "        if len(all_labels) > 0:\n",
        "            classes = data_struct[0, 0]['classes'][0, 0][0]\n",
        "            print(\"类别对应关系:\")\n",
        "            for class_idx, class_name in enumerate(classes):\n",
        "                print(f\"  标签 {class_idx+1}: {class_name[0]}\")\n",
        "\n",
        "    else:\n",
        "        # 备用方法：使用事件信息生成标签\n",
        "        print(\"使用事件信息生成标签\")\n",
        "        labels = events[i][:, 2]\n",
        "        label_mapping = {7: 1, 8: 2, 9: 3, 10: 4}\n",
        "        if i == 3:  # 特殊处理第4个文件\n",
        "            label_mapping = {5: 1, 6: 2, 7: 3, 8: 4}\n",
        "        labels = np.array([label_mapping.get(event_id, 0) for event_id in labels])\n",
        "        labels = labels[labels != 0]  # 移除无效标签\n",
        "\n",
        "    print(f\"最终标签形状: {labels.shape}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "    epochs_data = epochs.get_data(copy=True)[:, :, :-1]\n",
        "\n",
        "    # 确保标签数量与数据样本数量匹配\n",
        "    n_samples = epochs_data.shape[0]\n",
        "    if len(labels) != n_samples:\n",
        "        print(f\"警告: 标签数量 ({len(labels)}) 与数据样本数量 ({n_samples}) 不匹配\")\n",
        "        # 截取或调整标签以匹配数据数量\n",
        "        min_length = min(len(labels), n_samples)\n",
        "        labels = labels[:min_length]\n",
        "        epochs_data = epochs_data[:min_length]\n",
        "        print(f\"调整后: 标签数量 {len(labels)}, 数据样本数量 {epochs_data.shape[0]}\")\n",
        "\n",
        "    n_channels, n_timepoints = epochs_data.shape[1], epochs_data.shape[2]\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    np.savez('2a_train_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "# 5、创建训练集和验证集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(1, 10):\n",
        "        train_data = np.load(f'2a_train_pre/A0{i}T.npz')\n",
        "        x_train.append(train_data['data'])\n",
        "        y_train.append(train_data['label'])\n",
        "\n",
        "    # 合并数据\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_train = torch.FloatTensor(x_train).unsqueeze(1)\n",
        "    y_train = torch.LongTensor(y_train - 1)\n",
        "\n",
        "    # 创建完整数据集\n",
        "    full_dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "    # 计算训练集和验证集大小\n",
        "    dataset_size = len(full_dataset)\n",
        "    val_size = int(dataset_size * 0.2)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # 划分训练集和验证集\n",
        "    train_data, val_data = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # 创建DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# 6、训练模型\n",
        "def train_model_process(model, train_loader, val_loader, num_epochs):\n",
        "    # 设定训练所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # 使用Adam优化器，学习率为0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    # 损失函数为交叉熵函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # 将模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "    # 复制当前模型的参数\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 初始化参数\n",
        "    best_acc = 0.0\n",
        "    train_loss_all = []\n",
        "    val_loss_all = []\n",
        "    train_acc_all = []\n",
        "    val_acc_all = []\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        # 初始化参数\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        train_num = 0\n",
        "        val_num = 0\n",
        "\n",
        "        # 对每一个batch训练和计算\n",
        "        for step, (b_x, b_y) in enumerate(train_loader):\n",
        "            # 将特征放入到训练设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到训练设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为训练模式\n",
        "            model.train()\n",
        "\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 将梯度初始化为0\n",
        "            optimizer.zero_grad()\n",
        "            # 反向传播计算\n",
        "            loss.backward()\n",
        "            # 根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用\n",
        "            optimizer.step()\n",
        "            # 对损失函数进行累加\n",
        "            train_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            train_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于训练的样本数量\n",
        "            train_num += b_x.size(0)\n",
        "\n",
        "        for step, (b_x, b_y) in enumerate(val_loader):\n",
        "            # 将特征放入到验证设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到验证设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 对损失函数进行累加\n",
        "            val_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            val_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于验证的样本数量\n",
        "            val_num += b_x.size(0)\n",
        "\n",
        "        # 计算并保存每一次迭代的loss值和准确率\n",
        "        # 计算并保存训练集的loss值\n",
        "        train_loss_all.append(train_loss / train_num)\n",
        "        # 计算并保存训练集的准确率\n",
        "        train_acc_all.append(train_corrects.double().item() / train_num)\n",
        "\n",
        "        # 计算并保存验证集的loss值\n",
        "        val_loss_all.append(val_loss / val_num)\n",
        "        # 计算并保存验证集的准确率\n",
        "        val_acc_all.append(val_corrects.double().item() / val_num)\n",
        "\n",
        "        print(\"{} train loss:{:.4f} train acc: {:.4f}\".format(epoch, train_loss_all[-1], train_acc_all[-1]))\n",
        "        print(\"{} val loss:{:.4f} val acc: {:.4f}\".format(epoch, val_loss_all[-1], val_acc_all[-1]))\n",
        "\n",
        "        if val_acc_all[-1] > best_acc:\n",
        "            # 保存当前最高准确度\n",
        "            best_acc = val_acc_all[-1]\n",
        "            # 保存当前最高准确度的模型参数\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # 计算训练和验证的耗时\n",
        "        time_use = time.time() - since\n",
        "        print(\"训练和验证耗费的时间{:.0f}m{:.0f}s\".format(time_use//60, time_use % 60))\n",
        "\n",
        "    # 选择最优参数，保存最优参数的模型\n",
        "    torch.save(best_model_wts, \"best_model.pth\")\n",
        "\n",
        "    train_process = pd.DataFrame(data={\"epoch\": range(num_epochs),\n",
        "\"train_loss_all\": train_loss_all,\n",
        "\"val_loss_all\": val_loss_all,\n",
        "\"train_acc_all\": train_acc_all,\n",
        "\"val_acc_all\": val_acc_all})\n",
        "\n",
        "    return train_process\n",
        "\n",
        "\n",
        "# 7、可视化训练集和验证集的损失函数和准确率\n",
        "def matplot_acc_loss(train_process):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 8、模型开始训练\n",
        "if __name__ == '__main__':\n",
        "    model = EEGNet()\n",
        "    train_loader, val_loader = create_simple_dataloaders()\n",
        "    train_process = train_model_process(model, train_loader, val_loader, num_epochs=500)\n",
        "    matplot_acc_loss(train_process)\n",
        "\n"
      ],
      "metadata": {
        "id": "aN-b-UvU3Y7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639a6a22-529e-4f8c-9814-62f25e7a1dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A01T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A02T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A03T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A04T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 7)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 标签数量 48\n",
            "Trial 3: 标签数量 48\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A05T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A06T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A07T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A08T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A09T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Epoch 0/499\n",
            "----------\n",
            "0 train loss:1.3804 train acc: 0.2883\n",
            "0 val loss:1.3709 val acc: 0.3320\n",
            "训练和验证耗费的时间0m1s\n",
            "Epoch 1/499\n",
            "----------\n",
            "1 train loss:1.3541 train acc: 0.3679\n",
            "1 val loss:1.3488 val acc: 0.3726\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 2/499\n",
            "----------\n",
            "2 train loss:1.3274 train acc: 0.3973\n",
            "2 val loss:1.3235 val acc: 0.4170\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 3/499\n",
            "----------\n",
            "3 train loss:1.2917 train acc: 0.4484\n",
            "3 val loss:1.2864 val acc: 0.4807\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 4/499\n",
            "----------\n",
            "4 train loss:1.2641 train acc: 0.4797\n",
            "4 val loss:1.2496 val acc: 0.5232\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 5/499\n",
            "----------\n",
            "5 train loss:1.2394 train acc: 0.4908\n",
            "5 val loss:1.2340 val acc: 0.5193\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 6/499\n",
            "----------\n",
            "6 train loss:1.2202 train acc: 0.5217\n",
            "6 val loss:1.2237 val acc: 0.5193\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 7/499\n",
            "----------\n",
            "7 train loss:1.2069 train acc: 0.5391\n",
            "7 val loss:1.2194 val acc: 0.5174\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 8/499\n",
            "----------\n",
            "8 train loss:1.1960 train acc: 0.5448\n",
            "8 val loss:1.2064 val acc: 0.5328\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 9/499\n",
            "----------\n",
            "9 train loss:1.1887 train acc: 0.5535\n",
            "9 val loss:1.2008 val acc: 0.5386\n",
            "训练和验证耗费的时间0m6s\n",
            "Epoch 10/499\n",
            "----------\n",
            "10 train loss:1.1788 train acc: 0.5583\n",
            "10 val loss:1.1922 val acc: 0.5579\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 11/499\n",
            "----------\n",
            "11 train loss:1.1688 train acc: 0.5733\n",
            "11 val loss:1.1876 val acc: 0.5463\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 12/499\n",
            "----------\n",
            "12 train loss:1.1694 train acc: 0.5680\n",
            "12 val loss:1.1814 val acc: 0.5656\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 13/499\n",
            "----------\n",
            "13 train loss:1.1594 train acc: 0.5834\n",
            "13 val loss:1.1798 val acc: 0.5656\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 14/499\n",
            "----------\n",
            "14 train loss:1.1587 train acc: 0.5853\n",
            "14 val loss:1.1756 val acc: 0.5637\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 15/499\n",
            "----------\n",
            "15 train loss:1.1502 train acc: 0.5993\n",
            "15 val loss:1.1709 val acc: 0.5753\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 16/499\n",
            "----------\n",
            "16 train loss:1.1530 train acc: 0.5868\n",
            "16 val loss:1.1712 val acc: 0.5772\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 17/499\n",
            "----------\n",
            "17 train loss:1.1306 train acc: 0.6119\n",
            "17 val loss:1.1713 val acc: 0.5734\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 18/499\n",
            "----------\n",
            "18 train loss:1.1393 train acc: 0.6027\n",
            "18 val loss:1.1645 val acc: 0.5753\n",
            "训练和验证耗费的时间0m11s\n",
            "Epoch 19/499\n",
            "----------\n",
            "19 train loss:1.1362 train acc: 0.6109\n",
            "19 val loss:1.1635 val acc: 0.5792\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 20/499\n",
            "----------\n",
            "20 train loss:1.1339 train acc: 0.6080\n",
            "20 val loss:1.1631 val acc: 0.5830\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 21/499\n",
            "----------\n",
            "21 train loss:1.1246 train acc: 0.6220\n",
            "21 val loss:1.1685 val acc: 0.5676\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 22/499\n",
            "----------\n",
            "22 train loss:1.1209 train acc: 0.6133\n",
            "22 val loss:1.1570 val acc: 0.5811\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 23/499\n",
            "----------\n",
            "23 train loss:1.1215 train acc: 0.6205\n",
            "23 val loss:1.1551 val acc: 0.5849\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 24/499\n",
            "----------\n",
            "24 train loss:1.1262 train acc: 0.6143\n",
            "24 val loss:1.1492 val acc: 0.5907\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 25/499\n",
            "----------\n",
            "25 train loss:1.1156 train acc: 0.6345\n",
            "25 val loss:1.1482 val acc: 0.5849\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 26/499\n",
            "----------\n",
            "26 train loss:1.1085 train acc: 0.6393\n",
            "26 val loss:1.1479 val acc: 0.5946\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 27/499\n",
            "----------\n",
            "27 train loss:1.1119 train acc: 0.6345\n",
            "27 val loss:1.1481 val acc: 0.5946\n",
            "训练和验证耗费的时间0m16s\n",
            "Epoch 28/499\n",
            "----------\n",
            "28 train loss:1.1107 train acc: 0.6297\n",
            "28 val loss:1.1447 val acc: 0.6042\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 29/499\n",
            "----------\n",
            "29 train loss:1.1048 train acc: 0.6389\n",
            "29 val loss:1.1530 val acc: 0.5907\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 30/499\n",
            "----------\n",
            "30 train loss:1.1161 train acc: 0.6225\n",
            "30 val loss:1.1480 val acc: 0.5907\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 31/499\n",
            "----------\n",
            "31 train loss:1.1160 train acc: 0.6244\n",
            "31 val loss:1.1409 val acc: 0.5849\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 32/499\n",
            "----------\n",
            "32 train loss:1.0984 train acc: 0.6437\n",
            "32 val loss:1.1365 val acc: 0.6100\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 33/499\n",
            "----------\n",
            "33 train loss:1.1063 train acc: 0.6384\n",
            "33 val loss:1.1317 val acc: 0.6139\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 34/499\n",
            "----------\n",
            "34 train loss:1.1016 train acc: 0.6471\n",
            "34 val loss:1.1272 val acc: 0.6236\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 35/499\n",
            "----------\n",
            "35 train loss:1.0946 train acc: 0.6475\n",
            "35 val loss:1.1275 val acc: 0.6158\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 36/499\n",
            "----------\n",
            "36 train loss:1.0931 train acc: 0.6500\n",
            "36 val loss:1.1220 val acc: 0.6255\n",
            "训练和验证耗费的时间0m21s\n",
            "Epoch 37/499\n",
            "----------\n",
            "37 train loss:1.0982 train acc: 0.6451\n",
            "37 val loss:1.1274 val acc: 0.6062\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 38/499\n",
            "----------\n",
            "38 train loss:1.0945 train acc: 0.6422\n",
            "38 val loss:1.1125 val acc: 0.6371\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 39/499\n",
            "----------\n",
            "39 train loss:1.0866 train acc: 0.6553\n",
            "39 val loss:1.1163 val acc: 0.6197\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 40/499\n",
            "----------\n",
            "40 train loss:1.1017 train acc: 0.6365\n",
            "40 val loss:1.1133 val acc: 0.6390\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 41/499\n",
            "----------\n",
            "41 train loss:1.0990 train acc: 0.6355\n",
            "41 val loss:1.1167 val acc: 0.6351\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 42/499\n",
            "----------\n",
            "42 train loss:1.0794 train acc: 0.6577\n",
            "42 val loss:1.1079 val acc: 0.6371\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 43/499\n",
            "----------\n",
            "43 train loss:1.0870 train acc: 0.6524\n",
            "43 val loss:1.1093 val acc: 0.6178\n",
            "训练和验证耗费的时间0m25s\n",
            "Epoch 44/499\n",
            "----------\n",
            "44 train loss:1.0731 train acc: 0.6731\n",
            "44 val loss:1.1034 val acc: 0.6313\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 45/499\n",
            "----------\n",
            "45 train loss:1.0779 train acc: 0.6644\n",
            "45 val loss:1.1086 val acc: 0.6409\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 46/499\n",
            "----------\n",
            "46 train loss:1.0798 train acc: 0.6688\n",
            "46 val loss:1.1041 val acc: 0.6274\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 47/499\n",
            "----------\n",
            "47 train loss:1.0727 train acc: 0.6702\n",
            "47 val loss:1.0997 val acc: 0.6467\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 48/499\n",
            "----------\n",
            "48 train loss:1.0792 train acc: 0.6620\n",
            "48 val loss:1.1048 val acc: 0.6448\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 49/499\n",
            "----------\n",
            "49 train loss:1.0748 train acc: 0.6712\n",
            "49 val loss:1.1020 val acc: 0.6255\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 50/499\n",
            "----------\n",
            "50 train loss:1.0791 train acc: 0.6606\n",
            "50 val loss:1.1003 val acc: 0.6390\n",
            "训练和验证耗费的时间0m29s\n",
            "Epoch 51/499\n",
            "----------\n",
            "51 train loss:1.0622 train acc: 0.6900\n",
            "51 val loss:1.0969 val acc: 0.6371\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 52/499\n",
            "----------\n",
            "52 train loss:1.0709 train acc: 0.6712\n",
            "52 val loss:1.0951 val acc: 0.6390\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 53/499\n",
            "----------\n",
            "53 train loss:1.0704 train acc: 0.6794\n",
            "53 val loss:1.0950 val acc: 0.6467\n",
            "训练和验证耗费的时间0m31s\n",
            "Epoch 54/499\n",
            "----------\n",
            "54 train loss:1.0586 train acc: 0.6779\n",
            "54 val loss:1.0911 val acc: 0.6409\n",
            "训练和验证耗费的时间0m31s\n",
            "Epoch 55/499\n",
            "----------\n",
            "55 train loss:1.0673 train acc: 0.6803\n",
            "55 val loss:1.0885 val acc: 0.6448\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 56/499\n",
            "----------\n",
            "56 train loss:1.0654 train acc: 0.6760\n",
            "56 val loss:1.0902 val acc: 0.6429\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 57/499\n",
            "----------\n",
            "57 train loss:1.0649 train acc: 0.6784\n",
            "57 val loss:1.0874 val acc: 0.6506\n",
            "训练和验证耗费的时间0m33s\n",
            "Epoch 58/499\n",
            "----------\n",
            "58 train loss:1.0656 train acc: 0.6750\n",
            "58 val loss:1.0916 val acc: 0.6467\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 59/499\n",
            "----------\n",
            "59 train loss:1.0650 train acc: 0.6688\n",
            "59 val loss:1.0828 val acc: 0.6564\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 60/499\n",
            "----------\n",
            "60 train loss:1.0626 train acc: 0.6842\n",
            "60 val loss:1.0815 val acc: 0.6448\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 61/499\n",
            "----------\n",
            "61 train loss:1.0606 train acc: 0.6770\n",
            "61 val loss:1.0861 val acc: 0.6448\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 62/499\n",
            "----------\n",
            "62 train loss:1.0667 train acc: 0.6798\n",
            "62 val loss:1.0829 val acc: 0.6622\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 63/499\n",
            "----------\n",
            "63 train loss:1.0614 train acc: 0.6765\n",
            "63 val loss:1.0807 val acc: 0.6583\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 64/499\n",
            "----------\n",
            "64 train loss:1.0637 train acc: 0.6755\n",
            "64 val loss:1.0808 val acc: 0.6583\n",
            "训练和验证耗费的时间0m37s\n",
            "Epoch 65/499\n",
            "----------\n",
            "65 train loss:1.0640 train acc: 0.6765\n",
            "65 val loss:1.0767 val acc: 0.6564\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 66/499\n",
            "----------\n",
            "66 train loss:1.0489 train acc: 0.6861\n",
            "66 val loss:1.0749 val acc: 0.6583\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 67/499\n",
            "----------\n",
            "67 train loss:1.0644 train acc: 0.6774\n",
            "67 val loss:1.0718 val acc: 0.6776\n",
            "训练和验证耗费的时间0m39s\n",
            "Epoch 68/499\n",
            "----------\n",
            "68 train loss:1.0474 train acc: 0.6991\n",
            "68 val loss:1.0765 val acc: 0.6564\n",
            "训练和验证耗费的时间0m39s\n",
            "Epoch 69/499\n",
            "----------\n",
            "69 train loss:1.0525 train acc: 0.6813\n",
            "69 val loss:1.0764 val acc: 0.6486\n",
            "训练和验证耗费的时间0m40s\n",
            "Epoch 70/499\n",
            "----------\n",
            "70 train loss:1.0474 train acc: 0.6909\n",
            "70 val loss:1.0770 val acc: 0.6718\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 71/499\n",
            "----------\n",
            "71 train loss:1.0543 train acc: 0.6866\n",
            "71 val loss:1.0746 val acc: 0.6757\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 72/499\n",
            "----------\n",
            "72 train loss:1.0444 train acc: 0.6972\n",
            "72 val loss:1.0721 val acc: 0.6564\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 73/499\n",
            "----------\n",
            "73 train loss:1.0467 train acc: 0.6948\n",
            "73 val loss:1.0763 val acc: 0.6564\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 74/499\n",
            "----------\n",
            "74 train loss:1.0516 train acc: 0.6909\n",
            "74 val loss:1.0670 val acc: 0.6737\n",
            "训练和验证耗费的时间0m43s\n",
            "Epoch 75/499\n",
            "----------\n",
            "75 train loss:1.0408 train acc: 0.7068\n",
            "75 val loss:1.0725 val acc: 0.6680\n",
            "训练和验证耗费的时间0m44s\n",
            "Epoch 76/499\n",
            "----------\n",
            "76 train loss:1.0390 train acc: 0.7073\n",
            "76 val loss:1.0727 val acc: 0.6564\n",
            "训练和验证耗费的时间0m44s\n",
            "Epoch 77/499\n",
            "----------\n",
            "77 train loss:1.0396 train acc: 0.7015\n",
            "77 val loss:1.0713 val acc: 0.6622\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 78/499\n",
            "----------\n",
            "78 train loss:1.0454 train acc: 0.6943\n",
            "78 val loss:1.0730 val acc: 0.6602\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 79/499\n",
            "----------\n",
            "79 train loss:1.0419 train acc: 0.7059\n",
            "79 val loss:1.0655 val acc: 0.6641\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 80/499\n",
            "----------\n",
            "80 train loss:1.0464 train acc: 0.6958\n",
            "80 val loss:1.0727 val acc: 0.6583\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 81/499\n",
            "----------\n",
            "81 train loss:1.0522 train acc: 0.6779\n",
            "81 val loss:1.0667 val acc: 0.6660\n",
            "训练和验证耗费的时间0m47s\n",
            "Epoch 82/499\n",
            "----------\n",
            "82 train loss:1.0426 train acc: 0.6938\n",
            "82 val loss:1.0675 val acc: 0.6757\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 83/499\n",
            "----------\n",
            "83 train loss:1.0559 train acc: 0.6880\n",
            "83 val loss:1.0676 val acc: 0.6699\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 84/499\n",
            "----------\n",
            "84 train loss:1.0387 train acc: 0.7030\n",
            "84 val loss:1.0633 val acc: 0.6622\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 85/499\n",
            "----------\n",
            "85 train loss:1.0419 train acc: 0.6967\n",
            "85 val loss:1.0609 val acc: 0.6680\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 86/499\n",
            "----------\n",
            "86 train loss:1.0328 train acc: 0.7146\n",
            "86 val loss:1.0611 val acc: 0.6699\n",
            "训练和验证耗费的时间0m50s\n",
            "Epoch 87/499\n",
            "----------\n",
            "87 train loss:1.0352 train acc: 0.7059\n",
            "87 val loss:1.0611 val acc: 0.6641\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 88/499\n",
            "----------\n",
            "88 train loss:1.0392 train acc: 0.7035\n",
            "88 val loss:1.0645 val acc: 0.6680\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 89/499\n",
            "----------\n",
            "89 train loss:1.0400 train acc: 0.7006\n",
            "89 val loss:1.0581 val acc: 0.6853\n",
            "训练和验证耗费的时间0m52s\n",
            "Epoch 90/499\n",
            "----------\n",
            "90 train loss:1.0360 train acc: 0.7083\n",
            "90 val loss:1.0597 val acc: 0.6602\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 91/499\n",
            "----------\n",
            "91 train loss:1.0250 train acc: 0.7170\n",
            "91 val loss:1.0523 val acc: 0.6815\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 92/499\n",
            "----------\n",
            "92 train loss:1.0364 train acc: 0.7059\n",
            "92 val loss:1.0530 val acc: 0.6737\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 93/499\n",
            "----------\n",
            "93 train loss:1.0316 train acc: 0.7122\n",
            "93 val loss:1.0599 val acc: 0.6680\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 94/499\n",
            "----------\n",
            "94 train loss:1.0376 train acc: 0.7001\n",
            "94 val loss:1.0533 val acc: 0.6757\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 95/499\n",
            "----------\n",
            "95 train loss:1.0337 train acc: 0.7112\n",
            "95 val loss:1.0566 val acc: 0.6757\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 96/499\n",
            "----------\n",
            "96 train loss:1.0319 train acc: 0.7088\n",
            "96 val loss:1.0570 val acc: 0.6680\n",
            "训练和验证耗费的时间0m56s\n",
            "Epoch 97/499\n",
            "----------\n",
            "97 train loss:1.0318 train acc: 0.7073\n",
            "97 val loss:1.0590 val acc: 0.6699\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 98/499\n",
            "----------\n",
            "98 train loss:1.0289 train acc: 0.7146\n",
            "98 val loss:1.0593 val acc: 0.6680\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 99/499\n",
            "----------\n",
            "99 train loss:1.0254 train acc: 0.7117\n",
            "99 val loss:1.0551 val acc: 0.6834\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 100/499\n",
            "----------\n",
            "100 train loss:1.0421 train acc: 0.6967\n",
            "100 val loss:1.0558 val acc: 0.6873\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 101/499\n",
            "----------\n",
            "101 train loss:1.0308 train acc: 0.7126\n",
            "101 val loss:1.0598 val acc: 0.6699\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 102/499\n",
            "----------\n",
            "102 train loss:1.0416 train acc: 0.7006\n",
            "102 val loss:1.0593 val acc: 0.6718\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 103/499\n",
            "----------\n",
            "103 train loss:1.0338 train acc: 0.7020\n",
            "103 val loss:1.0577 val acc: 0.6757\n",
            "训练和验证耗费的时间1m0s\n",
            "Epoch 104/499\n",
            "----------\n",
            "104 train loss:1.0325 train acc: 0.7073\n",
            "104 val loss:1.0536 val acc: 0.6834\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 105/499\n",
            "----------\n",
            "105 train loss:1.0301 train acc: 0.7078\n",
            "105 val loss:1.0541 val acc: 0.6795\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 106/499\n",
            "----------\n",
            "106 train loss:1.0243 train acc: 0.7184\n",
            "106 val loss:1.0538 val acc: 0.6718\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 107/499\n",
            "----------\n",
            "107 train loss:1.0205 train acc: 0.7203\n",
            "107 val loss:1.0535 val acc: 0.6776\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 108/499\n",
            "----------\n",
            "108 train loss:1.0281 train acc: 0.7088\n",
            "108 val loss:1.0477 val acc: 0.6853\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 109/499\n",
            "----------\n",
            "109 train loss:1.0275 train acc: 0.7131\n",
            "109 val loss:1.0443 val acc: 0.6911\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 110/499\n",
            "----------\n",
            "110 train loss:1.0310 train acc: 0.7088\n",
            "110 val loss:1.0416 val acc: 0.6950\n",
            "训练和验证耗费的时间1m4s\n",
            "Epoch 111/499\n",
            "----------\n",
            "111 train loss:1.0240 train acc: 0.7141\n",
            "111 val loss:1.0505 val acc: 0.6795\n",
            "训练和验证耗费的时间1m5s\n",
            "Epoch 112/499\n",
            "----------\n",
            "112 train loss:1.0185 train acc: 0.7281\n",
            "112 val loss:1.0417 val acc: 0.6892\n",
            "训练和验证耗费的时间1m5s\n",
            "Epoch 113/499\n",
            "----------\n",
            "113 train loss:1.0233 train acc: 0.7160\n",
            "113 val loss:1.0370 val acc: 0.7008\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 114/499\n",
            "----------\n",
            "114 train loss:1.0240 train acc: 0.7175\n",
            "114 val loss:1.0418 val acc: 0.6853\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 115/499\n",
            "----------\n",
            "115 train loss:1.0285 train acc: 0.7122\n",
            "115 val loss:1.0481 val acc: 0.6853\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 116/499\n",
            "----------\n",
            "116 train loss:1.0215 train acc: 0.7237\n",
            "116 val loss:1.0468 val acc: 0.6815\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 117/499\n",
            "----------\n",
            "117 train loss:1.0333 train acc: 0.7073\n",
            "117 val loss:1.0450 val acc: 0.6795\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 118/499\n",
            "----------\n",
            "118 train loss:1.0228 train acc: 0.7146\n",
            "118 val loss:1.0423 val acc: 0.6988\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 119/499\n",
            "----------\n",
            "119 train loss:1.0307 train acc: 0.7102\n",
            "119 val loss:1.0489 val acc: 0.6853\n",
            "训练和验证耗费的时间1m9s\n",
            "Epoch 120/499\n",
            "----------\n",
            "120 train loss:1.0071 train acc: 0.7392\n",
            "120 val loss:1.0423 val acc: 0.6950\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 121/499\n",
            "----------\n",
            "121 train loss:1.0252 train acc: 0.7175\n",
            "121 val loss:1.0413 val acc: 0.6931\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 122/499\n",
            "----------\n",
            "122 train loss:1.0244 train acc: 0.7146\n",
            "122 val loss:1.0409 val acc: 0.6969\n",
            "训练和验证耗费的时间1m11s\n",
            "Epoch 123/499\n",
            "----------\n",
            "123 train loss:1.0273 train acc: 0.7112\n",
            "123 val loss:1.0434 val acc: 0.6911\n",
            "训练和验证耗费的时间1m11s\n",
            "Epoch 124/499\n",
            "----------\n",
            "124 train loss:1.0151 train acc: 0.7218\n",
            "124 val loss:1.0369 val acc: 0.6988\n",
            "训练和验证耗费的时间1m12s\n",
            "Epoch 125/499\n",
            "----------\n",
            "125 train loss:1.0106 train acc: 0.7285\n",
            "125 val loss:1.0376 val acc: 0.6931\n",
            "训练和验证耗费的时间1m13s\n",
            "Epoch 126/499\n",
            "----------\n",
            "126 train loss:1.0244 train acc: 0.7189\n",
            "126 val loss:1.0397 val acc: 0.6892\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 127/499\n",
            "----------\n",
            "127 train loss:1.0195 train acc: 0.7218\n",
            "127 val loss:1.0434 val acc: 0.6834\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 128/499\n",
            "----------\n",
            "128 train loss:1.0182 train acc: 0.7247\n",
            "128 val loss:1.0343 val acc: 0.7008\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 129/499\n",
            "----------\n",
            "129 train loss:1.0264 train acc: 0.7097\n",
            "129 val loss:1.0361 val acc: 0.7066\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 130/499\n",
            "----------\n",
            "130 train loss:1.0205 train acc: 0.7228\n",
            "130 val loss:1.0332 val acc: 0.7143\n",
            "训练和验证耗费的时间1m16s\n",
            "Epoch 131/499\n",
            "----------\n",
            "131 train loss:1.0272 train acc: 0.7102\n",
            "131 val loss:1.0484 val acc: 0.6853\n",
            "训练和验证耗费的时间1m16s\n",
            "Epoch 132/499\n",
            "----------\n",
            "132 train loss:1.0199 train acc: 0.7232\n",
            "132 val loss:1.0395 val acc: 0.6950\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 133/499\n",
            "----------\n",
            "133 train loss:1.0210 train acc: 0.7170\n",
            "133 val loss:1.0394 val acc: 0.6969\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 134/499\n",
            "----------\n",
            "134 train loss:1.0216 train acc: 0.7170\n",
            "134 val loss:1.0394 val acc: 0.7046\n",
            "训练和验证耗费的时间1m18s\n",
            "Epoch 135/499\n",
            "----------\n",
            "135 train loss:1.0241 train acc: 0.7136\n",
            "135 val loss:1.0434 val acc: 0.6873\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 136/499\n",
            "----------\n",
            "136 train loss:1.0082 train acc: 0.7334\n",
            "136 val loss:1.0343 val acc: 0.7008\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 137/499\n",
            "----------\n",
            "137 train loss:1.0157 train acc: 0.7252\n",
            "137 val loss:1.0372 val acc: 0.6931\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 138/499\n",
            "----------\n",
            "138 train loss:1.0225 train acc: 0.7242\n",
            "138 val loss:1.0369 val acc: 0.7008\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 139/499\n",
            "----------\n",
            "139 train loss:1.0036 train acc: 0.7411\n",
            "139 val loss:1.0369 val acc: 0.6873\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 140/499\n",
            "----------\n",
            "140 train loss:1.0127 train acc: 0.7295\n",
            "140 val loss:1.0337 val acc: 0.6969\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 141/499\n",
            "----------\n",
            "141 train loss:1.0065 train acc: 0.7411\n",
            "141 val loss:1.0398 val acc: 0.6931\n",
            "训练和验证耗费的时间1m22s\n",
            "Epoch 142/499\n",
            "----------\n",
            "142 train loss:1.0101 train acc: 0.7290\n",
            "142 val loss:1.0435 val acc: 0.6988\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 143/499\n",
            "----------\n",
            "143 train loss:1.0093 train acc: 0.7319\n",
            "143 val loss:1.0377 val acc: 0.6931\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 144/499\n",
            "----------\n",
            "144 train loss:1.0020 train acc: 0.7372\n",
            "144 val loss:1.0387 val acc: 0.7008\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 145/499\n",
            "----------\n",
            "145 train loss:1.0122 train acc: 0.7281\n",
            "145 val loss:1.0349 val acc: 0.7066\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 146/499\n",
            "----------\n",
            "146 train loss:1.0171 train acc: 0.7261\n",
            "146 val loss:1.0357 val acc: 0.6950\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 147/499\n",
            "----------\n",
            "147 train loss:1.0139 train acc: 0.7290\n",
            "147 val loss:1.0361 val acc: 0.6911\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 148/499\n",
            "----------\n",
            "148 train loss:1.0249 train acc: 0.7155\n",
            "148 val loss:1.0425 val acc: 0.6950\n",
            "训练和验证耗费的时间1m26s\n",
            "Epoch 149/499\n",
            "----------\n",
            "149 train loss:1.0145 train acc: 0.7276\n",
            "149 val loss:1.0369 val acc: 0.6911\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 150/499\n",
            "----------\n",
            "150 train loss:1.0196 train acc: 0.7189\n",
            "150 val loss:1.0320 val acc: 0.7027\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 151/499\n",
            "----------\n",
            "151 train loss:0.9991 train acc: 0.7416\n",
            "151 val loss:1.0365 val acc: 0.6950\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 152/499\n",
            "----------\n",
            "152 train loss:1.0111 train acc: 0.7348\n",
            "152 val loss:1.0285 val acc: 0.7085\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 153/499\n",
            "----------\n",
            "153 train loss:1.0129 train acc: 0.7232\n",
            "153 val loss:1.0372 val acc: 0.6988\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 154/499\n",
            "----------\n",
            "154 train loss:1.0079 train acc: 0.7305\n",
            "154 val loss:1.0275 val acc: 0.6988\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 155/499\n",
            "----------\n",
            "155 train loss:1.0166 train acc: 0.7146\n",
            "155 val loss:1.0304 val acc: 0.7104\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 156/499\n",
            "----------\n",
            "156 train loss:1.0189 train acc: 0.7160\n",
            "156 val loss:1.0352 val acc: 0.6950\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 157/499\n",
            "----------\n",
            "157 train loss:1.0165 train acc: 0.7218\n",
            "157 val loss:1.0310 val acc: 0.6988\n",
            "训练和验证耗费的时间1m31s\n",
            "Epoch 158/499\n",
            "----------\n",
            "158 train loss:1.0217 train acc: 0.7160\n",
            "158 val loss:1.0360 val acc: 0.7046\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 159/499\n",
            "----------\n",
            "159 train loss:1.0124 train acc: 0.7305\n",
            "159 val loss:1.0335 val acc: 0.6988\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 160/499\n",
            "----------\n",
            "160 train loss:1.0017 train acc: 0.7430\n",
            "160 val loss:1.0334 val acc: 0.7008\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 161/499\n",
            "----------\n",
            "161 train loss:1.0217 train acc: 0.7199\n",
            "161 val loss:1.0315 val acc: 0.6950\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 162/499\n",
            "----------\n",
            "162 train loss:1.0105 train acc: 0.7353\n",
            "162 val loss:1.0288 val acc: 0.6988\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 163/499\n",
            "----------\n",
            "163 train loss:1.0154 train acc: 0.7242\n",
            "163 val loss:1.0410 val acc: 0.6950\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 164/499\n",
            "----------\n",
            "164 train loss:1.0116 train acc: 0.7271\n",
            "164 val loss:1.0287 val acc: 0.7046\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 165/499\n",
            "----------\n",
            "165 train loss:1.0094 train acc: 0.7343\n",
            "165 val loss:1.0345 val acc: 0.7046\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 166/499\n",
            "----------\n",
            "166 train loss:1.0048 train acc: 0.7348\n",
            "166 val loss:1.0313 val acc: 0.7027\n",
            "训练和验证耗费的时间1m36s\n",
            "Epoch 167/499\n",
            "----------\n",
            "167 train loss:1.0127 train acc: 0.7285\n",
            "167 val loss:1.0286 val acc: 0.6969\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 168/499\n",
            "----------\n",
            "168 train loss:1.0092 train acc: 0.7310\n",
            "168 val loss:1.0295 val acc: 0.7066\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 169/499\n",
            "----------\n",
            "169 train loss:1.0145 train acc: 0.7228\n",
            "169 val loss:1.0364 val acc: 0.6988\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 170/499\n",
            "----------\n",
            "170 train loss:1.0091 train acc: 0.7247\n",
            "170 val loss:1.0372 val acc: 0.6911\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 171/499\n",
            "----------\n",
            "171 train loss:1.0056 train acc: 0.7334\n",
            "171 val loss:1.0309 val acc: 0.7066\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 172/499\n",
            "----------\n",
            "172 train loss:1.0103 train acc: 0.7310\n",
            "172 val loss:1.0279 val acc: 0.7104\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 173/499\n",
            "----------\n",
            "173 train loss:1.0032 train acc: 0.7367\n",
            "173 val loss:1.0252 val acc: 0.7066\n",
            "训练和验证耗费的时间1m40s\n",
            "Epoch 174/499\n",
            "----------\n",
            "174 train loss:1.0056 train acc: 0.7348\n",
            "174 val loss:1.0224 val acc: 0.7181\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 175/499\n",
            "----------\n",
            "175 train loss:1.0048 train acc: 0.7329\n",
            "175 val loss:1.0331 val acc: 0.6988\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 176/499\n",
            "----------\n",
            "176 train loss:1.0084 train acc: 0.7314\n",
            "176 val loss:1.0314 val acc: 0.7046\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 177/499\n",
            "----------\n",
            "177 train loss:1.0084 train acc: 0.7281\n",
            "177 val loss:1.0293 val acc: 0.7008\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 178/499\n",
            "----------\n",
            "178 train loss:1.0188 train acc: 0.7213\n",
            "178 val loss:1.0340 val acc: 0.6988\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 179/499\n",
            "----------\n",
            "179 train loss:1.0028 train acc: 0.7387\n",
            "179 val loss:1.0308 val acc: 0.7027\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 180/499\n",
            "----------\n",
            "180 train loss:1.0078 train acc: 0.7300\n",
            "180 val loss:1.0334 val acc: 0.6911\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 181/499\n",
            "----------\n",
            "181 train loss:1.0050 train acc: 0.7392\n",
            "181 val loss:1.0232 val acc: 0.7181\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 182/499\n",
            "----------\n",
            "182 train loss:0.9971 train acc: 0.7488\n",
            "182 val loss:1.0248 val acc: 0.7181\n",
            "训练和验证耗费的时间1m45s\n",
            "Epoch 183/499\n",
            "----------\n",
            "183 train loss:1.0159 train acc: 0.7247\n",
            "183 val loss:1.0274 val acc: 0.7104\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 184/499\n",
            "----------\n",
            "184 train loss:1.0023 train acc: 0.7382\n",
            "184 val loss:1.0251 val acc: 0.7085\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 185/499\n",
            "----------\n",
            "185 train loss:1.0059 train acc: 0.7319\n",
            "185 val loss:1.0265 val acc: 0.7027\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 186/499\n",
            "----------\n",
            "186 train loss:1.0110 train acc: 0.7261\n",
            "186 val loss:1.0244 val acc: 0.7104\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 187/499\n",
            "----------\n",
            "187 train loss:1.0074 train acc: 0.7372\n",
            "187 val loss:1.0267 val acc: 0.7124\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 188/499\n",
            "----------\n",
            "188 train loss:1.0009 train acc: 0.7387\n",
            "188 val loss:1.0191 val acc: 0.7201\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 189/499\n",
            "----------\n",
            "189 train loss:1.0011 train acc: 0.7420\n",
            "189 val loss:1.0169 val acc: 0.7278\n",
            "训练和验证耗费的时间1m49s\n",
            "Epoch 190/499\n",
            "----------\n",
            "190 train loss:1.0020 train acc: 0.7387\n",
            "190 val loss:1.0221 val acc: 0.7201\n",
            "训练和验证耗费的时间1m50s\n",
            "Epoch 191/499\n",
            "----------\n",
            "191 train loss:1.0058 train acc: 0.7290\n",
            "191 val loss:1.0231 val acc: 0.7162\n",
            "训练和验证耗费的时间1m50s\n",
            "Epoch 192/499\n",
            "----------\n",
            "192 train loss:1.0022 train acc: 0.7440\n",
            "192 val loss:1.0287 val acc: 0.7066\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 193/499\n",
            "----------\n",
            "193 train loss:1.0060 train acc: 0.7377\n",
            "193 val loss:1.0305 val acc: 0.7085\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 194/499\n",
            "----------\n",
            "194 train loss:0.9969 train acc: 0.7454\n",
            "194 val loss:1.0228 val acc: 0.7162\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 195/499\n",
            "----------\n",
            "195 train loss:0.9991 train acc: 0.7401\n",
            "195 val loss:1.0160 val acc: 0.7239\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 196/499\n",
            "----------\n",
            "196 train loss:0.9971 train acc: 0.7396\n",
            "196 val loss:1.0251 val acc: 0.7124\n",
            "训练和验证耗费的时间1m53s\n",
            "Epoch 197/499\n",
            "----------\n",
            "197 train loss:1.0054 train acc: 0.7329\n",
            "197 val loss:1.0313 val acc: 0.6988\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 198/499\n",
            "----------\n",
            "198 train loss:0.9898 train acc: 0.7594\n",
            "198 val loss:1.0214 val acc: 0.7201\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 199/499\n",
            "----------\n",
            "199 train loss:0.9984 train acc: 0.7416\n",
            "199 val loss:1.0181 val acc: 0.7259\n",
            "训练和验证耗费的时间1m55s\n",
            "Epoch 200/499\n",
            "----------\n",
            "200 train loss:1.0016 train acc: 0.7377\n",
            "200 val loss:1.0226 val acc: 0.7124\n",
            "训练和验证耗费的时间1m55s\n",
            "Epoch 201/499\n",
            "----------\n",
            "201 train loss:1.0052 train acc: 0.7334\n",
            "201 val loss:1.0329 val acc: 0.7046\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 202/499\n",
            "----------\n",
            "202 train loss:1.0045 train acc: 0.7319\n",
            "202 val loss:1.0254 val acc: 0.7104\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 203/499\n",
            "----------\n",
            "203 train loss:0.9946 train acc: 0.7469\n",
            "203 val loss:1.0204 val acc: 0.7143\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 204/499\n",
            "----------\n",
            "204 train loss:0.9989 train acc: 0.7430\n",
            "204 val loss:1.0210 val acc: 0.7162\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 205/499\n",
            "----------\n",
            "205 train loss:1.0060 train acc: 0.7290\n",
            "205 val loss:1.0290 val acc: 0.7027\n",
            "训练和验证耗费的时间1m58s\n",
            "Epoch 206/499\n",
            "----------\n",
            "206 train loss:1.0009 train acc: 0.7420\n",
            "206 val loss:1.0269 val acc: 0.7066\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 207/499\n",
            "----------\n",
            "207 train loss:0.9932 train acc: 0.7498\n",
            "207 val loss:1.0248 val acc: 0.7104\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 208/499\n",
            "----------\n",
            "208 train loss:1.0014 train acc: 0.7367\n",
            "208 val loss:1.0265 val acc: 0.7046\n",
            "训练和验证耗费的时间1m60s\n",
            "Epoch 209/499\n",
            "----------\n",
            "209 train loss:0.9967 train acc: 0.7406\n",
            "209 val loss:1.0227 val acc: 0.7143\n",
            "训练和验证耗费的时间2m0s\n",
            "Epoch 210/499\n",
            "----------\n",
            "210 train loss:0.9949 train acc: 0.7459\n",
            "210 val loss:1.0211 val acc: 0.7201\n",
            "训练和验证耗费的时间2m1s\n",
            "Epoch 211/499\n",
            "----------\n",
            "211 train loss:0.9899 train acc: 0.7498\n",
            "211 val loss:1.0261 val acc: 0.7046\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 212/499\n",
            "----------\n",
            "212 train loss:1.0021 train acc: 0.7382\n",
            "212 val loss:1.0244 val acc: 0.7104\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 213/499\n",
            "----------\n",
            "213 train loss:1.0077 train acc: 0.7372\n",
            "213 val loss:1.0321 val acc: 0.7162\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 214/499\n",
            "----------\n",
            "214 train loss:1.0013 train acc: 0.7416\n",
            "214 val loss:1.0345 val acc: 0.7085\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 215/499\n",
            "----------\n",
            "215 train loss:1.0006 train acc: 0.7363\n",
            "215 val loss:1.0252 val acc: 0.7181\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 216/499\n",
            "----------\n",
            "216 train loss:0.9926 train acc: 0.7507\n",
            "216 val loss:1.0275 val acc: 0.7104\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 217/499\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_test.py**"
      ],
      "metadata": {
        "id": "U02guqzH4SWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from model import EEGNet\n",
        "import os\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_test_pre'):\n",
        "    os.makedirs('2a_test_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'E' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./2a/'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose='error') for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "event_to_id = dict({'783': 7})\n",
        "events = []\n",
        "event_ids = []\n",
        "for i in range(len(raw)):\n",
        "    event, _ = mne.events_from_annotations(raw[i])\n",
        "    events.append(event)\n",
        "    ids = np.unique(events[i][:, 2])\n",
        "    event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "    event_ids.append(event_id)\n",
        "\n",
        "    raw[i].load_data()\n",
        "    data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    epochs = mne.Epochs(raw[i], events[i], event_ids[i], tmin, tmax, proj=False, baseline=None, preload=True)\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('2a_labels/'+data_path[i]+'.mat')\n",
        "    labels = labels_file['classlabel'].reshape(288)\n",
        "    epochs_data = epochs.get_data(copy=True)[:, :, :-1]\n",
        "\n",
        "    n_samples, n_channels, n_timepoints = epochs_data.shape\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    np.savez('2a_test_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "\n",
        "\n",
        "# 5、创建测试集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_test, y_test = [], []\n",
        "    for i in range(1, 10):\n",
        "        test_data = np.load(f'2a_test_pre/A0{i}E.npz')\n",
        "        x_test.append(test_data['data'])\n",
        "        y_test.append(test_data['label'])\n",
        "\n",
        "    # 合并数据\n",
        "    x_test = np.concatenate(x_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_test = torch.FloatTensor(x_test).unsqueeze(1)\n",
        "    y_test = torch.LongTensor(y_test - 1)\n",
        "\n",
        "    # 创建DataLoader\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(x_test, y_test),\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "# 6、模型测试\n",
        "def test_model_process(model, test_loader):\n",
        "    # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "    # 讲模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "    # 初始化参数\n",
        "    test_corrects = 0.0\n",
        "    test_num = 0\n",
        "    # 只进行前向传播计算，不计算梯度，从而节省内存，加快运行速度\n",
        "    with torch.no_grad():\n",
        "        for test_data_x, test_data_y in test_loader:\n",
        "            # 将特征放入到测试设备中\n",
        "            test_data_x = test_data_x.to(device)\n",
        "            # 将标签放入到测试设备中\n",
        "            test_data_y = test_data_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为测试数据集，输出为对每个样本的预测值\n",
        "            output = model(test_data_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 如果预测正确，则准确度test_corrects加1\n",
        "            test_corrects += torch.sum(pre_lab == test_data_y.data)\n",
        "            # 将所有的测试样本进行累加\n",
        "            test_num += test_data_x.size(0)\n",
        "\n",
        "    # 计算测试准确率\n",
        "    test_acc = test_corrects.double().item() / test_num\n",
        "    print(\"测试的准确率为：\", test_acc)\n",
        "\n",
        "\n",
        "# 7、模型开始测试\n",
        "if __name__ == \"__main__\":\n",
        "    model = EEGNet()\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loader = create_simple_dataloaders()\n",
        "    test_model_process(model, test_loader)\n"
      ],
      "metadata": {
        "id": "OqfBuPsq3p2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "836962e2-5c96-4b71-9577-f5123d7ac6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-665368722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEEGNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***获取数据并解压***"
      ],
      "metadata": {
        "id": "Y_5mZ-dPoeWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
        "!unzip -q BCICIV_2a_gdf.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo7_FvGtnYgE",
        "outputId": "7eceda4c-ed6c-434a-af95-82689cd11eb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-05 07:24:19--  https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
            "Resolving www.bbci.de (www.bbci.de)... 141.23.71.83\n",
            "Connecting to www.bbci.de (www.bbci.de)|141.23.71.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439968864 (420M) [application/zip]\n",
            "Saving to: ‘BCICIV_2a_gdf.zip’\n",
            "\n",
            "BCICIV_2a_gdf.zip   100%[===================>] 419.59M  22.4MB/s    in 20s     \n",
            "\n",
            "2025-11-05 07:24:40 (21.5 MB/s) - ‘BCICIV_2a_gdf.zip’ saved [439968864/439968864]\n",
            "\n",
            "--2025-11-05 07:24:40--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A01T.mat [following]\n",
            "--2025-11-05 07:24:41--  https://lampx.tugraz.at/~bci/database/001-2014/A01T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42806453 (41M)\n",
            "Saving to: ‘A01T.mat’\n",
            "\n",
            "A01T.mat            100%[===================>]  40.82M  11.8MB/s    in 3.5s    \n",
            "\n",
            "2025-11-05 07:24:45 (11.8 MB/s) - ‘A01T.mat’ saved [42806453/42806453]\n",
            "\n",
            "--2025-11-05 07:24:45--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A02T.mat [following]\n",
            "--2025-11-05 07:24:46--  https://lampx.tugraz.at/~bci/database/001-2014/A02T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43068077 (41M)\n",
            "Saving to: ‘A02T.mat’\n",
            "\n",
            "A02T.mat            100%[===================>]  41.07M  11.3MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:24:51 (8.79 MB/s) - ‘A02T.mat’ saved [43068077/43068077]\n",
            "\n",
            "--2025-11-05 07:24:51--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A03T.mat [following]\n",
            "--2025-11-05 07:24:52--  https://lampx.tugraz.at/~bci/database/001-2014/A03T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44057065 (42M)\n",
            "Saving to: ‘A03T.mat’\n",
            "\n",
            "A03T.mat            100%[===================>]  42.02M  11.6MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:24:57 (8.97 MB/s) - ‘A03T.mat’ saved [44057065/44057065]\n",
            "\n",
            "--2025-11-05 07:24:57--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A04T.mat [following]\n",
            "--2025-11-05 07:24:58--  https://lampx.tugraz.at/~bci/database/001-2014/A04T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37150377 (35M)\n",
            "Saving to: ‘A04T.mat’\n",
            "\n",
            "A04T.mat            100%[===================>]  35.43M  10.1MB/s    in 4.2s    \n",
            "\n",
            "2025-11-05 07:25:03 (8.51 MB/s) - ‘A04T.mat’ saved [37150377/37150377]\n",
            "\n",
            "--2025-11-05 07:25:03--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A05T.mat [following]\n",
            "--2025-11-05 07:25:04--  https://lampx.tugraz.at/~bci/database/001-2014/A05T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42452392 (40M)\n",
            "Saving to: ‘A05T.mat’\n",
            "\n",
            "A05T.mat            100%[===================>]  40.49M  11.4MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:25:09 (8.98 MB/s) - ‘A05T.mat’ saved [42452392/42452392]\n",
            "\n",
            "--2025-11-05 07:25:09--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A06T.mat [following]\n",
            "--2025-11-05 07:25:10--  https://lampx.tugraz.at/~bci/database/001-2014/A06T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44581256 (43M)\n",
            "Saving to: ‘A06T.mat’\n",
            "\n",
            "A06T.mat            100%[===================>]  42.52M  11.2MB/s    in 4.8s    \n",
            "\n",
            "2025-11-05 07:25:15 (8.77 MB/s) - ‘A06T.mat’ saved [44581256/44581256]\n",
            "\n",
            "--2025-11-05 07:25:15--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A07T.mat [following]\n",
            "--2025-11-05 07:25:16--  https://lampx.tugraz.at/~bci/database/001-2014/A07T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42809746 (41M)\n",
            "Saving to: ‘A07T.mat’\n",
            "\n",
            "A07T.mat            100%[===================>]  40.83M  11.3MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:25:22 (8.74 MB/s) - ‘A07T.mat’ saved [42809746/42809746]\n",
            "\n",
            "--2025-11-05 07:25:22--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A08T.mat [following]\n",
            "--2025-11-05 07:25:23--  https://lampx.tugraz.at/~bci/database/001-2014/A08T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45032065 (43M)\n",
            "Saving to: ‘A08T.mat’\n",
            "\n",
            "A08T.mat            100%[===================>]  42.95M  11.2MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 07:25:28 (8.80 MB/s) - ‘A08T.mat’ saved [45032065/45032065]\n",
            "\n",
            "--2025-11-05 07:25:28--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A09T.mat [following]\n",
            "--2025-11-05 07:25:29--  https://lampx.tugraz.at/~bci/database/001-2014/A09T.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44785478 (43M)\n",
            "Saving to: ‘A09T.mat’\n",
            "\n",
            "A09T.mat            100%[===================>]  42.71M  11.3MB/s    in 4.8s    \n",
            "\n",
            "2025-11-05 07:25:34 (8.84 MB/s) - ‘A09T.mat’ saved [44785478/44785478]\n",
            "\n",
            "--2025-11-05 07:25:34--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A01E.mat [following]\n",
            "--2025-11-05 07:25:35--  https://lampx.tugraz.at/~bci/database/001-2014/A01E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43772146 (42M)\n",
            "Saving to: ‘A01E.mat’\n",
            "\n",
            "A01E.mat            100%[===================>]  41.74M  13.4MB/s    in 3.1s    \n",
            "\n",
            "2025-11-05 07:25:39 (13.4 MB/s) - ‘A01E.mat’ saved [43772146/43772146]\n",
            "\n",
            "--2025-11-05 07:25:39--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A02E.mat [following]\n",
            "--2025-11-05 07:25:40--  https://lampx.tugraz.at/~bci/database/001-2014/A02E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44218409 (42M)\n",
            "Saving to: ‘A02E.mat’\n",
            "\n",
            "A02E.mat            100%[===================>]  42.17M  11.6MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:25:45 (8.99 MB/s) - ‘A02E.mat’ saved [44218409/44218409]\n",
            "\n",
            "--2025-11-05 07:25:45--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A03E.mat [following]\n",
            "--2025-11-05 07:25:46--  https://lampx.tugraz.at/~bci/database/001-2014/A03E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42316292 (40M)\n",
            "Saving to: ‘A03E.mat’\n",
            "\n",
            "A03E.mat            100%[===================>]  40.36M  11.3MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:25:52 (8.90 MB/s) - ‘A03E.mat’ saved [42316292/42316292]\n",
            "\n",
            "--2025-11-05 07:25:52--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A04E.mat [following]\n",
            "--2025-11-05 07:25:52--  https://lampx.tugraz.at/~bci/database/001-2014/A04E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41730824 (40M)\n",
            "Saving to: ‘A04E.mat’\n",
            "\n",
            "A04E.mat            100%[===================>]  39.80M  11.2MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:25:58 (8.84 MB/s) - ‘A04E.mat’ saved [41730824/41730824]\n",
            "\n",
            "--2025-11-05 07:25:58--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A05E.mat [following]\n",
            "--2025-11-05 07:25:58--  https://lampx.tugraz.at/~bci/database/001-2014/A05E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44392496 (42M)\n",
            "Saving to: ‘A05E.mat’\n",
            "\n",
            "A05E.mat            100%[===================>]  42.34M  11.1MB/s    in 4.9s    \n",
            "\n",
            "2025-11-05 07:26:04 (8.70 MB/s) - ‘A05E.mat’ saved [44392496/44392496]\n",
            "\n",
            "--2025-11-05 07:26:04--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A06E.mat [following]\n",
            "--2025-11-05 07:26:05--  https://lampx.tugraz.at/~bci/database/001-2014/A06E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43395510 (41M)\n",
            "Saving to: ‘A06E.mat’\n",
            "\n",
            "A06E.mat            100%[===================>]  41.38M  11.5MB/s    in 4.6s    \n",
            "\n",
            "2025-11-05 07:26:10 (9.09 MB/s) - ‘A06E.mat’ saved [43395510/43395510]\n",
            "\n",
            "--2025-11-05 07:26:10--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A07E.mat [following]\n",
            "--2025-11-05 07:26:11--  https://lampx.tugraz.at/~bci/database/001-2014/A07E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42242294 (40M)\n",
            "Saving to: ‘A07E.mat’\n",
            "\n",
            "A07E.mat            100%[===================>]  40.29M  11.3MB/s    in 4.5s    \n",
            "\n",
            "2025-11-05 07:26:16 (8.93 MB/s) - ‘A07E.mat’ saved [42242294/42242294]\n",
            "\n",
            "--2025-11-05 07:26:16--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A08E.mat [following]\n",
            "--2025-11-05 07:26:17--  https://lampx.tugraz.at/~bci/database/001-2014/A08E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46282127 (44M)\n",
            "Saving to: ‘A08E.mat’\n",
            "\n",
            "A08E.mat            100%[===================>]  44.14M  11.2MB/s    in 5.0s    \n",
            "\n",
            "2025-11-05 07:26:23 (8.84 MB/s) - ‘A08E.mat’ saved [46282127/46282127]\n",
            "\n",
            "--2025-11-05 07:26:23--  https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
            "Resolving bnci-horizon-2020.eu (bnci-horizon-2020.eu)... 91.227.205.222, 2a03:f80:ad15:91:227:205:222:1\n",
            "Connecting to bnci-horizon-2020.eu (bnci-horizon-2020.eu)|91.227.205.222|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://lampx.tugraz.at/~bci/database/001-2014/A09E.mat [following]\n",
            "--2025-11-05 07:26:24--  https://lampx.tugraz.at/~bci/database/001-2014/A09E.mat\n",
            "Resolving lampx.tugraz.at (lampx.tugraz.at)... 129.27.124.233\n",
            "Connecting to lampx.tugraz.at (lampx.tugraz.at)|129.27.124.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44780912 (43M)\n",
            "Saving to: ‘A09E.mat’\n",
            "\n",
            "A09E.mat            100%[===================>]  42.71M  11.6MB/s    in 4.7s    \n",
            "\n",
            "2025-11-05 07:26:29 (9.06 MB/s) - ‘A09E.mat’ saved [44780912/44780912]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}